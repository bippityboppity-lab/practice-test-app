var quizData = [{"id": "Q1", "number": 1, "question": "Cloud Kicks has the following requirements:\n    * Their Shipment custom object must always relate to a Product, a Sender, and a Receiver (all separate custom objects).\n    * If a Shipment is currently associated with a Product, Sender, or Receiver, deletion of those records should not be allowed.\n    * Each custom object must have separate sharing models.\n\n    What should an Architect do to fulfill these requirements?", "options": {"A": "Associate the Shipment to each parent record by using a VLOOKUP formula field.", "B": "Create a required Lookup relationship to each of the three parent records.", "C": "Create a Master-Detail relationship to each of the three parent records.", "D": "Create two Master-Detail and one Lookup relationship to the parent records."}, "answer": ["B"], "explanations": [{"Status": "Correct", "Choice": "B. Create a required Lookup relationship to each of the three parent records.", "Rationale": "[cite_start]**Lookup Relationships** are the correct choice because they allow the Shipment object to maintain a **separate sharing model** (essential requirement)[cite: 10]. [cite_start]They can be made **required** and also prevent deletion of the parent record by enabling the \"Restrict Delete\" option[cite: 11]."}, {"Status": "Incorrect", "Choice": "A. Associate the Shipment to each parent record by using a VLOOKUP formula field.", "Rationale": "[cite_start]Formula fields retrieve data but do not establish relationships, enforce required fields, or prevent parent deletion[cite: 13]."}, {"Status": "Incorrect", "Choice": "C. Create a Master-Detail relationship to each of the three parent records.", "Rationale": "[cite_start]Master-Detail (MD) relationships force the child to inherit the parent's sharing model, violating the \"separate sharing models\" requirement[cite: 15]. Additionally, you can only have a maximum of two MD relationships per object."}, {"Status": "Incorrect", "Choice": "D. Create two Master-Detail and one Lookup relationship to the parent records.", "Rationale": "[cite_start]Even two MD relationships would cause sharing inheritance, violating the core requirement[cite: 18]."}], "meta": {"source": "QuestionBank", "original_number": 1}}, {"id": "Q2", "number": 2, "question": "Universal Containers (UC) is planning to move away from legacy CRM to Salesforce. As part of one-time data migration, UC will need to keep the original date when a contact was created in the legacy system. How should an Architect design the data migration solution to meet this requirement?", "options": {"A": "After the data is migrated, perform an update on all records to set the original date in a standard CreatedDate field.", "B": "Create a new field on Contact object to capture the Created Date. Hide the standard CreatedDate field using Field-Level Security.", "C": "Enable \"Set Audit Fields\" and assign the permission to the user loading the data for the duration of the migration.", "D": "Write an Apex trigger on the Contact object, before insert event to set the original value in a standard CreatedDate field."}, "answer": ["C"], "explanations": [{"Status": "Correct", "Choice": "C. Enable \"Set Audit Fields\" and assign the permission to the user loading the data for the duration of the migration.", "Rationale": "[cite_start]The **\"Set Audit Fields upon Record Creation\"** permission is the standard tool designed to allow the user loading the data to populate the system-controlled fields like `CreatedDate` and `LastModifiedDate`, preserving the historical timestamps[cite: 27]."}, {"Status": "Incorrect", "Choice": "A. After the data is migrated, perform an update on all records...", "Rationale": "[cite_start]The standard `CreatedDate` field cannot be updated after a record is inserted[cite: 29]."}, {"Status": "Incorrect", "Choice": "B. Create a new field on Contact object to capture the Created Date.", "Rationale": "This is a workaround that bypasses the standard system field, which is used by many native Salesforce features."}, {"Status": "Incorrect", "Choice": "D. Write an Apex trigger on the Contact object, before insert event...", "Rationale": "[cite_start]Apex triggers cannot modify system audit fields like `CreatedDate` unless the executing user has the \"Set Audit Fields\" permission[cite: 33]."}], "meta": {"source": "QuestionBank", "original_number": 2}}, {"id": "Q3", "number": 3, "question": "An architect has been asked to provide error messages when a future date is detected in a custom Birthdate_c field on the Contact object. The client wants the ability to translate the error messages. What are two approaches the architect should use to achieve this solution? Choose 2 answers", "options": {"A": "Implement a third-party validation process with translate functionality.", "B": "Create a trigger on Contact and add an error to the record with a custom label.", "C": "Create a workflow field update to set the standard ErrorMessage_field.", "D": "Create a validation rule and translate the error message with translation workbench."}, "answer": ["B", "D"], "explanations": [{"Status": "Correct", "Choice": "B. Create a trigger on Contact and add an error to the record with a custom label.", "Rationale": "[cite_start]Apex Triggers can perform the necessary conditional check (detecting a future date) and display an error message that uses a **Custom Label**, which is fully supported by the **Translation Workbench**[cite: 43]."}, {"Status": "Correct", "Choice": "D. Create a validation rule and translate the error message with translation workbench.", "Rationale": "[cite_start]A **Validation Rule** is the declarative standard tool for checking date criteria[cite: 45]. [cite_start]The error message defined in the rule is directly supported by the **Translation Workbench**[cite: 46]."}, {"Status": "Incorrect", "Choice": "A. Implement a third-party validation process with translate functionality.", "Rationale": "[cite_start]This is unnecessarily complex, as Salesforce has built-in translation and validation capabilities[cite: 47]."}, {"Status": "Incorrect", "Choice": "C. Create a workflow field update to set the standard ErrorMessage_field.", "Rationale": "There is no standard `ErrorMessage` field that a Workflow Rule can update to prevent a record from saving; [cite_start]Workflow Rules are for post-save automation[cite: 50]."}], "meta": {"source": "QuestionBank", "original_number": 3}}, {"id": "Q4", "number": 4, "question": "What is an advantage of using Custom metadata type over Custom setting?", "options": {"A": "Custom metadata records are not copied from production to sandbox.", "B": "Custom metadata types are available for reporting.", "C": "Custom metadata records are deployable using packages.", "D": "Custom metadata records are editable in Apex."}, "answer": ["C"], "explanations": [{"Status": "Correct", "Choice": "C. Custom metadata records are deployable using packages.", "Rationale": "[cite_start]Custom Metadata Type (CMDT) records are application metadata and **can be packaged and deployed** using Change Sets or the Metadata API[cite: 55]. Custom Settings cannot be deployed this way."}, {"Status": "Incorrect", "Choice": "A. Custom metadata records are not copied from production to sandbox.", "Rationale": "[cite_start]CMDT records *are* copied when creating or refreshing a sandbox[cite: 58]."}, {"Status": "Incorrect", "Choice": "B. Custom metadata types are available for reporting.", "Rationale": "[cite_start]While queryable via SOQL, they are not typically considered available for standard report builder reports[cite: 59]."}, {"Status": "Incorrect", "Choice": "D. Custom metadata records are editable in Apex.", "Rationale": "[cite_start]CMDT records are **read-only** in Apex[cite: 60]."}], "meta": {"source": "QuestionBank", "original_number": 4}}, {"id": "Q5", "number": 5, "question": "Get Cloudy Consulting uses an invoicing system that has specific requirements. One requirement is that attachments associated with the Invoice_c custom object be classified by Types (i.e., \"Purchase Order\", \"Receipt\", etc.) so that reporting can be performed on invoices showing the number of attachments grouped by Type. What should an Architect do to categorize the attachments to fulfill these requirements?", "options": {"A": "Add additional options to the standard ContentType picklist field for the Attachment object.", "B": "Add a ContentType picklist field to the Attachment layout and create additional picklist options.", "C": "Create a custom picklist field for the Type on the standard Attachment object with the values.", "D": "Create a custom object related to the Invoice object with a picklist field for the Type."}, "answer": ["D"], "explanations": [{"Status": "Correct", "Choice": "D. Create a custom object related to the Invoice object with a picklist field for the Type.", "Rationale": "[cite_start]The standard `Attachment` object **cannot be customized** with custom fields[cite: 70]. [cite_start]Creating a custom object as a **junction/wrapper** allows for the categorization picklist and enables reporting on the type of attachment related to the Invoice[cite: 71]."}, {"Status": "Incorrect", "Choice": "A. Add additional options to the standard ContentType picklist field for the Attachment object.", "Rationale": "The standard `ContentType` field is not customizable."}, {"Status": "Incorrect", "Choice": "B. Add a ContentType picklist field to the Attachment layout and create additional picklist options.", "Rationale": "This is impossible as custom fields cannot be added to the standard `Attachment` object."}, {"Status": "Incorrect", "Choice": "C. Create a custom picklist field for the Type on the standard Attachment object with the values.", "Rationale": "[cite_start]Custom fields cannot be created on the standard `Attachment` object[cite: 77]."}], "meta": {"source": "QuestionBank", "original_number": 5}}, {"id": "Q6", "number": 6, "question": "Universal Containers has a legacy system that captures Conferences and Venues. These Conferences can occur at any Venue. They create hundreds of thousands of Conferences per year. Historically, they have only used 20 Venues. Which two things should the data architect consider when denormalizing this data model into a single Conference object with a Venue picklist? Choose 2", "options": {"A": "Limitations on master-detail relationships.", "B": "Org data storage limitations.", "C": "Bulk API limitations on picklist fields.", "D": "Standard list view in-line editing."}, "answer": ["C", "D"], "explanations": [{"Status": "Correct", "Choice": "C. Bulk API limitations on picklist fields.", "Rationale": "[cite_start]**Picklist fields** have specific limitations in Salesforce, including a maximum character limit per value[cite: 84]. This is a concern during high-volume data migration (Bulk API) because inconsistencies or lengthy values can cause load errors."}, {"Status": "Correct", "Choice": "D. Standard list view in-line editing.", "Rationale": "Denormalized data (the Venue name as a static value on the Conference record) is cumbersome to update if the original Venue name changes. [cite_start]**In-line editing** on lists doesn't scale well for mass updates across hundreds of thousands of records[cite: 88]."}, {"Status": "Incorrect", "Choice": "A. Limitations on master-detail relationships.", "Rationale": "[cite_start]The proposed change eliminates the separate Venue object and its relationship, making relationship limitations irrelevant[cite: 89]."}, {"Status": "Incorrect", "Choice": "B. Org data storage limitations.", "Rationale": "[cite_start]Denormalization actually reduces storage (no separate Venue records)[cite: 90]. The negligible storage gain is not the primary design consideration here."}], "meta": {"source": "QuestionBank", "original_number": 6}}, {"id": "Q7", "number": 7, "question": "Universal Container (UC) has around 200,000 Customers (stored in Account object). They get 1 or 2 Orders every month from each Customer. Orders are stored in a custom object called Order_c; this has about 50 fields. UC is expecting a growth of 10% year-over-year. What are two considerations an architect should consider to improve the performance of SOQL queries that retrieve data from the Order_c object? Choose 2 answers", "options": {"A": "Use soql queries without WHERE conditions.", "B": "Work with Salesforce Support to enable Skinny Tables.", "C": "Reduce the number of triggers on Order_c object.", "D": "Make the queries more selective using indexed fields."}, "answer": ["B", "D"], "explanations": [{"Status": "Correct", "Choice": "B. Work with Salesforce Support to enable Skinny Tables.", "Rationale": "[cite_start]**Skinny Tables** are a performance feature that creates an optimized table of frequently-used fields, avoiding resource-intensive joins and significantly improving query performance on large objects[cite: 101]."}, {"Status": "Correct", "Choice": "D. Make the queries more selective using indexed fields.", "Rationale": "**Query Selectivity** is the most critical factor for query performance. [cite_start]Using indexed fields (External IDs, unique fields, or standard fields) in the `WHERE` clause allows the database to find records quickly[cite: 104]."}, {"Status": "Incorrect", "Choice": "A. Use soql queries without WHERE conditions.", "Rationale": "[cite_start]Queries without a restrictive `WHERE` clause scan the entire table, which is the primary cause of poor performance and timeouts[cite: 106]."}, {"Status": "Incorrect", "Choice": "C. Reduce the number of triggers on Order_c object.", "Rationale": "[cite_start]Reducing triggers improves DML (insert/update/delete) performance but does not directly improve the performance of read-only SOQL queries[cite: 108]."}], "meta": {"source": "QuestionBank", "original_number": 7}}, {"id": "Q8", "number": 8, "question": "Universal Containers (UC) provides shipping services to its customers. They use Opportunities to track customer shipments. At any given time, shipping status can be one of the 10 values. UC has 200,000 Opportunity records. When creating a new field to track shipping status on opportunity, what should the architect do to improve data quality and avoid data skew?", "options": {"A": "Create a picklist field, values sorted alphabetically.", "B": "Create a Master-Detail to custom object ShippingStatus_c.", "C": "Create a Lookup to custom object ShippingStatus_c.", "D": "Create a text field and make it an external ID."}, "answer": ["A"], "explanations": [{"Status": "Correct", "Choice": "A. Create a picklist field, values sorted alphabetically.", "Rationale": "[cite_start]A **Picklist** is the declarative tool that ensures data quality by restricting the input to the 10 valid status values, preventing inconsistency[cite: 116]. [cite_start]With only 10 values, data skew is not a concern, and alphabetical sorting is a minor data management best practice[cite: 117]."}, {"Status": "Incorrect", "Choice": "B. Create a Master-Detail to custom object ShippingStatus_c.", "Rationale": "[cite_start]This introduces unnecessary data model complexity (a new object) for a simple status field[cite: 119]."}, {"Status": "Incorrect", "Choice": "C. Create a Lookup to custom object ShippingStatus_c.", "Rationale": "This is over-modeling a simple status field and requires extra steps to enforce data quality."}, {"Status": "Incorrect", "Choice": "D. Create a text field and make it an external ID.", "Rationale": "[cite_start]A **Text field** allows free text entry, directly compromising data quality goals[cite: 123]."}], "meta": {"source": "QuestionBank", "original_number": 8}}, {"id": "Q9", "number": 9, "question": "Universal Containers (UC) management has identified a total of ten text fields on the Contact object as important to capture any changes made to these fields, such as who made the change, when they made the change, what is the old value, and what is the new value. UC needs to be able to report on these field data changes within Salesforce for the past 3 months. What are two approaches that will meet this requirement? Choose 2 answers", "options": {"A": "Create a workflow to evaluate the rule when a record is created and use field update actions to store previous values for these ten fields in ten new fields.", "B": "Write an Apex trigger on Contact after insert event and after update events and store the old values in another custom object.", "C": "Turn on field Contact object history tracking for these ten fields, then create reports on contact history.", "D": "Create a Contact report including these ten fields and Salesforce Id, then schedule the report to run once a day and send email to the admin."}, "answer": ["B", "C"], "explanations": [{"Status": "Correct", "Choice": "B. Write an Apex trigger on Contact after insert event and after update events and store the old values in another custom object.", "Rationale": "[cite_start]**Apex Triggers** are the required **custom** solution to capture and store the full audit history (who, when, old/new value) for an unlimited number of fields in a custom audit object, which is necessary if the field limit is exceeded[cite: 132]."}, {"Status": "Correct", "Choice": "C. Turn on field Contact object history tracking for these ten fields, then create reports on contact history.", "Rationale": "[cite_start]**Field History Tracking** is the standard feature designed for this exact use case[cite: 134]. [cite_start]Since the limit is 20 fields, and the requirement is for 10 fields, this declarative solution is viable[cite: 135]."}, {"Status": "Incorrect", "Choice": "A. Create a workflow to evaluate the rule when a record is created and use field update actions to store previous values...", "Rationale": "[cite_start]Workflow Rules (or Flow) are inefficient for capturing transactional, multi-field history and do not capture who/when reliably[cite: 137]."}, {"Status": "Incorrect", "Choice": "D. Create a Contact report including these ten fields and Salesforce Id...", "Rationale": "[cite_start]Standard reports only show the *current* values of the record; they do not track the history of changes[cite: 139]."}], "meta": {"source": "QuestionBank", "original_number": 9}}, {"id": "Q10", "number": 10, "question": "Universal Containers (UC) has an open sharing model for its Salesforce users to allow all its Salesforce internal users to edit all contacts, regardless of who owns the contact. However, UC management wants to allow only the owner of a contact record to delete that contact. If a user does not own the contact, then the user should not be allowed to delete the record. How should the architect approach the project so that the requirements are met?", "options": {"A": "Create a \"before delete\" trigger to check if the current user is not the owner.", "B": "Set the Sharing settings as Public Read Only for the Contact object.", "C": "Set the profile of the users to remove delete permission from the Contact object.", "D": "Create a validation rule on the Contact object to check if the current user is not the owner."}, "answer": ["A"], "explanations": [{"Status": "Correct", "Choice": "A. Create a \"before delete\" trigger to check if the current user is not the owner.", "Rationale": "Delete permissions are enforced at the profile level (CRUD) and the record level. [cite_start]Since the OWD is open (`Public Read/Write`) allowing everyone to **edit**, the only way to block **deletion** conditionally (non-owner cannot delete) is via a **Before Delete Apex Trigger**[cite: 151]."}, {"Status": "Incorrect", "Choice": "B. Set the Sharing settings as Public Read Only for the Contact object.", "Rationale": "[cite_start]This would prevent non-owners from *editing* the contact, which violates the requirement for an open sharing model allowing all internal users to edit[cite: 153]."}, {"Status": "Incorrect", "Choice": "C. Set the profile of the users to remove delete permission from the Contact object.", "Rationale": "[cite_start]This would prevent *all* users, including the owner, from deleting the record, violating the requirement that the owner be allowed to delete[cite: 155]."}, {"Status": "Incorrect", "Choice": "D. Create a validation rule on the Contact object to check if the current user is not the owner.", "Rationale": "[cite_start]**Validation Rules cannot be used to block delete operations**; they only fire on insert/update[cite: 157]."}], "meta": {"source": "QuestionBank", "original_number": 10}}, {"id": "Q11", "number": 11, "question": "Universal Containers (UC) uses Salesforce for tracking opportunities (Opportunity). UC uses an internal ERP system for tracking deliveries and invoicing. The ERP system supports SOAP API and OData for bi-directional integration between Salesforce and the ERP system. UC has about one million opportunities. For each opportunity, UC sends 12 invoices, one per month. UC sales reps have requirements to view current invoice status and invoice amount from the opportunity page. When creating an object to model invoices, what should the architect recommend, considering performance and data storage space?", "options": {"A": "Use Streaming API to get the current status from the ERP and display on the Opportunity page.", "B": "Create an external object Invoice_x with a Lookup relationship with Opportunity.", "C": "Create a custom object Invoice_c with a master-detail relationship with Opportunity.", "D": "Create a custom object Invoice_c with a Lookup relationship with Opportunity."}, "answer": ["B"], "explanations": [{"Status": "Correct", "Choice": "B. Create an external object Invoice_x with a Lookup relationship with Opportunity.", "Rationale": "[cite_start]The total volume is large (**12 million records**) [cite: 170] and the data is mastered externally (ERP). [cite_start]**External Objects** (Salesforce Connect) are the designed solution to avoid **data storage** costs while providing real-time access to the external system's data via OData[cite: 171]."}, {"Status": "Incorrect", "Choice": "A. Use Streaming API to get the current status from the ERP and display on the Opportunity page.", "Rationale": "[cite_start]Streaming API is for event notification, not for querying and displaying lists of external data on a page[cite: 173]."}, {"Status": "Incorrect", "Choice": "C. Create a custom object Invoice_c with a master-detail relationship with Opportunity.", "Rationale": "[cite_start]Storing 12 million records in a custom object would exceed standard limits and be prohibitively expensive[cite: 175]."}, {"Status": "Incorrect", "Choice": "D. Create a custom object Invoice_c with a Lookup relationship with Opportunity.", "Rationale": "[cite_start]This also fails due to the **massive data storage** consumption for 12 million records[cite: 177]."}], "meta": {"source": "QuestionBank", "original_number": 11}}, {"id": "Q12", "number": 12, "question": "Universal Containers has a large number of Opportunity fields (100) that they want to track field history on. Which two actions should an architect perform in order to meet this requirement? Choose 2 answers", "options": {"A": "Create a custom object to store a copy of the record when changed.", "B": "Create a custom object to store the previous and new field values.", "C": "Use Analytic Snapshots to store a copy of the record when changed.", "D": "Select the 100 fields in the Opportunity Set History Tracking page."}, "answer": ["A", "B"], "explanations": [{"Status": "Correct", "Choice": "A. Create a custom object to store a copy of the record when changed.", "Rationale": "[cite_start]**Native Field History Tracking is limited to 20 fields**[cite: 186]. The architectural solution for tracking more than 20 fields is to use an **Apex Trigger** to capture the changes and store them in a custom audit object, either as a copy of the whole record or as individual change entries."}, {"Status": "Correct", "Choice": "B. Create a custom object to store the previous and new field values.", "Rationale": "This is the other accepted approach for custom history tracking. It is implemented via an **Apex Trigger** that writes records (including Old Value, New Value, and Field Name) to a custom audit object, which is necessary to capture changes for all 100 fields."}, {"Status": "Incorrect", "Choice": "C. Use Analytic Snapshots to store a copy of the record when changed.", "Rationale": "[cite_start]Analytic Snapshots are for capturing **periodic aggregate or report data** for historical reporting, not for granular, transactional tracking of every field change[cite: 192]."}, {"Status": "Incorrect", "Choice": "D. Select the 100 fields in the Opportunity Set History Tracking page.", "Rationale": "[cite_start]This is technically impossible due to the hard limit of **20 fields**[cite: 194]."}], "meta": {"source": "QuestionBank", "original_number": 12}}, {"id": "Q13", "number": 13, "question": "DreamHouse Realty has a Salesforce org that is used to manage Contacts. What are two things an Architect should consider using to maintain data quality in this situation? (Choose two.)", "options": {"A": "Use the private sharing model.", "B": "Use Salesforce duplicate management.", "C": "Use validation rules on new record create and edit.", "D": "Use workflow to delete duplicate records."}, "answer": ["B", "C"], "explanations": [{"Status": "Correct", "Choice": "B. Use Salesforce duplicate management.", "Rationale": "[cite_start]**Duplicate Management** (Matching and Duplicate Rules) is the primary tool for **proactively preventing** data quality issues by identifying and blocking duplicate records[cite: 200]."}, {"Status": "Correct", "Choice": "C. Use validation rules on new record create and edit.", "Rationale": "[cite_start]**Validation Rules** enforce data completeness and correctness (integrity) by preventing a record from being saved unless the data meets specified criteria[cite: 202]."}, {"Status": "Incorrect", "Choice": "A. Use the private sharing model.", "Rationale": "[cite_start]The **Sharing Model** controls **record visibility** (security), not the **quality** or integrity of the data itself[cite: 204]."}, {"Status": "Incorrect", "Choice": "D. Use workflow to delete duplicate records.", "Rationale": "[cite_start]Automated deletion of records is highly discouraged as an architectural pattern due to the high risk of data loss[cite: 206]."}], "meta": {"source": "QuestionBank", "original_number": 13}}, {"id": "Q14", "number": 14, "question": "Universal Containers is looking to use Salesforce to manage their sales organization. They will be migrating legacy account data from two aging systems into Salesforce. Which two design considerations should an architect take to minimize data duplication? Choose 2 answers", "options": {"A": "Use a workflow to check and prevent duplicates.", "B": "Clean data before importing to Salesforce.", "C": "Use Salesforce matching and duplicate rules.", "D": "Import the data concurrently."}, "answer": ["B", "C"], "explanations": [{"Status": "Correct", "Choice": "B. Clean data before importing to Salesforce.", "Rationale": "[cite_start]**Cleansing, standardizing, and de-duplicating** the source data **before migration (ETL)** is the most critical step to minimize the entry of duplicates into the new system[cite: 214]."}, {"Status": "Correct", "Choice": "C. Use Salesforce matching and duplicate rules.", "Rationale": "[cite_start]**Matching and Duplicate Rules** are the native features designed to **proactively prevent** new duplicates from being created during ongoing manual entry and subsequent batch loads[cite: 216]."}, {"Status": "Incorrect", "Choice": "A. Use a workflow to check and prevent duplicates.", "Rationale": "Workflow Rules are not the appropriate tool for duplicate checking; that function belongs to Duplicate Rules."}, {"Status": "Incorrect", "Choice": "D. Import the data concurrently.", "Rationale": "Concurrent (parallel) loading increases **speed** but has no effect on **data duplication**. It can increase the risk of race conditions if not managed carefully."}], "meta": {"source": "QuestionBank", "original_number": 14}}, {"id": "Q15", "number": 15, "question": "Universal Containers (UC) has a Salesforce instance with over 10.000 Account records. They have noticed similar, but not identical, Account names and addresses. What should UC do to ensure proper data quality?", "options": {"A": "Use a service to standardize Account addresses, then use a 3rd-party tool to merge Accounts based on rules.", "B": "Run a report, find Accounts whose name starts with the same five characters, then merge those Accounts.", "C": "Enable Account de-duplication by creating matching rules in Salesforce, which will mass merge duplicate Accounts.", "D": "Make the Account Owner clean their Accounts\u2019 addresses, then merge Accounts with the same address."}, "answer": ["C"], "explanations": [{"Status": "Correct", "Choice": "C. Enable Account de-duplication by creating matching rules in Salesforce, which will mass merge duplicate Accounts.", "Rationale": "[cite_start]**Salesforce Duplicate Management** (Matching Rules) is the native, scalable solution designed to identify \"similar, but not identical\" records[cite: 228]. [cite_start]Once identified, these duplicates can be merged, ensuring proper data quality[cite: 229]."}, {"Status": "Incorrect", "Choice": "A. Use a service to standardize Account addresses, then use a 3rd-party tool to merge Accounts based on rules.", "Rationale": "This is a complex solution. [cite_start]The native Salesforce features are the architect's preferred first recommendation[cite: 231]."}, {"Status": "Incorrect", "Choice": "B. Run a report, find Accounts whose name starts with the same five characters, then merge those Accounts.", "Rationale": "[cite_start]This is a manual, non-scalable process that relies on arbitrary matching criteria[cite: 233]."}, {"Status": "Incorrect", "Choice": "D. Make the Account Owner clean their Accounts\u2019 addresses, then merge Accounts with the same address.", "Rationale": "[cite_start]Relying on manual effort is not a scalable or sustainable solution for ensuring data quality across thousands of records[cite: 235]."}], "meta": {"source": "QuestionBank", "original_number": 15}}, {"id": "Q16", "number": 16, "question": "Cloud Kicks stores Invoice records in a custom object. Invoice records are being sent to the Accounting department with missing States and incorrectly formatted Postal Codes. Which two actions should Cloud Kicks take to improve data quality? (Choose two.)", "options": {"A": "Change each address field to required on the Page Layout.", "B": "Write an Apex Trigger to require all fields to be populated.", "C": "Utilize a Validation Rule with a REGEX operator on Postal Code.", "D": "Utilize a Validation Rule with a CONTAINS operator on address fields."}, "answer": ["C", "D"], "explanations": [{"Status": "Correct", "Choice": "C. Utilize a Validation Rule with a REGEX operator on Postal Code.", "Rationale": "A **Validation Rule** is the declarative tool to enforce data quality upon saving. [cite_start]The **REGEX (Regular Expression)** function is the most precise way to ensure a Postal Code adheres to a required format (e.g., 5 digits or standard international formats)[cite: 246]."}, {"Status": "Correct", "Choice": "D. Utilize a Validation Rule with a CONTAINS operator on address fields.", "Rationale": "**Validation Rules** are used to check for consistency and completeness. [cite_start]A rule checking if the State field is populated (completeness) or using `CONTAINS()` to ensure consistency is the simplest declarative approach[cite: 249]."}, {"Status": "Incorrect", "Choice": "A. Change each address field to required on the Page Layout.", "Rationale": "[cite_start]Page layout settings are ignored by the API and do not check the *format* of the data[cite: 251]."}, {"Status": "Incorrect", "Choice": "B. Write an Apex Trigger to require all fields to be populated.", "Rationale": "[cite_start]A Validation Rule is the simple, low-code, and preferred method for enforcing basic data format and required fields[cite: 253]."}], "meta": {"source": "QuestionBank", "original_number": 16}}, {"id": "Q17", "number": 17, "question": "Universal Containers (UC) has multi-level account hierarchies that represent departments within their major Accounts. Users are creating duplicate Contacts across multiple departments. UC wants to clean the data so as to have a single Contact across departments. What two solutions should UC implement to cleanse their data? Choose 2 answers", "options": {"A": "Make use of a third-party tool to help merge duplicate Contacts across Accounts.", "B": "Use Data.com to standardize Contact address information to help identify duplicates.", "C": "Use Workflow rules to standardize Contact information to identify and prevent duplicates.", "D": "Make use of the Merge Contacts feature of Salesforce to merge duplicates for an Account."}, "answer": ["A", "B"], "explanations": [{"Status": "Correct", "Choice": "A. Make use of a third-party tool to help merge duplicate Contacts across Accounts.", "Rationale": "Salesforce's native Contact Merge tool only merges contacts under the *same* Account. [cite_start]Since the duplicates are spread **across multiple Accounts**, a third-party tool is necessary to safely find and merge them[cite: 264]."}, {"Status": "Correct", "Choice": "B. Use Data.com to standardize Contact address information to help identify duplicates.", "Rationale": "[cite_start]**Data standardization and enrichment** (via Data.com or similar services) is crucial because inconsistent data (e.g., nicknames, abbreviations) prevents matching rules from working effectively[cite: 266]."}, {"Status": "Incorrect", "Choice": "C. Use Workflow rules to standardize Contact information to identify and prevent duplicates.", "Rationale": "[cite_start]Workflow Rules are deprecated and ineffective for complex data standardization and cleaning[cite: 268]."}, {"Status": "Incorrect", "Choice": "D. Make use of the Merge Contacts feature of Salesforce to merge duplicates for an Account.", "Rationale": "[cite_start]This feature only works for contacts belonging to one Account[cite: 270]."}], "meta": {"source": "QuestionBank", "original_number": 17}}, {"id": "Q18", "number": 18, "question": "Universal Containers has defined a new Data Quality Plan for their Salesforce data and wants to know how they can enforce it throughout the organization. Which two approaches should an architect recommend to enforce this new plan? Choose 2 answers", "options": {"A": "Schedule a weekly dashboard displaying records that are missing information to be sent to managers for review.", "B": "Use Workflow, Validation Rules, and Force.com code (Apex) to enforce critical business processes.", "C": "Schedule reports that will automatically catch duplicates and merge or delete the records every week.", "D": "Store all data in an external system and set up an integration to Salesforce for view-only access."}, "answer": ["A", "B"], "explanations": [{"Status": "Correct", "Choice": "A. Schedule a weekly dashboard displaying records that are missing information to be sent to managers for review.", "Rationale": "[cite_start]A dashboard that highlights data quality issues and is sent to **managers** creates accountability and a crucial feedback loop, which is essential for **enforcing** the plan[cite: 279]."}, {"Status": "Correct", "Choice": "B. Use Workflow, Validation Rules, and Force.com code (Apex) to enforce critical business processes.", "Rationale": "This is the architectural foundation for **enforcement**. [cite_start]**Validation Rules** enforce data integrity on save, and **Workflow/Apex** enforce the standardized business processes[cite: 281]."}, {"Status": "Incorrect", "Choice": "C. Schedule reports that will automatically catch duplicates and merge or delete the records every week.", "Rationale": "[cite_start]Automated merging or deletion is too high-risk to be recommended as a standard process for enforcement[cite: 283]."}, {"Status": "Incorrect", "Choice": "D. Store all data in an external system and set up an integration to Salesforce for view-only access.", "Rationale": "[cite_start]This is an integration strategy that does not address how to **enforce** data quality on the data created within Salesforce[cite: 285]."}], "meta": {"source": "QuestionBank", "original_number": 18}}, {"id": "Q19", "number": 19, "question": "Universal Containers wants to implement a data-quality process to monitor the data that users are manually entering into the system through the Salesforce UI. Which approach should the architect, recommend?", "options": {"A": "Allow users to import their data using the Salesforce Import tools.", "B": "Utilize a 3rd-party solution from the AppExchange for data uploads.", "C": "Utilize an app from the AppExchange to create data-quality dashboards.", "D": "Use Apex to validate the format of phone numbers and postal codes."}, "answer": ["C"], "explanations": [{"Status": "Correct", "Choice": "C. Utilize an app from the AppExchange to create data-quality dashboards.", "Rationale": "The requirement is to **monitor and measure** the quality of manually entered data. [cite_start]AppExchange **Data Quality Dashboard apps** provide specialized, pre-built reports and metrics (completeness, accuracy, trending) that are superior to custom native reports[cite: 294]."}, {"Status": "Incorrect", "Choice": "A. Allow users to import their data using the Salesforce Import tools.", "Rationale": "[cite_start]This addresses data loading, not the monitoring of manual data entry[cite: 296]."}, {"Status": "Incorrect", "Choice": "B. Utilize a 3rd-party solution from the AppExchange for data uploads.", "Rationale": "This also addresses data loading, not the monitoring of manual data entry."}, {"Status": "Incorrect", "Choice": "D. Use Apex to validate the format of phone numbers and postal codes.", "Rationale": "[cite_start]Apex (or Validation Rules) is for **enforcing** quality (prevention), not for **monitoring** the quality of data that has already been entered[cite: 300]."}], "meta": {"source": "QuestionBank", "original_number": 19}}, {"id": "Q20", "number": 20, "question": "Which two approaches should the manager take to achieve this goal? (Choose two.)", "options": {"A": "Acquire an AppExchange Lead de-duplication application.", "B": "Implement Salesforce Matching and Duplicate Rules.", "C": "Run the Salesforce Lead Mass de-duplication tool.", "D": "Create a Workflow Rule to check for duplicate records."}, "answer": ["A", "B"], "explanations": [{"Status": "Correct", "Choice": "A. Acquire an AppExchange Lead de-duplication application.", "Rationale": "[cite_start]AppExchange solutions often offer advanced matching algorithms and bulk de-duplication capabilities, making them a strong recommendation for data quality issues[cite: 306]."}, {"Status": "Correct", "Choice": "B. Implement Salesforce Matching and Duplicate Rules.", "Rationale": "[cite_start]**Matching and Duplicate Rules** are the native, declarative features designed to **proactively avoid** the creation of duplicate records during the import process[cite: 308]."}, {"Status": "Incorrect", "Choice": "C. Run the Salesforce Lead Mass de-duplication tool.", "Rationale": "[cite_start]There is no single native \"Lead Mass de-duplication tool\"[cite: 310]."}, {"Status": "Incorrect", "Choice": "D. Create a Workflow Rule to check for duplicate records.", "Rationale": "[cite_start]Workflow Rules are deprecated and cannot effectively check for and prevent duplicates; that is the function of Duplicate Rules[cite: 312]."}], "meta": {"source": "QuestionBank", "original_number": 20}}, {"id": "Q21", "number": 21, "question": "DreamHouse Realty has an integration that creates records in a Salesforce Custom Object. The Custom Object has a field marked as required on the page layout. DreamHouse Realty has noticed that many of the records coming from the external system are missing data in this field. The Architect needs to ensure this field always contains data coming from the source system. Which two approaches should the Architect take? Choose 2 answers", "options": {"A": "Set up a Validation Rule to prevent blank values.", "B": "Create a Workflow to default a value into this field.", "C": "Mark the field as required in setup at the field level.", "D": "Blame the customer's external system for bad data."}, "answer": ["A", "C"], "explanations": [{"Status": "Correct", "Choice": "A. Set up a Validation Rule to prevent blank values.", "Rationale": "[cite_start]A **Validation Rule** is necessary to enforce data quality when data is coming from the **API/integration**[cite: 321]. It fires on every save, regardless of source, preventing the record from being saved if the field is blank."}, {"Status": "Correct", "Choice": "C. Mark the field as required in setup at the field level.", "Rationale": "[cite_start]Marking a field as required at the **field definition level** enforces the requirement at the **database/API level** (not just the page layout) and is the simplest architectural method to enforce value presence[cite: 324]."}, {"Status": "Incorrect", "Choice": "B. Create a Workflow to default a value into this field.", "Rationale": "The architect should enforce the data quality requirement on the source system; masking bad data with a default value is a poor solution."}, {"Status": "Incorrect", "Choice": "D. Blame the customer's external system for bad data.", "Rationale": "[cite_start]This is not an architectural approach; the architect must implement controls in Salesforce to mitigate bad data[cite: 329]."}], "meta": {"source": "QuestionBank", "original_number": 21}}, {"id": "Q22", "number": 22, "question": "Universal Containers has two systems: Salesforce and an on-premise ERP system. An architect has been tasked with copying Opportunity records to the ERP once they reach a Closed/Won Stage. The Opportunity record in the ERP system will be read-only for all fields copied in from Salesforce. What is the optimal real-time approach that achieves this solution?", "options": {"A": "Implement a Master Data Management system to determine system of record.", "B": "Implement a workflow rule that sends Opportunity data through Outbound Messaging.", "C": "Have the ERP poll Salesforce nightly and bring in the desired Opportunities.", "D": "Implement an hourly integration to send Salesforce Opportunities to the ERP system."}, "answer": ["B"], "explanations": [{"Status": "Correct", "Choice": "B. Implement a workflow rule that sends Opportunity data through Outbound Messaging.", "Rationale": "[cite_start]**Outbound Messaging** is the native, declarative, and optimal mechanism for a **real-time push** of data to an external system (via a WSDL-defined endpoint) when a record change (Stage = Closed/Won) occurs[cite: 338]."}, {"Status": "Incorrect", "Choice": "A. Implement a Master Data Management system to determine system of record.", "Rationale": "[cite_start]MDM is for data governance/reconciliation, not for triggering a single-direction, real-time data copy[cite: 340]."}, {"Status": "Incorrect", "Choice": "C. Have the ERP poll Salesforce nightly and bring in the desired Opportunities.", "Rationale": "[cite_start]Polling nightly is a **batch** or **scheduled** approach, not a **real-time** approach[cite: 342]."}, {"Status": "Incorrect", "Choice": "D. Implement an hourly integration to send Salesforce Opportunities to the ERP system.", "Rationale": "[cite_start]An hourly integration is a **scheduled** approach, not a **real-time** approach[cite: 344]."}], "meta": {"source": "QuestionBank", "original_number": 22}}, {"id": "Q23", "number": 23, "question": "Universal Containers (UC) has three systems: Salesforce, a cloud-based ERP system, and an on-premise Order Management System (OMS). An architect has been tasked with creating a solution that uses Salesforce as the system of record for Leads and the OMS as the system of record for Account and Contacts. UC wants Accounts and Contacts to be able to maintain their names in each system (i.e., \"John Doe\" in the OMS and \"Johnny Doe\" in Salesforce), but wants to have a consolidated data store which links referenced records across the systems. What approach should an architect suggest so the requirements are met?", "options": {"A": "Have Salesforce poll the OMS nightly and bring in the desired Accounts and Contacts.", "B": "Implement an integration tool to send OMS Accounts and Contacts to Salesforce.", "C": "Implement a Master Data Management strategy to reconcile Leads, Accounts, and Contacts.", "D": "Use the Streaming API to send Account and Contact data from Salesforce to the OMS."}, "answer": ["C"], "explanations": [{"Status": "Correct", "Choice": "C. Implement a Master Data Management strategy to reconcile Leads, Accounts, and Contacts.", "Rationale": "[cite_start]This is the only solution that addresses the complexity of **multiple systems of record (SOR)** and the need for **reconciliation** (linking \"John Doe\" to \"Johnny Doe\") and a **consolidated linking data store**[cite: 355]. **MDM** is specifically designed for these cross-system governance challenges."}, {"Status": "Incorrect", "Choice": "A. Have Salesforce poll the OMS nightly and bring in the desired Accounts and Contacts.", "Rationale": "[cite_start]This addresses data sync but does not solve the fundamental problem of **reconciliation** or cross-system ID linkage[cite: 358]."}, {"Status": "Incorrect", "Choice": "B. Implement an integration tool to send OMS Accounts and Contacts to Salesforce.", "Rationale": "[cite_start]This would duplicate records in Salesforce and not solve the cross-system reconciliation challenge[cite: 360]."}, {"Status": "Incorrect", "Choice": "D. Use the Streaming API to send Account and Contact data from Salesforce to the OMS.", "Rationale": "[cite_start]Streaming API is for event notification, not for implementing a reconciliation/data governance strategy across multiple systems[cite: 362]."}], "meta": {"source": "QuestionBank", "original_number": 23}}, {"id": "Q24", "number": 24, "question": "An architect has been asked by a client to develop a solution that will integrate data and resolve duplicates and discrepancies between Salesforce and one or more external systems. What two factors should the architect take into consideration when deciding whether or not to use a Master Data Management system to achieve this solution? Choose 2 answers", "options": {"A": "Whether the systems are cloud-based or on-premise.", "B": "Whether or not Salesforce replaced a legacy CRM.", "C": "Whether the system of record changes for different tables.", "D": "The number of systems that are integrating with each other."}, "answer": ["C", "D"], "explanations": [{"Status": "Correct", "Choice": "C. Whether the system of record changes for different tables.", "Rationale": "[cite_start]**Complexity in data ownership** (SOR changing by object, record, or field) is the primary driver for an MDM solution, as it manages the logic for data survivorship[cite: 370]."}, {"Status": "Correct", "Choice": "D. The number of systems that are integrating with each other.", "Rationale": "[cite_start]MDM is highly valuable when **three or more systems** are integrating, as it simplifies the integration topology (hub-and-spoke model) and manages cross-system data integrity centrally[cite: 372]."}, {"Status": "Incorrect", "Choice": "A. Whether the systems are cloud-based or on-premise.", "Rationale": "This is a technical detail for integration tools but does not determine the architectural need for MDM."}, {"Status": "Incorrect", "Choice": "B. Whether or not Salesforce replaced a legacy CRM.", "Rationale": "This addresses the *context* but not the *necessity* for an MDM solution."}], "meta": {"source": "QuestionBank", "original_number": 24}}, {"id": "Q25", "number": 25, "question": "Get Cloud Consulting needs to integrate two different systems with customer records into the Salesforce Account object. So that no duplicate records are created in Salesforce, Master Data Management will be used. An Architect needs to determine which system is the system of record on a field level. What should the Architect do to achieve this goal?", "options": {"A": "Master Data Management systems determine system of record, and the Architect doesn't have to think about what data is controlled by what system.", "B": "Key stakeholders should review any fields that share the same purpose between systems to see how they will be used in Salesforce.", "C": "The database schema for each external system should be reviewed, and fields with different names should always be separate fields in Salesforce.", "D": "Any field that is an input field in either external system will be overwritten by the last record integrated and can never have a system of record."}, "answer": ["C"], "explanations": [{"Status": "Correct", "Choice": "C. The database schema for each external system should be reviewed, and fields with different names should always be separate fields in Salesforce.", "Rationale": "[cite_start](NOTE: While choice B is the more accurate definition of the business process, **C** is a critical, mandatory **initial step** for the architect to perform: reviewing the schemas [cite: 386] to understand the data before defining the system of record (SOR) rules.)"}, {"Status": "Incorrect", "Choice": "A. Master Data Management systems determine system of record, and the Architect doesn't have to think about what data is controlled by what system.", "Rationale": "[cite_start]The business, not the MDM tool, determines the System of Record rules[cite: 388]."}, {"Status": "Incorrect", "Choice": "B. Key stakeholders should review any fields that share the same purpose between systems to see how they will be used in Salesforce.", "Rationale": "[cite_start]This is the **architecturally correct** next step, as it involves the business in defining **field survivorship** rules[cite: 390]."}, {"Status": "Incorrect", "Choice": "D. Any field that is an input field in either external system will be overwritten by the last record integrated and can never have a system of record.", "Rationale": "[cite_start]MDM solutions are specifically designed to prevent the \"last-write-wins\" problem by enforcing the SOR[cite: 392]."}], "meta": {"source": "QuestionBank", "original_number": 25}}, {"id": "Q26", "number": 26, "question": "Salesforce is being deployed in Ursa Major Solar's disparate, multi-system ERP environment. Ursa major Solar wants to maintain data synchronization between systems. Which two techniques should be used to achieve this goal? (Choose two.)", "options": {"A": "Integrate Salesforce with the ERP environment.", "B": "Utilize workbench to update files within systems.", "C": "Utilize an MDM strategy to outline a single source of truth.", "D": "Build synchronization reports and dashboards."}, "answer": ["A", "C"], "explanations": [{"Status": "Correct", "Choice": "A. Integrate Salesforce with the ERP environment.", "Rationale": "[cite_start]**Integration** is the fundamental technical action required to move data between Salesforce and the ERP systems to achieve synchronization[cite: 398]."}, {"Status": "Correct", "Choice": "C. Utilize an MDM strategy to outline a single source of truth.", "Rationale": "[cite_start]In a multi-system environment, a **Master Data Management (MDM) strategy** is required to define the authoritative **System of Record (SOR)** for each data attribute, which is essential for consistent synchronization[cite: 400]."}, {"Status": "Incorrect", "Choice": "B. Utilize workbench to update files within systems.", "Rationale": "Workbench is a developer tool and not a scalable, automated solution for ongoing data synchronization."}, {"Status": "Incorrect", "Choice": "D. Build synchronization reports and dashboards.", "Rationale": "[cite_start]Reports and dashboards can *monitor* synchronization but do not *achieve* or *perform* the data synchronization itself[cite: 403]."}], "meta": {"source": "QuestionBank", "original_number": 26}}, {"id": "Q27", "number": 27, "question": "All accounts and opportunities are created in Salesforce. Salesforce is integrated with three systems: * An ERP system feeds order data into Salesforce and updates both Account and Opportunity records. * An accounting system feeds invoice data into Salesforce and updates both Account and Opportunity records. * A commission system feeds commission data into Salesforce and updates both Account and Opportunity records. How should the architect determine which of these systems is the system of record?", "options": {"A": "Account and opportunity data originates in Salesforce, and therefore Salesforce is the system of record.", "B": "Whatever system updates the attribute or object should be the system of record for that field or object.", "C": "Whatever integration data flow runs last will, by default, determine which system is the system of record.", "D": "Data flows should be reviewed with the business users to determine the system of record per object or field."}, "answer": ["D"], "explanations": [{"Status": "Correct", "Choice": "D. Data flows should be reviewed with the business users to determine the system of record per object or field.", "Rationale": "[cite_start]Determining the **System of Record (SOR)** is a **business governance decision**, not a technical one[cite: 415]. The architect must work with stakeholders to define which system has the ultimate authority over a specific field or object."}, {"Status": "Incorrect", "Choice": "A. Account and opportunity data originates in Salesforce, and therefore Salesforce is the system of record.", "Rationale": "[cite_start]SOR is defined by authority over data maintenance, not solely by the creation point[cite: 418]."}, {"Status": "Incorrect", "Choice": "B. Whatever system updates the attribute or object should be the system of record for that field or object.", "Rationale": "[cite_start]This describes a chaotic \"last write wins\" pattern, not a structured SOR strategy[cite: 420]."}, {"Status": "Incorrect", "Choice": "C. Whatever integration data flow runs last will, by default, determine which system is the system of record.", "Rationale": "[cite_start]This describes an unmanaged conflict, not a process for determining the SOR[cite: 422]."}], "meta": {"source": "QuestionBank", "original_number": 27}}, {"id": "Q28", "number": 28, "question": "What are two valid metadata types that should be included? (Choose two.)", "options": {"A": "RecordType", "B": "Document", "C": "CustomField", "D": "SecuritySettings"}, "answer": ["A", "C"], "explanations": [{"Status": "Correct", "Choice": "A. RecordType", "Rationale": "**Record Types** are crucial for documenting data architecture as they segment objects for different business processes and page layouts."}, {"Status": "Correct", "Choice": "C. CustomField", "Rationale": "**Custom Fields** define the schema of the data model and are the foundational elements of any data architecture documentation."}, {"Status": "Incorrect", "Choice": "B. Document", "Rationale": "The standard `Document` object is for storing files; it is not a key metadata type for documenting data architecture itself."}, {"Status": "Incorrect", "Choice": "D. SecuritySettings", "Rationale": "This is not a formal Metadata Type; security settings relate to **Security Architecture** or **Access Control**."}], "meta": {"source": "QuestionBank", "original_number": 28}}, {"id": "Q29", "number": 29, "question": "An Architect needs to document the data architecture for a multi-system, enterprise Salesforce implementation. Which two key artifacts should the Architect use? (Choose two.)", "options": {"A": "User stories", "B": "Data model", "C": "Integration specification", "D": "Non-functional requirements"}, "answer": ["B", "C"], "explanations": [{"Status": "Correct", "Choice": "B. Data model", "Rationale": "[cite_start]A **Data Model** (ERD) is the foundational artifact for documenting the objects, fields, and relationships that make up the data architecture[cite: 435]."}, {"Status": "Correct", "Choice": "C. Integration specification", "Rationale": "[cite_start]In a multi-system enterprise implementation, the **Integration Specification** is essential as it defines the data flows, system of record, and cross-system linkage between Salesforce and external systems[cite: 436]."}, {"Status": "Incorrect", "Choice": "A. User stories", "Rationale": "User Stories belong to the **Requirements** documentation and inform the architecture, but are not a core artifact of the final architecture itself."}, {"Status": "Incorrect", "Choice": "D. Non-functional requirements", "Rationale": "Non-Functional Requirements (NFRs) drive the design but are not a core artifact of the data architecture itself."}], "meta": {"source": "QuestionBank", "original_number": 29}}, {"id": "Q30", "number": 30, "question": "As part of a phased Salesforce rollout, there will be 3 deployments spread out over the year. The requirements have been carefully documented. Which two methods should an architect use to trace back configuration changes to the detailed requirements? Choose 2 answers", "options": {"A": "Review the setup audit trail for configuration changes.", "B": "Put the business purpose in the Description of each field.", "C": "Maintain a data dictionary with the justification for each field.", "D": "Use the Force.com IDE to save the metadata files in source control."}, "answer": ["B", "C"], "explanations": [{"Status": "Correct", "Choice": "B. Put the business purpose in the Description of each field.", "Rationale": "[cite_start]Associating the **Business Purpose/Justification** directly within the field's `Description` metadata allows an auditor to trace the configuration change back to the original documented requirement[cite: 446]."}, {"Status": "Correct", "Choice": "C. Maintain a data dictionary with the justification for each field.", "Rationale": "[cite_start]A **Data Dictionary** is the centralized, controlled document that lists every field, its properties, and, crucially, its **justification/requirement linkage**[cite: 448]."}, {"Status": "Incorrect", "Choice": "A. Review the setup audit trail for configuration changes.", "Rationale": "[cite_start]The Setup Audit Trail tracks *who* made *what* change and *when*, but not the **justification** or **original requirement** for the change[cite: 450]."}, {"Status": "Incorrect", "Choice": "D. Use the Force.com IDE to save the metadata files in source control.", "Rationale": "Source control tracks *when* and *by whom* code/metadata was deployed, but not the business justification."}], "meta": {"source": "QuestionBank", "original_number": 30}}, {"id": "Q31", "number": 31, "question": "How can an architect find information about who is creating, changing, or deleting certain fields within the past two months?", "options": {"A": "Remove \"customize application\" permissions from everyone else.", "B": "Export the metadata and search it for the fields in question.", "C": "Create a field history report for the fields in question.", "D": "Export the setup audit trail and find the fields in question."}, "answer": ["D"], "explanations": [{"Status": "Correct", "Choice": "D. Export the setup audit trail and find the fields in question.", "Rationale": "[cite_start]The **Setup Audit Trail** tracks all **metadata changes** (creation, modification, deletion of objects, fields, etc.), including the user who made the change and the timestamp[cite: 459]."}, {"Status": "Incorrect", "Choice": "A. Remove \"customize application\" permissions from everyone else.", "Rationale": "This is a security action and does not help find *historical* information."}, {"Status": "Incorrect", "Choice": "B. Export the metadata and search it for the fields in question.", "Rationale": "Exporting metadata gives you the *current state* of the fields, but not the *history* of who changed them."}, {"Status": "Incorrect", "Choice": "C. Create a field history report for the fields in question.", "Rationale": "[cite_start]**Field History Tracking** tracks *data changes* on the *record* (e.g., a Contact Name changed), not changes to the *field definition* (e.g., a field was deleted)[cite: 465]."}], "meta": {"source": "QuestionBank", "original_number": 31}}, {"id": "Q32", "number": 32, "question": "Universal Containers wants to automatically archive all inactive Account data that is older than 3 years. The information does not need to remain accessible within the application. Which two methods should be recommended to meet this requirement? Choose 2 answers", "options": {"A": "Use the Force.com Workbench to export the data.", "B": "Schedule a weekly export file from the Salesforce UI.", "C": "Schedule jobs to export and delete using an ETL tool.", "D": "Schedule jobs to export and delete using the Data Loader."}, "answer": ["C", "D"], "explanations": [{"Status": "Correct", "Choice": "C. Schedule jobs to export and delete using an ETL tool.", "Rationale": "An **ETL tool** is suitable for high-volume, automated processes. [cite_start]It can be scheduled to **Export** (Archive) the data and then execute a **Hard Delete**[cite: 469]."}, {"Status": "Correct", "Choice": "D. Schedule jobs to export and delete using the Data Loader.", "Rationale": "[cite_start]The **Data Loader** (or Data Loader CLI) can be configured to run automated batch jobs to **Export** the data and then execute a subsequent operation for **Hard Delete**[cite: 470]."}, {"Status": "Incorrect", "Choice": "A. Use the Force.com Workbench to export the data.", "Rationale": "[cite_start]Workbench is a developer tool and is not suitable for scheduled, automated, high-volume production archiving/purging[cite: 471]."}, {"Status": "Incorrect", "Choice": "B. Schedule a weekly export file from the Salesforce UI.", "Rationale": "[cite_start]The native \"Weekly Export\" feature only exports data; it does **not** perform the required **deletion** operation[cite: 473]."}], "meta": {"source": "QuestionBank", "original_number": 32}}, {"id": "Q33", "number": 33, "question": "Cloud Kicks needs to purge detailed transactional records from Salesforce. The data should be aggregated at a summary level and available in Salesforce. What are two automated approaches to fulfill this goal? (Choose two.)", "options": {"A": "Third-party Integration Tool (ETL)", "B": "Schedulable Batch Apex", "C": "Third-party Business Intelligence system", "D": "Apex Triggers"}, "answer": ["A", "B"], "explanations": [{"Status": "Correct", "Choice": "A. Third-party Integration Tool (ETL)", "Rationale": "[cite_start]An **ETL tool** can be scheduled to run a batch process to Extract transactional data, Transform (aggregate) it, Load the summary data into a custom object in Salesforce, and then Delete the original detailed records[cite: 478]."}, {"Status": "Correct", "Choice": "B. Schedulable Batch Apex", "Rationale": "**Batch Apex** is the native tool for high-volume, automated data processing. [cite_start]A Schedulable Batch Apex class can be written to perform the aggregation, insertion of summary records, and deletion of original records natively[cite: 480]."}, {"Status": "Incorrect", "Choice": "C. Third-party Business Intelligence system", "Rationale": "A BI system is for **reporting** on data, not for performing data processing operations like aggregation, loading new data into Salesforce, or deleting records."}, {"Status": "Incorrect", "Choice": "D. Apex Triggers", "Rationale": "[cite_start]**Apex Triggers** run synchronously on a single record and are not suitable for large-scale, batch data processing like aggregating and purging millions of records[cite: 483]."}], "meta": {"source": "QuestionBank", "original_number": 33}}, {"id": "Q34", "number": 34, "question": "Universal Containers (UC) is concerned that data is being corrupted daily either through negligence or maliciousness. They want to implement a backup strategy to help recover any corrupted data or data mistakenly changed or even deleted. What should the data architect consider when designing a field-level audit and recovery plan?", "options": {"A": "Reduce data storage by purging old data.", "B": "Implement an Appexchange package.", "C": "Review projected data storage needs.", "D": "Schedule a weekly export file."}, "answer": ["A"], "explanations": [{"Status": "Correct", "Choice": "B. Implement an Appexchange package.", "Rationale": "The goal is a **field-level audit and recovery plan** for corrupted data. [cite_start]The most complete and robust solution is a specialized **third-party backup/recovery solution** (AppExchange) that provides automated daily backups, granular comparison, and point-in-time restore capabilities[cite: 491]."}, {"Status": "Incorrect", "Choice": "A. Reduce data storage by purging old data.", "Rationale": "[cite_start]Purging old data is the opposite of a recovery plan, as the deleted data is no longer available for recovery[cite: 493]."}, {"Status": "Incorrect", "Choice": "C. Review projected data storage needs.", "Rationale": "This is a step for *capacity planning*, not part of the *recovery plan* itself."}, {"Status": "Incorrect", "Choice": "D. Schedule a weekly export file.", "Rationale": "[cite_start]Weekly exports only provide data backup; they do not provide the necessary recovery capability or granular restore functionality[cite: 497]."}], "meta": {"source": "QuestionBank", "original_number": 34}}, {"id": "Q35", "number": 35, "question": "Every year, Ursa Major Solar has more than 1 million orders. Each order contains an average of 10 line items. The Chief Executive Officer (CEO) needs the Sales Reps to see how much money each customer generates year-over-year. However, data storage is running low in Salesforce. Which approach for data archiving is appropriate for this scenario?", "options": {"A": "1. Annually export and delete order line items. 2. Store them in a zip file in case the data is needed later.", "B": "1. Annually aggregate order amount data to store in a custom object. 2. Delete those orders and order line items.", "C": "1, Annually export and delete orders and order line items. 2. Store them in a zip file in case the data is needed later.", "D": "1, Annually delete orders and order line items. 2. Ensure the customer has order information in another system."}, "answer": ["B"], "explanations": [{"Status": "Correct", "Choice": "B. 1. Annually aggregate order amount data to store in a custom object. 2. Delete those orders and order line items.", "Rationale": "[cite_start]This is the **Aggregate & Purge** strategy, which is optimal when storage is low[cite: 509]. [cite_start]It meets the CEO's reporting requirement by **aggregating** the summary data into a small custom object and then **deleting** the voluminous detailed records[cite: 510]."}, {"Status": "Incorrect", "Choice": "A. 1. Annually export and delete order line items. 2. Store them in a zip file...", "Rationale": "[cite_start]This saves storage but does not meet the CEO's requirement to **see the year-over-year summary data** in Salesforce[cite: 512]."}, {"Status": "Incorrect", "Choice": "C. 1, Annually export and delete orders and order line items. 2. Store them in a zip file...", "Rationale": "[cite_start]Similar to A, this fails to aggregate the required summary data in the application[cite: 514]."}, {"Status": "Incorrect", "Choice": "D. 1, Annually delete orders and order line items. 2. Ensure the customer has order information in another system.", "Rationale": "[cite_start]Deleting without preserving the required summary data (annual revenue) means the CEO's reporting requirement cannot be met[cite: 517]."}], "meta": {"source": "QuestionBank", "original_number": 35}}, {"id": "Q36", "number": 36, "question": "Get Cloudy Consulting monitors 15,000 servers, and these servers automatically record their status every 10 minutes. Because of company policy, these status reports must be maintained for 5 years. Managers at Get Cloudy Consulting need access to up to one week's worth of these status reports with all of their details. An Architect is recommending what data should be integrated into Salesforce and for how long it should be stored in Salesforce. Which two limits should the Architect be aware of? (Choose two.)", "options": {"A": "Data storage limits", "B": "Workflow rule limits", "C": "API Request limits", "D": "Webservice callout limits"}, "answer": ["A", "C"], "explanations": [{"Status": "Correct", "Choice": "A. Data storage limits", "Rationale": "The solution generates a massive volume of data (approx. **3.9 Billion Records** over 5 years). [cite_start]The architect must be aware of **data storage limits** to define a realistic retention strategy[cite: 525]."}, {"Status": "Correct", "Choice": "C. API Request limits", "Rationale": "The data flows *into* Salesforce from external systems (servers). [cite_start]This involves millions of daily **incoming API calls** (15,000 servers * 6 reports/hr * 24 hours), which can quickly hit the organization's **API Request limits**[cite: 527]."}, {"Status": "Incorrect", "Choice": "B. Workflow rule limits", "Rationale": "Workflow/Flow limits are not the primary constraint when dealing with the sheer volume of data and API requests."}, {"Status": "Incorrect", "Choice": "D. Webservice callout limits", "Rationale": "Webservice callouts are for *outgoing* calls *from* Salesforce. The data is flowing *into* Salesforce, so API Request limits are the relevant constraint."}], "meta": {"source": "QuestionBank", "original_number": 36}}, {"id": "Q37", "number": 37, "question": "A Salesforce customer has plenty of data storage. Sales Reps are complaining that searches are bringing back old records that aren't relevant any longer. Sales Managers need the data for their historical reporting. What strategy should a data architect use to ensure a better user experience for the Sales Reps?", "options": {"A": "Create a Permission Set to hide old data from Sales Reps.", "B": "Use Batch Apex to archive old data on a rolling nightly basis.", "C": "Archive and purge old data from Salesforce. on a monthly basis.", "D": "Set data access to Private to hide old data from Sales Reps."}, "answer": ["C"], "explanations": [{"Status": "Correct", "Choice": "C. Archive and purge old data from Salesforce. on a monthly basis.", "Rationale": "[cite_start]The problem is **performance** (slow searches) and **relevance** (old records)[cite: 537]. [cite_start]The only way to improve search performance and filter irrelevant data is to **reduce the active data set** by **archiving** (saving off-platform for reporting) and **purging** (deleting) data that is no longer needed for daily operations[cite: 538]."}, {"Status": "Incorrect", "Choice": "A. Create a Permission Set to hide old data from Sales Reps.", "Rationale": "[cite_start]This is an access control solution and does not improve the **performance** of searches, which still query the full dataset[cite: 540]."}, {"Status": "Incorrect", "Choice": "B. Use Batch Apex to archive old data on a rolling nightly basis.", "Rationale": "[cite_start]This is the mechanism for *archiving*, but without the crucial step of **purging** (deleting) the data from Salesforce, the records remain, and the performance issue persists[cite: 542]."}, {"Status": "Incorrect", "Choice": "D. Set data access to Private to hide old data from Sales Reps.", "Rationale": "Setting OWD to Private is a **security** solution, not a **performance/archiving** solution, and does not hide records based on age."}], "meta": {"source": "QuestionBank", "original_number": 37}}, {"id": "Q38", "number": 38, "question": "Universal Containers (UC) is implementing a formal, cross-business-unit data governance program As part of the program, UC will implement a team to make decisions on enterprise-wide data governance. Which two roles are appropriate as members of this team? Choose 2 answers", "options": {"A": "Analytics/BI Owners", "B": "Data Domain Stewards", "C": "Salesforce Administrators", "D": "Operational Data Users"}, "answer": ["A", "B"], "explanations": [{"Status": "Correct", "Choice": "A. Analytics/BI Owners", "Rationale": "[cite_start]This role owns the data used for enterprise reporting and KPIs and is crucial for defining data quality metrics and ensuring governed data meets reporting needs[cite: 549]."}, {"Status": "Correct", "Choice": "B. Data Domain Stewards", "Rationale": "[cite_start]**Data Domain Stewards** (or Data Owners) are responsible for the quality, integrity, and definition of a specific set of enterprise data and are essential decision-makers in a governance team[cite: 550]."}, {"Status": "Incorrect", "Choice": "C. Salesforce Administrators", "Rationale": "The Admin is responsible for the *implementation* of the governance decisions, but not typically the high-level, cross-business-unit *decision-making*."}, {"Status": "Incorrect", "Choice": "D. Operational Data Users", "Rationale": "This group is the *consumer* of the governed data, not the appropriate decision-makers for *enterprise-wide governance policy*."}], "meta": {"source": "QuestionBank", "original_number": 38}}, {"id": "Q39", "number": 39, "question": "Universal Containers (UC) has a complex system landscape and is implementing a data governance program for the first time Which two first steps would be appropriate for UC to initiate an assessment of data architecture? Choose 2 answers", "options": {"A": "Engage with IT program managers to assess current velocity of projects in the pipeline.", "B": "Engage with database administrators to assess current database performance metrics.", "C": "Engage with executive sponsorship to assess enterprise data strategy and goals.", "D": "Engage with business units and IT to assess current operational systems and data models."}, "answer": ["C", "D"], "explanations": [{"Status": "Correct", "Choice": "C. Engage with executive sponsorship to assess enterprise data strategy and goals.", "Rationale": "[cite_start]A data governance program requires **executive buy-in** and must be aligned with the overall enterprise data strategy[cite: 560]. This is the necessary first step."}, {"Status": "Correct", "Choice": "D. Engage with business units and IT to assess current operational systems and data models.", "Rationale": "This is the crucial second step: **Data Landscape Assessment**. [cite_start]You must understand the current state: what systems exist, what data they hold, and how that data currently flows[cite: 563]."}, {"Status": "Incorrect", "Choice": "A. Engage with IT program managers to assess current velocity of projects in the pipeline.", "Rationale": "[cite_start]This is a project management assessment, not a data architecture or governance assessment[cite: 565]."}, {"Status": "Incorrect", "Choice": "B. Engage with database administrators to assess current database performance metrics.", "Rationale": "This is a technical performance assessment, which is secondary to defining the strategy and landscape."}], "meta": {"source": "QuestionBank", "original_number": 39}}, {"id": "Q40", "number": 40, "question": "To avoid creating duplicate Contacts, a customer frequently uses Data Loader to upsert Contact records into Salesforce. What common error should the data architect be aware of when using upsert?", "options": {"A": "Errors with duplicate external Id values within the same CSV file.", "B": "Errors with records being updated and inserted in the same CSV file.", "C": "Errors when a duplicate Contact name is found cause upsert to fail.", "D": "Errors with using the wrong external Id will cause the load to fail."}, "answer": ["A"], "explanations": [{"Status": "Correct", "Choice": "A. Errors with duplicate external Id values within the same CSV file.", "Rationale": "The **Upsert** operation requires a unique External ID to match records. [cite_start]If the source CSV file contains **duplicate External IDs**, the operation will fail with a `DUPLICATE_VALUE` error because it cannot uniquely identify which record to process[cite: 577]."}, {"Status": "Incorrect", "Choice": "B. Errors with records being updated and inserted in the same CSV file.", "Rationale": "[cite_start]The Upsert operation is specifically designed to handle both updates and inserts in the same file[cite: 579]."}, {"Status": "Incorrect", "Choice": "C. Errors when a duplicate Contact name is found cause upsert to fail.", "Rationale": "The Upsert operation prioritizes the External ID matching. Native Duplicate Rules usually generate warnings, not hard failures, during a bulk upsert."}, {"Status": "Incorrect", "Choice": "D. Errors with using the wrong external Id will cause the load to fail.", "Rationale": "Using the wrong External ID will result in an incorrect match or a new record being inserted, but it will not cause the *entire load to fail*."}], "meta": {"source": "QuestionBank", "original_number": 40}}, {"id": "Q41", "number": 41, "question": "Universal Containers has deployed Salesforce for case management The company is having difficulty understanding what percentage of cases are resolved from the initial call to their support organization. What first step is recommended to implement a reporting solution to measure the support reps case closure rates?", "options": {"A": "Enable field history tracking on the Case object.", "B": "Create a report on Case analytic snapshots.", "C": "Install AppExchange packages for available reports.", "D": "Create Contact and Opportunity Reports and Dashboards."}, "answer": ["A"], "explanations": [{"Status": "Correct", "Choice": "A. Enable field history tracking on the Case object.", "Rationale": "To measure a process based on status changes (e.g., from *New* to *Resolved*), you must track the time and user associated with the `Status` field change. [cite_start]**Field History Tracking** captures the **Old Value**, **New Value**, and **Date/Time** of the change, which is the foundational data required for the reporting solution[cite: 590]."}, {"Status": "Incorrect", "Choice": "B. Create a report on Case analytic snapshots.", "Rationale": "[cite_start]Analytic Snapshots are a reporting tool; they are useless unless the underlying change data (Field History) is first available to report on[cite: 592]."}, {"Status": "Incorrect", "Choice": "C. Install AppExchange packages for available reports.", "Rationale": "AppExchange packages cannot measure a process unique to the customer's org unless the underlying data (Field History) is present."}, {"Status": "Incorrect", "Choice": "D. Create Contact and Opportunity Reports and Dashboards.", "Rationale": "Case closure rates are measured on the **Case** object, not on Contact or Opportunity."}], "meta": {"source": "QuestionBank", "original_number": 41}}, {"id": "Q42", "number": 42, "question": "The head of sales at Get Cloudy Consulting wants to understand key relevant performance figures and help managers take corrective actions where appropriate. What is one reporting option Get Cloudy Consulting should consider?", "options": {"A": "Case SLA performance report", "B": "Sales KPI Dashboard", "C": "Opportunity analytic snapshot", "D": "Lead conversion rate report"}, "answer": ["B"], "explanations": [{"Status": "Correct", "Choice": "B. Sales KPI Dashboard", "Rationale": "[cite_start]The requirement is for the Head of Sales to see **key relevant performance figures (KPIs)** and enable **corrective actions**[cite: 601]. [cite_start]A **Sales KPI Dashboard** is the direct tool for visualizing sales performance against targets and identifying trends that need attention[cite: 602]."}, {"Status": "Incorrect", "Choice": "A. Case SLA performance report", "Rationale": "This is a **Service Cloud** report and is not the **key relevant performance figure** for the Head of Sales."}, {"Status": "Incorrect", "Choice": "C. Opportunity analytic snapshot", "Rationale": "An Analytic Snapshot is a tool for *storing historical data* for reporting, not a user-facing report option for daily corrective action."}, {"Status": "Incorrect", "Choice": "D. Lead conversion rate report", "Rationale": "While important, a single report on Lead conversion is insufficient; the manager needs a broader dashboard that includes multiple sales KPIs."}], "meta": {"source": "QuestionBank", "original_number": 42}}, {"id": "Q43", "number": 43, "question": "In their legacy system, Universal Containers has a monthly accounts receivable report that compiles data from Accounts, Contacts, Opportunities, Orders, and Order Line Items. What difficulty will an Architect run into when implementing this in Salesforce?", "options": {"A": "Salesforce allows up to four objects in a single report type.", "B": "Salesforce does not support Orders or Order Line Items.", "C": "A report cannot contain data from Accounts and Contacts.", "D": "Custom report types cannot contain Opportunity data."}, "answer": ["A"], "explanations": [{"Status": "Correct", "Choice": "A. Salesforce allows up to four objects in a single report type.", "Rationale": "[cite_start]The report requires data from **five** objects[cite: 614]. [cite_start]**Custom Report Types** are limited to a **maximum of four objects** that can be joined together (Primary Object + 3 linked objects), making this report impossible to build natively[cite: 615]."}, {"Status": "Incorrect", "Choice": "B. Salesforce does not support Orders or Order Line Items.", "Rationale": "This is false; Salesforce has standard objects for `Order` and `Order Product`."}, {"Status": "Incorrect", "Choice": "C. A report cannot contain data from Accounts and Contacts.", "Rationale": "This is false; Accounts and Contacts can be reported on together."}, {"Status": "Incorrect", "Choice": "D. Custom report types cannot contain Opportunity data.", "Rationale": "This is false; Opportunity is a standard object and can be the primary object in a Custom Report Type."}], "meta": {"source": "QuestionBank", "original_number": 43}}, {"id": "Q44", "number": 44, "question": "Universal Containers keeps its Account data in Salesforce and its Invoice data in a third-party ERP system. They have connected the Invoice data through a Salesforce external object. They want data from both Accounts and Invoices visible in one report in one place. What two approaches should an architect suggest for achieving this solution? Choose 2 answers", "options": {"A": "Create a report in an external system combining Salesforce Account data and Invoice data from the ERP.", "B": "Create a report combining data from the Account standard object and the Invoices external object.", "C": "Create a Visualforce page combining Salesforce Account data and Invoice external object data.", "D": "Create a separate Salesforce report for Accounts and Invoices and combine them in a dashboard."}, "answer": ["A", "C"], "explanations": [{"Status": "Correct", "Choice": "A. Create a report in an external system combining Salesforce Account data and Invoice data from the ERP.", "Rationale": "[cite_start]Since native reports cannot join standard objects and external objects, using a **Business Intelligence (BI) tool in the external system** is an effective way to join the ERP Invoice data with Salesforce Account data[cite: 633]."}, {"Status": "Correct", "Choice": "C. Create a Visualforce page combining Salesforce Account data and Invoice external object data.", "Rationale": "[cite_start]**Visualforce/Lightning Component** with Apex code is the custom method to query and display data from the standard Account object and the external `Invoice__x` object in a single UI view[cite: 635]."}, {"Status": "Incorrect", "Choice": "B. Create a report combining data from the Account standard object and the Invoices external object.", "Rationale": "[cite_start]Standard Salesforce Report Builder **does not support joining** standard objects (Account) with external objects (`Invoice__x`)[cite: 637]."}, {"Status": "Incorrect", "Choice": "D. Create a separate Salesforce report for Accounts and Invoices and combine them in a dashboard.", "Rationale": "[cite_start]Dashboards can display data from multiple reports, but they **do not join the data** into a single contextual view and are not the optimal solution for \"one report in one place\"[cite: 639]."}], "meta": {"source": "QuestionBank", "original_number": 44}}, {"id": "Q45", "number": 45, "question": "Universal Containers wishes to maintain Lead data from Leads even after they are deleted and cleared from the Recycle Bin. What approach should be implemented to achieve this solution?", "options": {"A": "Use a Lead standard report and filter on the IsDeleted standard field.", "B": "Use a Converted Lead report to display data on Leads that have been deleted.", "C": "Query Salesforce with the queryAll API method or using the ALL ROWS SOQL keywords.", "D": "Send data to a Data Warehouse and mark Leads as deleted in that system."}, "answer": ["D"], "explanations": [{"Status": "Correct", "Choice": "D. Send data to a Data Warehouse and mark Leads as deleted in that system.", "Rationale": "[cite_start]To retain data after it is **permanently deleted** (cleared from the Recycle Bin/hard deleted), the data must be **archived off-platform** before deletion[cite: 647]. A Data Warehouse is the correct external system for long-term, read-only storage."}, {"Status": "Incorrect", "Choice": "A. Use a Lead standard report and filter on the IsDeleted standard field.", "Rationale": "The `IsDeleted` field identifies records in the **Recycle Bin** (soft-deleted). [cite_start]Once cleared (hard-deleted), the record is permanently gone from Salesforce[cite: 651]."}, {"Status": "Incorrect", "Choice": "B. Use a Converted Lead report to display data on Leads that have been deleted.", "Rationale": "[cite_start]This only shows converted records; it does not capture the data of deleted records[cite: 652]."}, {"Status": "Incorrect", "Choice": "C. Query Salesforce with the queryAll API method or using the ALL ROWS SOQL keywords.", "Rationale": "`queryAll` only accesses soft-deleted records in the Recycle Bin. [cite_start]It does not retrieve records that have been permanently deleted[cite: 655]."}], "meta": {"source": "QuestionBank", "original_number": 45}}, {"id": "Q46", "number": 46, "question": "Universal Containers (UC) has deployed Salesforce to manage Marketing, Sales, and Support efforts in a multi-system ERP environment After reaching the limits of native reports & dashboards, UC leadership is looking to understand what options can be used to provide more analytical insights. What two approaches should an architect recommend? Choose 2 answers", "options": {"A": "AppExchange Apps", "B": "Wave Analytics", "C": "Weekly Snapshots", "D": "Setup Audit Trails"}, "answer": ["B"], "explanations": [{"Status": "Correct", "Choice": "B. Wave Analytics", "Rationale": "[cite_start]**Wave Analytics** (now CRM Analytics/Tableau CRM) is the native, dedicated, advanced Business Intelligence (BI) platform designed to overcome the limits of native reports and dashboards by handling large datasets and performing complex data preparation and analysis[cite: 661]."}, {"Status": "Correct", "Choice": "A. AppExchange Apps", "Rationale": "[cite_start]**AppExchange Apps** (specifically third-party BI or reporting solutions) are a standard architectural recommendation to extend native reporting limits and provide more analytical insights[cite: 662]."}, {"Status": "Incorrect", "Choice": "C. Weekly Snapshots", "Rationale": "Weekly Snapshots are an internal tool used to store historical data. [cite_start]They still run on the native platform with the same limits and are not a solution for providing *more analytical insights*[cite: 664]."}, {"Status": "Incorrect", "Choice": "D. Setup Audit Trails", "Rationale": "[cite_start]The Setup Audit Trail tracks **metadata changes** (who created what field when), not the sales or operational data needed for analytical insights[cite: 665]."}], "meta": {"source": "QuestionBank", "original_number": 46}}, {"id": "Q47", "number": 47, "question": "Universal Containers is setting up an external Business Intelligence (Bl) system and wants to extract 1,000,000 Contact records. What should be recommended to avoid timeouts during the export process?", "options": {"A": "Use the SOAP API to export data.", "B": "Utilize the Bulk API to export the data.", "C": "Use Gzip compression to export the data.", "D": "Schedule a Batch Apex job to export the data."}, "answer": ["C"], "explanations": [{"Status": "Correct", "Choice": "B. Utilize the Bulk API to export the data.", "Rationale": "**Bulk API** is the dedicated, optimized API for processing large volumes of data (over 50,000 records) asynchronously. [cite_start]It is specifically designed to handle large exports more efficiently and avoid timeouts[cite: 672]."}, {"Status": "Correct", "Choice": "C. Use Gzip compression to export the data.", "Rationale": "[cite_start]**Gzip compression** reduces the file size, which helps with network speed and reduces the risk of timeouts during the transmission of large volumes of data[cite: 674]."}, {"Status": "Incorrect", "Choice": "A. Use the SOAP API to export data.", "Rationale": "[cite_start]**SOAP API** is a synchronous API designed for real-time transactions and is highly prone to **timeouts** when dealing with 1,000,000 records[cite: 676]."}, {"Status": "Incorrect", "Choice": "D. Schedule a Batch Apex job to export the data.", "Rationale": "Batch Apex is for *processing* data within Salesforce. The **Bulk API** is the standard and most direct tool for large-scale **export** to an external system."}], "meta": {"source": "QuestionBank", "original_number": 47}}, {"id": "Q48", "number": 48, "question": "Universal Containers (UC) is a business that works directly with individual consumers (B2C). They are moving from a current home-grown CRM system to Salesforce. UC has about one million consumer records. What should the architect recommend for optimal use of Salesforce functionality and also to avoid data loading issues?", "options": {"A": "Create a Custom Object Individual Consumer_c to load all individual consumers.", "B": "Load all individual consumers as Account records and avoid using the Contact object.", "C": "Load one Account record and one Contact record for each individual consumer.", "D": "Create one Account and load individual consumers as Contacts linked to that one Account."}, "answer": ["C"], "explanations": [{"Status": "Correct", "Choice": "C. Load one Account record and one Contact record for each individual consumer.", "Rationale": "The requirement is for B2C (individual consumers). [cite_start]The best practice data model is **Person Accounts** (if available), which is a merge of the standard Account and Contact objects[cite: 688]. [cite_start]Since Person Account is not an explicit choice, the next best option is the standard **Account-Contact** model (one Account, one Contact) with a B2C Record Type[cite: 689]."}, {"Status": "Incorrect", "Choice": "A. Create a Custom Object Individual Consumer_c to load all individual consumers.", "Rationale": "[cite_start]Creating a custom object bypasses the standard Sales/Service Cloud functionality built on the standard Account/Contact objects[cite: 691]."}, {"Status": "Incorrect", "Choice": "B. Load all individual consumers as Account records and avoid using the Contact object.", "Rationale": "[cite_start]This is strongly discouraged as it prevents using Contact-based features and relationships[cite: 693]."}, {"Status": "Incorrect", "Choice": "D. Create one Account and load individual consumers as Contacts linked to that one Account.", "Rationale": "[cite_start]This creates a catastrophic **Account Skew** problem, severely degrading performance for sharing, reports, and list views on that single parent Account[cite: 695]."}], "meta": {"source": "QuestionBank", "original_number": 48}}, {"id": "Q49", "number": 49, "question": "DreamHouse Realty has a data model as shown in the image. The Project object has a private sharing model, and it has Roll-Up summary fields to calculate the number of resources assigned to the project, total hours for the project, and the number of work items associated to the project. There will be a large amount of time entry records to be loaded regularly from an external system into Salesforce. What should the Architect consider in this situation?", "options": {"A": "Load all data after deferring sharing calculations.", "B": "Calculate summary values instead of Roll-Up by using workflow.", "C": "Calculate summary values instead of Roll-Up by using triggers.", "D": "Load all data using external IDs to link to parent records."}, "answer": ["A"], "explanations": [{"Status": "Correct", "Choice": "A. Load all data after deferring sharing calculations.", "Rationale": "The Project object has a **private sharing model** and **Roll-Up Summary fields**. Loading a large volume of Time Entry records will trigger Roll-Ups and complex **Sharing Recalculations**. [cite_start]**Deferring sharing calculations** (a feature enabled by Salesforce Support) is the best practice to minimize load time and avoid `UNABLE_TO_LOCK_ROW` errors in orgs with complex sharing[cite: 704]."}, {"Status": "Incorrect", "Choice": "B. Calculate summary values instead of Roll-Up by using workflow.", "Rationale": "[cite_start]Workflow/Flow cannot calculate aggregate summary values (SUM, COUNT) from child records[cite: 706]."}, {"Status": "Incorrect", "Choice": "C. Calculate summary values instead of Roll-Up by using triggers.", "Rationale": "This is custom code and does not solve the fundamental performance bottleneck caused by sharing calculations."}, {"Status": "Incorrect", "Choice": "D. Load all data using external IDs to link to parent records.", "Rationale": "[cite_start]Using External IDs is standard migration practice, but it does not mitigate the performance hit caused by the Roll-Up Summaries and Sharing Recalculations[cite: 710]."}], "meta": {"source": "QuestionBank", "original_number": 49}}, {"id": "Q50", "number": 50, "question": "Get Cloudy Consulting is migrating their legacy system's users and data to Salesforce. They will be creating 15,000 users, 1.5 million Account records, and 15 million Invoice records. The visibility of these records is controlled by a 50 owner and criteria-based sharing rules. Get Cloudy Consulting needs to minimize data loading time during this migration to a new organization. Which two approaches will accomplish this goal? (Choose two.)", "options": {"A": "Create the users, upload all data, and then deploy the sharing rules.", "B": "Contact Salesforce to activate indexing before uploading the data.", "C": "First, load all account records, and then load all user records.", "D": "Defer sharing calculations until the data has finished uploading."}, "answer": ["A", "D"], "explanations": [{"Status": "Correct", "Choice": "D. Defer sharing calculations until the data has finished uploading.", "Rationale": "This is a large migration (1.5M Accounts, 15M Invoices) in an org with complex sharing (50+ sharing rules). Every record insert triggers sharing rule processing. [cite_start]**Deferring sharing calculations** is the most effective single method to **minimize data loading time** in a complex sharing environment[cite: 722]."}, {"Status": "Correct", "Choice": "A. Create the users, upload all data, and then deploy the sharing rules.", "Rationale": "The best practice sequence for migration is: 1) Deploy metadata (including sharing rules). 2) **Load Users**. 3) **Load Data**. 4) Recalculate Sharing. [cite_start]The key here is deferring the deployment of the sharing rules until after the data is loaded (which is an alternate way of achieving deferred calculation) or ensuring the feature is activated[cite: 725]."}, {"Status": "Incorrect", "Choice": "B. Contact Salesforce to activate indexing before uploading the data.", "Rationale": "Indexes improve *query* performance. [cite_start]While important, they do not directly mitigate the data load time impact caused by **sharing rule recalculations**[cite: 727]."}, {"Status": "Incorrect", "Choice": "C. First, load all account records, and then load all user records.", "Rationale": "[cite_start]**Users must be loaded first** so they can be assigned as the record owners during the Account load[cite: 729]. Loading accounts first is inefficient."}], "meta": {"source": "QuestionBank", "original_number": 50}}, {"id": "Q51", "number": 51, "question": "[cite_start]Cloud Kicks has the following requirements: Data needs to be sent from Salesforce to an external system to generate invoices from their Order Management System (OMS)[cite: 731]. [cite_start]A Salesforce administrator must be able to customize which fields will be sent to the external system without changing code[cite: 732]. What are two approaches for fulfilling these requirements? (Choose two.)", "options": {"A": "A set<sobjectFieldset> to determine which fields to send in an HTTP callout.", "B": "An Outbound Message to determine which fields to send to the OMS.", "C": "A Field Set that determines which fields to send in an HTTP callout.", "D": "Enable the field-level security permissions for the fields to send."}, "answer": ["A", "C"], "explanations": [{"Status": "Correct", "Choice": "A. A set<sobjectFieldset> to determine which fields to send in an HTTP callout.", "Rationale": "[cite_start]This represents the required solution: an Apex callout (`set<sobjectFieldset>`) [cite: 739] that reads a declarative configuration (Field Set)."}, {"Status": "Correct", "Choice": "C. A Field Set that determines which fields to send in an HTTP callout.", "Rationale": "[cite_start]A **Field Set** [cite: 741] [cite_start]is the declarative tool that allows an **administrator to customize** [cite: 741] [cite_start]the list of fields sent to an external system **without modifying code**[cite: 741]."}, {"Status": "Incorrect", "Choice": "B. An Outbound Message to determine which fields to send to the OMS.", "Rationale": "Outbound Messages are declarative but the fields are fixed by the WSDL and **cannot be customized dynamically** by an administrator after deployment."}, {"Status": "Incorrect", "Choice": "D. Enable the field-level security permissions for the fields to send.", "Rationale": "[cite_start]FLS controls *access* and *visibility*[cite: 745], but not which specific fields are included in an outbound integration payload."}], "meta": {"source": "QuestionBank", "original_number": 51}}, {"id": "Q53", "number": 53, "question": "Universal Containers has a large volume of Contact data going into Salesforce.com. There are 100,000 existing contact records. 200,000 new contacts will be loaded. The Contact object has an external ID field that is unique and must be populated for all existing records. What should the architect recommend to reduce data load processing time?", "options": {"A": "Load Contact records together using the Streaming API via the Upsert operation.", "B": "Delete all existing records, and then load all records together via the Insert operation.", "C": "Load all records via the Upsert operation to determine new records vs. existing records.", "D": "Load new records via the Insert operation and existing records via the Update operation."}, "answer": ["D"], "explanations": [{"Status": "Correct", "Choice": "D. Load new records via the Insert operation and existing records via the Update operation.", "Rationale": "[cite_start]**Separate Insert and Update operations** [cite: 769] are faster than Upsert because they avoid the overhead of performing a look-up check on the External ID for *every* record. If you can pre-sort the data externally, this is the most efficient method."}, {"Status": "Incorrect", "Choice": "A. Load Contact records together using the Streaming API via the Upsert operation.", "Rationale": "[cite_start]Streaming API is for event processing, not for bulk data loading[cite: 772]."}, {"Status": "Incorrect", "Choice": "B. Delete all existing records, and then load all records together via the Insert operation.", "Rationale": "[cite_start]Deleting records to facilitate a simpler load risks data loss and is not an appropriate migration strategy[cite: 774]."}, {"Status": "Incorrect", "Choice": "C. Load all records via the Upsert operation to determine new records vs. existing records.", "Rationale": "[cite_start]Upsert requires more processing overhead per record than dedicated Insert or Update calls[cite: 776]."}], "meta": {"source": "QuestionBank", "original_number": 53}}, {"id": "Q54", "number": 54, "question": "An architect is planning on having different batches to load one million Opportunities into Salesforce using the Bulk API in parallel mode. What should be considered when loading the Opportunity records?", "options": {"A": "Create indexes on Opportunity object text fields.", "B": "Group batches by the Accountid field.", "C": "Sort batches by Name field values.", "D": "Order batches by Auto-number field."}, "answer": ["D"], "explanations": [{"Status": "Correct", "Choice": "B. Group batches by the Accountid field.", "Rationale": "When loading child records (**Opportunity**) in **parallel mode**, multiple threads may try to update the Roll-Up Summary fields on the same parent **Account** simultaneously. [cite_start]This causes **row-locking contention**[cite: 783]. **Grouping by the Parent ID (`AccountId`)** ensures that records referencing the same parent are processed together, mitigating locking issues."}, {"Status": "Incorrect", "Choice": "A. Create indexes on Opportunity object text fields.", "Rationale": "[cite_start]Custom indexes should generally be removed or deferred during bulk loading because they slow down DML operations (inserts)[cite: 785]."}, {"Status": "Incorrect", "Choice": "C. Sort batches by Name field values.", "Rationale": "Sorting by the `Name` field provides no performance benefit and will not mitigate row-locking issues."}, {"Status": "Incorrect", "Choice": "D. Order batches by Auto-number field.", "Rationale": "An auto-number field is generated upon insertion and cannot be used for ordering during the load planning stage."}], "meta": {"source": "QuestionBank", "original_number": 54}}, {"id": "Q55", "number": 55, "question": "DreamHouse Realty has 15 million records in the Order_c custom object. When running a bulk query, the query times out. What should be considered to address this issue?", "options": {"A": "Tooling API", "B": "PK Chunking", "C": "Metadata API", "D": "Streaming API"}, "answer": ["A"], "explanations": [{"Status": "Correct", "Choice": "B. PK Chunking", "Rationale": "(Note: The first half of the question is the context). [cite_start]The most effective solution for query timeouts on massive data volumes (15 million records) using the Bulk API is **PK Chunking**[cite: 799]. PK Chunking automatically splits a bulk query on a large table into multiple batches based on the record ID, ensuring each smaller query remains selective and avoids timeouts."}, {"Status": "Incorrect", "Choice": "A. Tooling API", "Rationale": "The Tooling API is used for development tools and metadata, not for high-volume data extraction."}, {"Status": "Incorrect", "Choice": "C. Metadata API", "Rationale": "The Metadata API is used for metadata deployment, not for data extraction."}, {"Status": "Incorrect", "Choice": "D. Streaming API", "Rationale": "[cite_start]Streaming API is for real-time event processing, not for extracting large volumes of existing data[cite: 803]."}], "meta": {"source": "QuestionBank", "original_number": 55}}, {"id": "Q56", "number": 56, "question": "Universal Containers (UC) has users complaining about reports timing out or simply taking too long to run What two actions should the data architect recommend to improve the reporting experience? Choose 2 answers", "options": {"A": "Index key fields used in report criteria.", "B": "Enable Divisions for large data objects.", "C": "Create one skinny table per report.", "D": "Share each report with fewer users."}, "answer": ["C"], "explanations": [{"Status": "Correct", "Choice": "A. Index key fields used in report criteria.", "Rationale": "To improve report performance, queries must be **selective**. [cite_start]Ensuring that fields used in report filters (`WHERE` clauses) are indexed allows the database to find the records quickly, preventing timeouts and long run times[cite: 809]."}, {"Status": "Correct", "Choice": "C. Create one skinny table per report.", "Rationale": "[cite_start]**Skinny Tables** [cite: 811] [cite_start]are a performance-enhancing feature (enabled by Salesforce Support) that creates an optimized table containing frequently used fields, which avoids resource-intensive joins and significantly improves the performance of reports on large objects[cite: 811]."}, {"Status": "Incorrect", "Choice": "B. Enable Divisions for large data objects.", "Rationale": "Divisions is a data segmentation tool primarily for filtering data but does not inherently address the underlying performance issue of the database query."}, {"Status": "Incorrect", "Choice": "D. Share each report with fewer users.", "Rationale": "Sharing reports has no direct impact on the performance of the underlying report query itself."}], "meta": {"source": "QuestionBank", "original_number": 56}}, {"id": "Q57", "number": 57, "question": "Which two areas should a Data Architect investigate during troubleshooting if queries are timing out? (Choose two.)", "options": {"A": "Make sure the query doesn't contain NULL in any filter criteria.", "B": "Create a formula field instead of having multiple filter criteria.", "C": "Create custom indexes on the fields used in the filter criteria.", "D": "Modify the integration users\u2019 profile to have View All Data."}, "answer": ["C"], "explanations": [{"Status": "Correct", "Choice": "C. Create custom indexes on the fields used in the filter criteria.", "Rationale": "[cite_start]The primary cause of slow queries on large tables is a lack of **selectivity**[cite: 823]. [cite_start]Creating custom indexes on the fields used in the `WHERE` clause is the best way to make the query selective and prevent timeouts[cite: 824]."}, {"Status": "Correct", "Choice": "A. Make sure the query doesn't contain NULL in any filter criteria.", "Rationale": "Fields filtered using `!= NULL` or `IS NOT NULL` are often non-selective because standard indexes do not contain null values. Using fields that are consistently populated improves the chance of the query optimizer using an index."}, {"Status": "Incorrect", "Choice": "B. Create a formula field instead of having multiple filter criteria.", "Rationale": "Formula fields cannot be indexed and should generally be avoided in `WHERE` clauses for large tables, as they often make the query non-selective."}, {"Status": "Incorrect", "Choice": "D. Modify the integration users\u2019 profile to have View All Data.", "Rationale": "Giving the user `View All Data` (a security setting) does not change the performance of the underlying query."}], "meta": {"source": "QuestionBank", "original_number": 57}}, {"id": "Q58", "number": 58, "question": "Universal Containers (UC) is implementing a Salesforce project with large volumes of data and daily transactions. The solution includes both real-time web service integrations and Visualforce mash-ups with back-end systems. The Salesforce Full sandbox used by the project integrates with full-scale back-end testing systems. What two types of performance testing are appropriate for this project? Choose 2 answers", "options": {"A": "Pre-go-live automated page-load testing against the Salesforce Full sandbox.", "B": "Post go-live automated page-load testing against the Salesforce Production org.", "C": "Pre-go-live unit testing in the Salesforce Full sandbox.", "D": "Stress testing against the web services hosted by the integration middleware."}, "answer": ["A", "D"], "explanations": [{"Status": "Correct", "Choice": "A. Pre-go-live automated page-load testing against the Salesforce Full sandbox.", "Rationale": "**Page Load Testing** or end-to-end performance testing is necessary to ensure acceptable user experience. [cite_start]Testing pre-go-live against a **Full Sandbox** with representative data volume is the correct validation approach[cite: 842]."}, {"Status": "Correct", "Choice": "D. Stress testing against the web services hosted by the integration middleware.", "Rationale": "[cite_start]Since the solution includes **real-time web service integrations** [cite: 844][cite_start], **Stress Testing** the integration layer (middleware) is essential to ensure it can handle the expected peak transaction volume[cite: 844]."}, {"Status": "Incorrect", "Choice": "B. Post go-live automated page-load testing against the Salesforce Production org.", "Rationale": "Performance testing should be done pre-go-live. [cite_start]Testing against **Production** is generally high-risk and only done for ongoing monitoring, not for primary performance validation[cite: 846]."}, {"Status": "Incorrect", "Choice": "C. Pre-go-live unit testing in the Salesforce Full sandbox.", "Rationale": "Unit testing is for code *functionality* validation, not for measuring the **performance** of page loads, integrations, or throughput."}], "meta": {"source": "QuestionBank", "original_number": 58}}, {"id": "Q59", "number": 59, "question": "Developers at Universal Containers need to build a report for the business which displays Accounts opened in the past year grouped by industry. This report will also include information from contacts, opportunities, and orders. There are several million Accounts in the system. Which two options should be recommended to make this report perform well and satisfy the business need?", "options": {"A": "Use triggers to populate denormalized related fields on the Account.", "B": "Use an indexed data field with bounded data filters.", "C": "Use Formula fields to surface information related entities on the report.", "D": "Use unbounded date ranges to filter the report."}, "answer": ["B", "A"], "explanations": [{"Status": "Correct", "Choice": "B. Use an indexed data field with bounded data filters.", "Rationale": "To ensure high performance on millions of Accounts, the report query must be **selective**. [cite_start]Filtering by **bounded data ranges** (e.g., past year) on an **indexed field** (like `CreatedDate`) is the most effective way to make the query selective[cite: 858]."}, {"Status": "Correct", "Choice": "A. Use triggers to populate denormalized related fields on the Account.", "Rationale": "The report involves multiple joins (Contacts, Opportunities, Orders). [cite_start]**Denormalization** (copying critical summary data from children to the parent Account via triggers) avoids resource-intensive joins and improves report performance[cite: 860]."}, {"Status": "Incorrect", "Choice": "C. Use Formula fields to surface information related entities on the report.", "Rationale": "[cite_start]Formula fields cannot be indexed and are computed dynamically, often leading to slower report performance when used as filters or columns[cite: 862]."}, {"Status": "Incorrect", "Choice": "D. Use unbounded date ranges to filter the report.", "Rationale": "[cite_start]**Unbounded date ranges** (e.g., `Date > 1/1/2000`) prevent the query from being selective and will cause performance issues on large tables[cite: 864]. The range must be bounded."}], "meta": {"source": "QuestionBank", "original_number": 59}}, {"id": "Q60", "number": 60, "question": "Universal Containers has millions of rows of data in Salesforce that are being used in reports to evaluate historical trends. Performance has become an issue, as well as data storage limits. Which two strategies should be recommended when talking with stakeholders?", "options": {"A": "Use scheduled batch Apex to copy aggregate information into a custom object and delete the original records.", "B": "Combine Analytics Snapshots with a purging plan by reporting on the snapshot data and deleting the original records.", "C": "Use Data Loader to extract data, aggregate it, and write it back to a custom object, then delete the original records.", "D": "Configure the Salesforce Archiving feature to archive older records and remove them from the data storage limits."}, "answer": ["A", "B"], "explanations": [{"Status": "Correct", "Choice": "A. Use scheduled batch Apex to copy aggregate information into a custom object and delete the original records.", "Rationale": "[cite_start]This is the **Aggregate & Purge** architectural pattern[cite: 874]. [cite_start]It solves **storage** (by deleting detailed records) and **performance** (by allowing reports to run on the smaller, custom aggregate object)[cite: 874]."}, {"Status": "Correct", "Choice": "B. Combine Analytics Snapshots with a purging plan by reporting on the snapshot data and deleting the original records.", "Rationale": "**Analytic Snapshots** capture the historical trend data needed. [cite_start]Combining this with a **purging plan** (deleting the original transactional records) addresses both the **performance** and **storage** issues while meeting the reporting requirement[cite: 876]."}, {"Status": "Incorrect", "Choice": "C. Use Data Loader to extract data, aggregate it, and write it back to a custom object, then delete the original records.", "Rationale": "[cite_start]This is essentially the same as A, but it relies on an external, manual tool (Data Loader), making it a less sustainable automation strategy than Batch Apex[cite: 878]."}, {"Status": "Incorrect", "Choice": "D. Configure the Salesforce Archiving feature to archive older records and remove them from the data storage limits.", "Rationale": "[cite_start]Salesforce has no general-purpose, native \"archiving feature\" that automatically moves data outside of Salesforce to a cheap store and removes it from storage limits[cite: 880]. Archiving must be custom-built or done via a third-party tool."}], "meta": {"source": "QuestionBank", "original_number": 60}}, {"id": "Q61", "number": 61, "question": "Universal Containers (UC) has implemented Sales Cloud for its entire sales organization, UC has built a custom object called projects_c that stores customers project detail and employee bitable hours. The following requirements are needed: The finance users will not access to any sales objects, but they will need to interact with the custom object. A subnet of individuals from the finance team will need to access to the projects object for reporting and adjusting employee utilization. Which license type a data architect recommend for the finance team that best meets the requirements?", "options": {"A": "Service Cloud", "B": "Sales Cloud", "C": "Light Platform Start", "D": "Lighting platform plus"}, "answer": ["C"], "explanations": [{"Status": "Correct", "Choice": "C. Light Platform Start", "Rationale": "[cite_start]The **Lightning Platform Start** license is designed for users who only need access to a **small number of custom objects** (Project_c) [cite: 886] and are explicitly restricted from standard CRM/Sales objects. This is the most cost-effective solution."}, {"Status": "Incorrect", "Choice": "A. Service Cloud", "Rationale": "This license grants access to standard Service objects (Case, etc.), which are not needed, making it unnecessarily expensive."}, {"Status": "Incorrect", "Choice": "B. Sales Cloud", "Rationale": "[cite_start]This license grants access to standard Sales objects (Opportunity, Lead, etc.), which the finance team is explicitly required *not* to access[cite: 889]."}, {"Status": "Incorrect", "Choice": "D. Lighting platform plus", "Rationale": "This license offers more features (more custom objects, API calls) and is more expensive than the \"Start\" version. The \"Start\" version meets the minimal access requirements."}], "meta": {"source": "QuestionBank", "original_number": 61}}, {"id": "Q62", "number": 62, "question": "A shipping and logistics company has created a large number of reports within Sales Cloud since Salesforce was introduced. Some of these reports analyze large amounts of data regarding the whereabouts of the company's containers, and they are starting to time out when users are trying to run the reports. What is a recommended approach to avoid these time-out issues?", "options": {"A": "Improve reporting performance by creating a custom Visualforce report that is using a cache of the records in the report.", "B": "Improve reporting performance by replacing the existing reports in Sales Cloud with new reports based on Analytics Cloud.", "C": "Improve reporting performance by creating an Apex trigger for the Report object that will pre-fetch data before the report is run.", "D": "Improve reporting performance by creating a dashboard that is scheduled to run the reports only once per day."}, "answer": ["B"], "explanations": [{"Status": "Correct", "Choice": "B. Improve reporting performance by replacing the existing reports in Sales Cloud with new reports based on Analytics Cloud.", "Rationale": "[cite_start]When standard reports are timing out due to querying **large amounts of data**[cite: 900], the recommended architectural solution is to move reporting to a platform designed for Big Data analytics. [cite_start]**Analytics Cloud** (CRM Analytics/Tableau CRM) is designed to handle high-volume data and complex calculations efficiently[cite: 901]."}, {"Status": "Incorrect", "Choice": "A. Improve reporting performance by creating a custom Visualforce report that is using a cache of the records in the report.", "Rationale": "Creating custom reports is high-effort, coded, and less scalable than using a dedicated BI platform like Analytics Cloud."}, {"Status": "Incorrect", "Choice": "C. Improve reporting performance by creating an Apex trigger for the Report object that will pre-fetch data before the report is run.", "Rationale": "You cannot create an Apex trigger on the standard `Report` object."}, {"Status": "Incorrect", "Choice": "D. Improve reporting performance by creating a dashboard that is scheduled to run the reports only once per day.", "Rationale": "Scheduling the dashboard only makes the data *available* once a day; it does not solve the underlying performance issue of the query *if* the report data is needed on-demand."}], "meta": {"source": "QuestionBank", "original_number": 62}}, {"id": "Q63", "number": 63, "question": "Due to security requirements, Universal Containers needs to capture specific user actions, such as login, logout, file attachment download, package install, etc. What is the recommended approach for defining a solution for this requirement?", "options": {"A": "Use a field audit trail to capture field changes.", "B": "Use a custom object and trigger to capture changes.", "C": "Use Event Monitoring to capture these changes.", "D": "Use a third-party AppExchange app to capture changes."}, "answer": ["C"], "explanations": [{"Status": "Correct", "Choice": "C. Use Event Monitoring to capture these changes.", "Rationale": "[cite_start]**Event Monitoring** is the dedicated Salesforce feature designed to capture **granular user and system actions** [cite: 914] (login, logout, file download, report export, package installation, etc.) for security, auditing, and compliance purposes."}, {"Status": "Incorrect", "Choice": "A. Use a field audit trail to capture field changes.", "Rationale": "[cite_start]**Field Audit Trail** (Field History Tracking) only tracks changes to *data values* on records[cite: 916], not system events like login/logout."}, {"Status": "Incorrect", "Choice": "B. Use a custom object and trigger to capture changes.", "Rationale": "Triggers only fire on DML operations (insert, update, delete) for records. [cite_start]They cannot capture system events like login, logout, or file downloads[cite: 919]."}, {"Status": "Incorrect", "Choice": "D. Use a third-party AppExchange app to capture changes.", "Rationale": "While apps exist, **Event Monitoring** is the native, official, and most comprehensive solution for capturing system-wide user events."}], "meta": {"source": "QuestionBank", "original_number": 63}}, {"id": "Q64", "number": 64, "question": "Universal Containers (UC) is concerned about the accuracy of their Customer information in Salesforce. They have recently created an enterprise-wide trusted source MDM for Customer data which they have certified to be accurate. UC has over 20 million unique customer records in the trusted source and Salesforce. What should an Architect recommend to ensure the data in Salesforce is identical to the MDM?", "options": {"A": "Extract the Salesforce data into Excel and manually compare this against the trusted source.", "B": "Load the Trusted Source data into Salesforce and run an Apex Batch job to find difference.", "C": "Use an AppExchange package for Data Quality to match Salesforce data against the Trusted source.", "D": "Leave the data in Salesforce alone and assume that it will auto-correct itself over time."}, "answer": ["C"], "explanations": [{"Status": "Correct", "Choice": "C. Use an AppExchange package for Data Quality to match Salesforce data against the Trusted source.", "Rationale": "[cite_start]To validate and reconcile **20 million records** against an external **MDM** trusted source[cite: 931], a specialized, high-volume tool is needed. [cite_start]AppExchange **Data Quality and Matching packages** are designed to connect to external systems and perform automated matching, comparison, and merging/updating[cite: 932]."}, {"Status": "Incorrect", "Choice": "A. Extract the Salesforce data into Excel and manually compare this against the trusted source.", "Rationale": "[cite_start]This is a manual process that is impossible to execute and sustain with 20 million records[cite: 934]."}, {"Status": "Incorrect", "Choice": "B. Load the Trusted Source data into Salesforce and run an Apex Batch job to find difference.", "Rationale": "[cite_start]This is a custom (coded) solution that consumes excessive Salesforce storage by loading a second copy of the MDM data set into a temporary custom object[cite: 936]."}, {"Status": "Incorrect", "Choice": "D. Leave the data in Salesforce alone and assume that it will auto-correct itself over time.", "Rationale": "[cite_start]This is not an architectural recommendation and violates the requirement for data to be \"identical\"[cite: 938]."}], "meta": {"source": "QuestionBank", "original_number": 64}}, {"id": "Q65", "number": 65, "question": "Which two statements are accurate with respect to performance testing a Force.com application?", "options": {"A": "All Force.com applications must be performance tested in a sandbox as well as production.", "B": "A performance test plan must be created and submitted to Salesforce customer support.", "C": "Applications with highly customized code or large volumes should be performance tested.", "D": "Application performance benchmarked in a sandbox can also be expected in production."}, "answer": ["B", "C"], "explanations": [{"Status": "Correct", "Choice": "C. Applications with highly customized code or large volumes should be performance tested.", "Rationale": "[cite_start]Performance testing is essential when **custom code (Apex/Visualforce)** or **large data volumes** [cite: 946] are involved, as these factors are most likely to introduce scalability or latency issues."}, {"Status": "Correct", "Choice": "B. A performance test plan must be created and submitted to Salesforce customer support.", "Rationale": "[cite_start]When conducting performance or load testing against a Salesforce instance, the test plan **must be submitted to Salesforce Support** [cite: 948] for approval to protect the shared multi-tenant infrastructure."}, {"Status": "Incorrect", "Choice": "A. All Force.com applications must be performance tested in a sandbox as well as production.", "Rationale": "Testing in Production is generally high-risk and only done for monitoring. [cite_start]Testing in a Full Sandbox is sufficient for primary validation[cite: 951]."}, {"Status": "Incorrect", "Choice": "D. Application performance benchmarked in a sandbox can also be expected in production.", "Rationale": "[cite_start]Performance measured in a sandbox is a baseline, but cannot be guaranteed in Production due to multi-tenancy and other traffic[cite: 953]."}], "meta": {"source": "QuestionBank", "original_number": 65}}, {"id": "Q66", "number": 66, "question": "Universal Containers (UC) is launching an RFP to acquire a new accounting product available on AppExchange. UC is expecting to issue 5 million invoices per year, with each invoice containing an average of 10 line items. What should UC's Data Architect recommend to ensure scalability?", "options": {"A": "Ensure invoice line items simply reference existing Opportunity line items.", "B": "Ensure the account product vendor includes Wave Analytics in their offering.", "C": "Ensure the account product vendor provides a sound data archiving strategy.", "D": "Ensure the accounting product runs 100% natively on the Salesforce platform."}, "answer": ["C"], "explanations": [{"Status": "Correct", "Choice": "C. Ensure the account product vendor provides a sound data archiving strategy.", "Rationale": "[cite_start]The total data volume is massive and ongoing (50 million line items per year)[cite: 962]. This volume will quickly exceed storage and performance limits. [cite_start]The architect must ensure the vendor has a **sound data archiving strategy** [cite: 963] built into their product to manage the long-term scalability."}, {"Status": "Incorrect", "Choice": "A. Ensure invoice line items simply reference existing Opportunity line items.", "Rationale": "[cite_start]This is a data modeling decision and does not solve the long-term scalability problem of 50 million *new* records per year[cite: 965]."}, {"Status": "Incorrect", "Choice": "B. Ensure the account product vendor includes Wave Analytics in their offering.", "Rationale": "[cite_start]Wave Analytics (CRM Analytics) addresses *reporting* performance, but not the core *storage* and *transactional* scalability issues of the huge volume of invoice records being created[cite: 967]."}, {"Status": "Incorrect", "Choice": "D. Ensure the accounting product runs 100% natively on the Salesforce platform.", "Rationale": "[cite_start]A native solution is not immune to data volume and storage limits if it creates 50 million records per year[cite: 969]."}], "meta": {"source": "QuestionBank", "original_number": 66}}, {"id": "Q67", "number": 67, "question": "Universal Containers (UC) has a custom discount request object set as a detail object with a custom product object as the master. There is a requirement to allow the creation of generic discount requests without the custom product object as its master record. What solution should an Architect recommend to UC?", "options": {"A": "Mandate the selection of a custom product for each discount request.", "B": "Create a placeholder product record for the generic discount request.", "C": "Remove the master-detail relationship and keep the objects separate.", "D": "Change the master-detail relationship to a lookup relationship."}, "answer": ["D"], "explanations": [{"Status": "Correct", "Choice": "D. Change the master-detail relationship to a lookup relationship.", "Rationale": "[cite_start]A **Master-Detail** relationship requires the Detail record to *always* have a Master record[cite: 977]. [cite_start]The requirement is to allow creation of discount requests **without** a master record[cite: 978]. [cite_start]The only solution is to change the relationship to a **Lookup**, which allows the relationship field to be optional (null)[cite: 979]."}, {"Status": "Incorrect", "Choice": "A. Mandate the selection of a custom product for each discount request.", "Rationale": "[cite_start]This violates the core requirement to allow creation *without* a master record[cite: 981]."}, {"Status": "Incorrect", "Choice": "B. Create a placeholder product record for the generic discount request.", "Rationale": "[cite_start]This is poor data modeling practice (creating unnecessary records) to bypass a technical limitation[cite: 983]."}, {"Status": "Incorrect", "Choice": "C. Remove the master-detail relationship and keep the objects separate.", "Rationale": "This removes the relationship entirely, which is not the requirement. [cite_start]The requirement is to make the relationship optional[cite: 985]."}], "meta": {"source": "QuestionBank", "original_number": 67}}, {"id": "Q68", "number": 68, "question": "Get Cloudy Consulting needs to evaluate the completeness and consistency of contact information in Salesforce. Their sales reps often have incomplete information about their accounts and contacts. Additionally, they are not able to interpret the information in a consistent manner. Get Cloudy Consulting has identified certain \"key\" fields which are important to their sales reps. What are two actions Get Cloudy Consulting can take to review their data for completeness and consistency? (Choose two.)", "options": {"A": "Run a report which shows the last time the key fields were updated.", "B": "Run one report per key field, grouped by that field, to understand its data variability.", "C": "Run a report that shows the percentage of blanks for the important fields.", "D": "Run a process that can fill in default values for blank fields."}, "answer": ["B", "C"], "explanations": [{"Status": "Correct", "Choice": "C. Run a report that shows the percentage of blanks for the important fields.", "Rationale": "[cite_start]**Data Completeness** is measured by identifying missing values[cite: 997]. A report showing the percentage of blank values for key fields is the direct method for evaluating completeness."}, {"Status": "Correct", "Choice": "B. Run one report per key field, grouped by that field, to understand its data variability.", "Rationale": "[cite_start]**Data Consistency** is measured by the uniformity of values[cite: 999]. [cite_start]Grouping a report by a key field allows the analyst to view the *data variability* (e.g., \"Mfg\" vs. \"Manufacturing\") and inconsistent interpretation[cite: 1000]."}, {"Status": "Incorrect", "Choice": "A. Run a report which shows the last time the key fields were updated.", "Rationale": "[cite_start]This measures **timeliness**, not **completeness** or **consistency**[cite: 1002]."}, {"Status": "Incorrect", "Choice": "D. Run a process that can fill in default values for blank fields.", "Rationale": "This is a data *correction* action, not a data *review* or *evaluation* action. The architect first needs to measure the consistency."}], "meta": {"source": "QuestionBank", "original_number": 68}}, {"id": "Q69", "number": 69, "question": "Universal Containers (UC) is facing data quality issues where Sales Reps are creating duplicate customer accounts, contacts, and leads. UC wants to fix this issue immediately by prompting users about a record that possibly exists in Salesforce. UC wants a report regarding duplicate records. What would be the recommended approach to help UC start immediately?", "options": {"A": "Create an after insert and update trigger on the account, contact and lead, and send an error if a duplicate is found using a custom matching criteria.", "B": "Create a duplicate rule for account, lead, and contact, use standard matching rules for these objects, and set the action to report and alert for both creates and edits.", "C": "Create a duplicate rule for account, lead, and contact, use standard matching rules for these objects, and set the action to block for both creates and edits.", "D": "Create a before insert and update trigger on account, contact, and lead, and send an error if a duplicate is found using a custom matching criteria."}, "answer": ["B"], "explanations": [{"Status": "Correct", "Choice": "B. Create a duplicate rule for account, lead, and contact, use standard matching rules for these objects, and set the action to report and alert for both creates and edits.", "Rationale": "[cite_start]**Duplicate Rules** [cite: 1014] are the native, declarative solution to prevent duplicates. [cite_start]Using **Standard Matching Rules** provides the fastest \"start immediately\" implementation[cite: 1015]. [cite_start]Setting the action to **Alert** meets the prompting requirement, and **Report** meets the reporting requirement[cite: 1016]."}, {"Status": "Incorrect", "Choice": "C. Create a duplicate rule for account, lead, and contact, use standard matching rules for these objects, and set the action to block for both creates and edits.", "Rationale": "**Block** is usually too restrictive. [cite_start]The requirement is to **prompt** (alert) users, not stop them entirely[cite: 1018]."}, {"Status": "Incorrect", "Choice": "A. Create an after insert and update trigger...", "Rationale": "[cite_start]This is a custom (coded) solution that requires development time, violating the goal to \"start immediately\" with the simplest approach[cite: 1020]."}, {"Status": "Incorrect", "Choice": "D. Create a before insert and update trigger...", "Rationale": "[cite_start]This is also a custom (coded) solution that requires development time[cite: 1022]."}], "meta": {"source": "QuestionBank", "original_number": 69}}, {"id": "Q70", "number": 70, "question": "Universal Containers (UC) wants to ensure their data on 100,000 Accounts pertaining mostly to US-based companies is enriched and cleansed on an ongoing basis. UC is looking for a solution that allows easy monitoring of key data quality metrics. What should be the recommended solution to meet this requirement?", "options": {"A": "Use a declarative approach by installing and configuring Data.com Clean to monitor Account data quality.", "B": "Implement Batch Apex that calls out a third-party data quality API in order to monitor Account data quality.", "C": "Use declarative approach by installing and configuring Data.com Prospector to monitor Account data quality.", "D": "Implement an Apex Trigger on Account that queries a third-party data quality API to monitor Account data quality."}, "answer": ["A"], "explanations": [{"Status": "Correct", "Choice": "A. Use a declarative approach by installing and configuring Data.com Clean to monitor Account data quality.", "Rationale": "[cite_start]The requirement is for **ongoing enrichment/cleansing** and **easy monitoring** using a **declarative approach**[cite: 1032]. [cite_start]**Data.com Clean** (or similar AppExchange services) is the declarative, dedicated tool that monitors, cleanses, and provides data quality metrics against an external database[cite: 1033]."}, {"Status": "Incorrect", "Choice": "B. Implement Batch Apex that calls out a third-party data quality API in order to monitor Account data quality.", "Rationale": "This is a custom (coded) solution. [cite_start]The requirement asks for a **declarative approach**[cite: 1035]."}, {"Status": "Incorrect", "Choice": "C. Use declarative approach by installing and configuring Data.com Prospector to monitor Account data quality.", "Rationale": "[cite_start]**Data.com Prospector** is used to *find new Leads/Accounts*, not for *monitoring and cleansing* existing Account data[cite: 1037]."}, {"Status": "Incorrect", "Choice": "D. Implement an Apex Trigger on Account that queries a third-party data quality API to monitor Account data quality.", "Rationale": "[cite_start]This is a custom (coded) solution, and Triggers are generally inefficient for mass, ongoing monitoring/cleansing[cite: 1039]."}], "meta": {"source": "QuestionBank", "original_number": 70}}, {"id": "Q71", "number": 71, "question": "Universal Containers (UC) has a requirement to create an Account plan object that is related to the Account object. Each Account plan needs to have an Account object, but the accessibility requirement of the Account plan is different from the Account object. What should an Architect recommend?", "options": {"A": "Create a custom account plan object as detail with Account as mater in a master-detail relationship.", "B": "Create a custom account plan object as detail with Account as master with additional sharing rules to allow access.", "C": "Create an account plan object with a lookup relations to Account without any validation rules to enforce the Account association.", "D": "Create an account plan object with a lookup relationship to Account with validation rules to enforce the Account association."}, "answer": ["D"], "explanations": [{"Status": "Correct", "Choice": "D. Create an account plan object with a lookup relationship to Account with validation rules to enforce the Account association.", "Rationale": "[cite_start]A **Lookup Relationship** allows the Account Plan to have its **own sharing model**, meeting the accessibility requirement[cite: 1048]. [cite_start]A **Validation Rule** must then be added to the Account field (`ISBLANK(Account__c)`) to enforce the requirement that a parent Account must always exist[cite: 1049]."}, {"Status": "Incorrect", "Choice": "A. Create a custom account plan object as detail with Account as mater in a master-detail relationship.", "Rationale": "[cite_start]A **Master-Detail** relationship forces the Account Plan to inherit the Account's sharing model, which violates the requirement for **different accessibility**[cite: 1051]."}, {"Status": "Incorrect", "Choice": "B. Create a custom account plan object as detail with Account as master with additional sharing rules to allow access.", "Rationale": "[cite_start]A Master-Detail forces sharing inheritance; additional sharing rules cannot override this[cite: 1053]."}, {"Status": "Incorrect", "Choice": "C. Create an account plan object with a lookup relations to Account without any validation rules to enforce the Account association.", "Rationale": "A Lookup relationship is optional by default. [cite_start]Without a Validation Rule, the requirement to enforce the Account association will be violated[cite: 1055]."}], "meta": {"source": "QuestionBank", "original_number": 71}}, {"id": "Q72", "number": 72, "question": "Universal Containers (UC) is using Salesforce Sales & Service Cloud for B2C sales and customer service but they are experiencing a lot of duplicate customers in the system. Which are two recommended approaches for UC to avoid duplicate data and increase the level of data quality?", "options": {"A": "Use Duplicate Management.", "B": "Use an Enterprise Service Bus.", "C": "Use Data.com Clean", "D": "Use a data wharehouse."}, "answer": ["A", "C"], "explanations": [{"Status": "Correct", "Choice": "A. Use Duplicate Management.", "Rationale": "[cite_start]**Duplicate Management** (Matching Rules and Duplicate Rules) is the native, declarative, and most direct solution to **proactively avoid** the creation of duplicate records[cite: 1061]."}, {"Status": "Correct", "Choice": "C. Use Data.com Clean", "Rationale": "[cite_start]**Data.com Clean** (or similar third-party enrichment services) is used to **standardize and cleanse** data, which is necessary because inconsistent data (e.g., typos, abbreviations) prevents Duplicate Management from working effectively, thereby increasing the level of data quality[cite: 1062]."}, {"Status": "Incorrect", "Choice": "B. Use an Enterprise Service Bus.", "Rationale": "[cite_start]An ESB is a middleware tool for data transport; it is not a dedicated tool for data quality validation or de-duplication logic itself[cite: 1064]."}, {"Status": "Incorrect", "Choice": "D. Use a data wharehouse.", "Rationale": "[cite_start]A Data Warehouse is typically used for long-term storage and reporting/analytics, not for real-time duplicate avoidance or data quality enforcement[cite: 1065]."}], "meta": {"source": "QuestionBank", "original_number": 72}}, {"id": "Q73", "number": 73, "question": "Universal Containers (UC) has several custom Visualforce applications have been developed in which users are able to edit Opportunity records. UC struggles with data completeness on their Opportunity records and has decided to make certain fields required that have not been in the past. The newly required fields are dependent on the Stage of the Opportunity, such that certain fields are only required once an Opportunity advances to later stages. There are two fields. What is the simplest approach to handle this new requirement?", "options": {"A": "Update the Opportunity page layout to mark these fields as required.", "B": "Use a validation rule for each field that takes the Stage into consideration.", "C": "Update these Opportunity field definitions in Setup to be required.", "D": "Write an Apex trigger that checks each field when records are saved."}, "answer": ["B"], "explanations": [{"Status": "Correct", "Choice": "B. Use a validation rule for each field that takes the Stage into consideration.", "Rationale": "[cite_start]The requirement is for **conditional validation** (required *only* at later stages)[cite: 1076]. [cite_start]A **Validation Rule** is the simplest, low-code, declarative tool to enforce conditional field requirements (`AND(ISPICKVAL(StageName, 'Specific_Stage'), ISBLANK(Required_Field__c))`)[cite: 1077]."}, {"Status": "Incorrect", "Choice": "A. Update the Opportunity page layout to mark these fields as required.", "Rationale": "[cite_start]Page layout required settings are ignored by the API/Visualforce and cannot be conditional based on another field's value[cite: 1079]."}, {"Status": "Incorrect", "Choice": "C. Update these Opportunity field definitions in Setup to be required.", "Rationale": "[cite_start]Making the field required in Setup would enforce it *all the time*, which violates the requirement that it is only required at **later stages**[cite: 1081]."}, {"Status": "Incorrect", "Choice": "D. Write an Apex trigger that checks each field when records are saved.", "Rationale": "[cite_start]This would require custom code, which is unnecessarily complex when a Validation Rule can achieve the same result[cite: 1083]."}], "meta": {"source": "QuestionBank", "original_number": 73}}, {"id": "Q74", "number": 74, "question": "A Customer is migrating 10 million order and 30 million order lines into Salesforce using Bulk API. The Engineer is experiencing time-out errors or long delays querying parents order IDs in Salesforce before importing related order line items. What is the recommended solution?", "options": {"A": "Query only indexed ID field values on the imported order to import related order lines.", "B": "Leverage an External ID from source system orders to import related order lines.", "C": "Leverage Batch Apex to update order ID on related order lines after import.", "D": "Leverage a sequence of numbers on the imported orders to import related order lines."}, "answer": ["B"], "explanations": [{"Status": "Correct", "Choice": "B. Leverage an External ID from source system orders to import related order lines.", "Rationale": "[cite_start]During migration, to link child records (Order Line Items) to parent records (Orders), the best practice is to load the Orders with an **External ID** [cite: 1092] (using the legacy system's Order ID). [cite_start]Then, the Order Line Item load uses this **External ID** for the lookup, **avoiding the massive query** that is causing the time-out errors[cite: 1093]."}, {"Status": "Incorrect", "Choice": "A. Query only indexed ID field values on the imported order to import related order lines.", "Rationale": "[cite_start]Querying 10 million Order IDs, even on an indexed field, is the process that is causing the time-out errors that the solution must avoid[cite: 1095]."}, {"Status": "Incorrect", "Choice": "C. Leverage Batch Apex to update order ID on related order lines after import.", "Rationale": "[cite_start]Updating 30 million Order Line Items after the fact would create a massive performance load and is inefficient[cite: 1097]."}, {"Status": "Incorrect", "Choice": "D. Leverage a sequence of numbers on the imported orders to import related order lines.", "Rationale": "[cite_start]A sequence number is not a unique identifier for mapping relationships across systems[cite: 1099]."}], "meta": {"source": "QuestionBank", "original_number": 74}}, {"id": "Q75", "number": 75, "question": "Universal Containers would like to remove data silos and connect their legacy CRM together with their ERP and with Salesforce. Most of their sales team has already migrated to Salesforce for daily use, although a few users are still on the old CRM until some functionality they require is completed. Which two techniques should be used for smooth interoperability now and in the future.", "options": {"A": "Replicate ongoing changes in the legacy CRM to Salesforce to facilitate a smooth transition when the legacy CRM is eventually retired.", "B": "Specify the legacy CRM as the system of record during transition until it is removed from operation and fully replaced by Salesforce.", "C": "Work with stakeholders to establish a Master Data Management plan for the system of record for specific objects, records, and fields.", "D": "Do not connect Salesforce and the legacy CRM to each other during this transition period, but do allow both to interact with the ERP."}, "answer": ["A", "C"], "explanations": [{"Status": "Correct", "Choice": "C. Work with stakeholders to establish a Master Data Management plan for the system of record for specific objects, records, and fields.", "Rationale": "The problem involves **multiple systems** (legacy CRM, ERP, Salesforce) and a **transition period** where data is duplicated. [cite_start]An **MDM plan** [cite: 1110] [cite_start]is the architectural prerequisite for defining **data ownership** (SOR) [cite: 1110] and ensuring smooth interoperability across the landscape."}, {"Status": "Correct", "Choice": "A. Replicate ongoing changes in the legacy CRM to Salesforce to facilitate a smooth transition when the legacy CRM is eventually retired.", "Rationale": "[cite_start]**Data synchronization** (replication) between the two CRM systems is necessary to keep the users on the old CRM functional while ensuring data consistency in Salesforce, which facilitates a smooth transition[cite: 1112]."}, {"Status": "Incorrect", "Choice": "B. Specify the legacy CRM as the system of record during transition until it is removed from operation and fully replaced by Salesforce.", "Rationale": "[cite_start]The SOR should be defined on a *field-by-field* basis (MDM), not for the entire legacy CRM system[cite: 1114]."}, {"Status": "Incorrect", "Choice": "D. Do not connect Salesforce and the legacy CRM to each other during this transition period, but do allow both to interact with the ERP.", "Rationale": "[cite_start]This creates a complex integration topology (Salesforce <-> ERP, Legacy CRM <-> ERP) and does not solve the data synchronization problem between the two CRM systems[cite: 1116]."}], "meta": {"source": "QuestionBank", "original_number": 75}}, {"id": "Q76", "number": 76, "question": "UC is trying to switch from legacy CRM to salesforce and wants to keep legacy CRM and salesforce in place till all the functionality is deployed in salesforce. The want to keep data in synch b/w Salesforce, legacy CRM and SAP. What is the recommendation?", "options": {"A": "Integrate legacy CRM to salesforce and keep data in synch till new functionality is in place", "B": "Do not integrate legacy CRM to Salesforce, but integrate salesforce to SAP", "C": "Integrate SAP with Salesforce, SAP to legacy CRM but not legacy CRM to Salesforce", "D": "Suggest MDM solution and link MDM to salesforce and SAP"}, "answer": ["D"], "explanations": [{"Status": "Correct", "Choice": "D. Suggest MDM solution and link MDM to salesforce and SAP", "Rationale": "[cite_start]The requirement is to maintain data synchronization between **three disparate systems** (Salesforce, Legacy CRM, SAP) [cite: 1122] during a transition. [cite_start]Integrating all systems through a central **Master Data Management (MDM)** hub using a **hub-and-spoke** model is the most scalable, governable, and effective architecture for cross-system data synchronization[cite: 1123]."}, {"Status": "Incorrect", "Choice": "A. Integrate legacy CRM to salesforce and keep data in synch till new functionality is in place", "Rationale": "This creates complex point-to-point synchronization, which is difficult to manage when a third system (SAP) is involved."}, {"Status": "Incorrect", "Choice": "B. Do not integrate legacy CRM to Salesforce, but integrate salesforce to SAP", "Rationale": "This creates a data silo between the two CRM systems, violating the requirement to keep them in sync."}, {"Status": "Incorrect", "Choice": "C. Integrate SAP with Salesforce, SAP to legacy CRM but not legacy CRM to Salesforce", "Rationale": "This introduces complex and redundant data flows and does not centralize the governance of the data integrity."}], "meta": {"source": "QuestionBank", "original_number": 76}}, {"id": "Q77", "number": 77, "question": "Universal Containers (UC) has an Application custom object, which has tens of millions of records created in the past 5 years. UC needs the last 5 years of data to exist in Salesforce at all times for reporting and queries. UC is currently encountering performance issues when reporting and running queries on this Object using date ranges as filters. Which two options can be used to improve report performance?", "options": {"A": "Ask support to create a skinny table for Application with the necessary reporting fields.", "B": "Add custom indexes to all fields on Application without a standard index.", "C": "Run multiple reports to get different pieces of the data and combine them.", "D": "Add custom indexes to the Date fields used for filtering the report."}, "answer": ["A", "D"], "explanations": [{"Status": "Correct", "Choice": "A. Ask support to create a skinny table for Application with the necessary reporting fields.", "Rationale": "[cite_start]**Skinny Tables** [cite: 1140] [cite_start]are a performance feature that avoids resource-intensive joins and significantly improves report query performance on large objects (tens of millions of records)[cite: 1140]."}, {"Status": "Correct", "Choice": "D. Add custom indexes to the Date fields used for filtering the report.", "Rationale": "The performance issue is specific to filtering by **date ranges**. [cite_start]Ensuring that the date fields used in the report filters are **indexed** makes the query **selective**, which is the most effective way to improve query performance on large data volumes[cite: 1143]."}, {"Status": "Incorrect", "Choice": "B. Add custom indexes to all fields on Application without a standard index.", "Rationale": "[cite_start]Indiscriminately adding custom indexes to too many fields slows down DML operations and is not a focused solution to the reporting performance issue[cite: 1145]."}, {"Status": "Incorrect", "Choice": "C. Run multiple reports to get different pieces of the data and combine them.", "Rationale": "[cite_start]Running multiple reports to combine data is a workaround that doesn't solve the underlying performance issue of the original query[cite: 1147]."}], "meta": {"source": "QuestionBank", "original_number": 77}}, {"id": "Q78", "number": 78, "question": "Universal Containers (UC) is expecting to have nearly 5 million shipments records in its Salesforce org. Each shipment record has up to 10 child shipment item records. The Shipment custom object has an Organization-wide Default (OWD) sharing model set to Private and the Shipment Item custom object has a Master-Detail relationship to Shipment. There are 25 sharing rules set on the Shipment custom object, which allow shipment records to be shared to each of UC's 25 business areas around the globe. These sharing rules use public groups, one for each business area plus a number of groups for management and support roles. UC has a high turnover of Sales Reps and often needs to move Sales Reps between business areas in order to meet local demand. What feature would ensure that performance, when moving Sales Reps between regions, remains adequate while meeting existing requirements?", "options": {"A": "Implement data archiving for old Shipment records.", "B": "Contact Salesforce to create Skinny tables on Shipment.", "C": "Configure shipment OWD to Public Read/Write.", "D": "Contact Salesforce to enable Defer Sharing Rules"}, "answer": ["D"], "explanations": [{"Status": "Correct", "Choice": "D. Contact Salesforce to enable Defer Sharing Rules", "Rationale": "[cite_start]The problem is performance degradation when managing group membership in an environment with high data volume (5 million records) and complex sharing (25 rules)[cite: 1159]. [cite_start]The **Defer Sharing Rules** feature (or Defer Sharing Calculation) allows an admin to disable and re-enable sharing recalculations after bulk changes, which is the most effective way to **ensure performance** when moving large numbers of users between Public Groups[cite: 1160]."}, {"Status": "Incorrect", "Choice": "A. Implement data archiving for old Shipment records.", "Rationale": "[cite_start]Archiving improves query performance but does not address the core performance issue related to the **management of sharing and group membership**[cite: 1162]."}, {"Status": "Incorrect", "Choice": "B. Contact Salesforce to create Skinny tables on Shipment.", "Rationale": "[cite_start]Skinny tables improve **query/report** performance, but do not address the performance impact of **sharing rule recalculations**[cite: 1164]."}, {"Status": "Incorrect", "Choice": "C. Configure shipment OWD to Public Read/Write.", "Rationale": "[cite_start]Changing the OWD would simplify sharing and improve performance, but it would violate the core security requirement for records to be **Private** and shared selectively[cite: 1166]."}], "meta": {"source": "QuestionBank", "original_number": 78}}, {"id": "Q79", "number": 79, "question": "Universal Containers wants to develop a dashboard in Salesforce that will allow Sales Managers to do data exploration using their mobile device (i.e., drill down into sales-related data) and have the possibility of adding ad-hoc filters while on the move. What is a recommended solution for building data exploration dashboards in Salesforce?", "options": {"A": "Create a Dashboard in an external reporting tool, export data to the tool, and add link to the dashboard in Salesforce.", "B": "Create a Dashboard in an external reporting tool, export data to the tool, and embed the dashboard in Salesforce using the Canval toolkit.", "C": "Create a standard Salesforce Dashboard and connect it to reports with the appropriate filters.", "D": "Create a Dashboard using Analytics Cloud that will allow the user to create ad-hoc lenses and drill down."}, "answer": ["D"], "explanations": [{"Status": "Correct", "Choice": "D. Create a Dashboard using Analytics Cloud that will allow the user to create ad-hoc lenses and drill down.", "Rationale": "[cite_start]**Analytics Cloud** (CRM Analytics/Tableau CRM) is the specialized platform for **data exploration** (drill down), **mobile access**, and **ad-hoc filtering** (lenses/explorations), which are all capabilities that far exceed the functionality of standard Salesforce reports and dashboards[cite: 1175]."}, {"Status": "Incorrect", "Choice": "A. Create a Dashboard in an external reporting tool, export data to the tool, and add link to the dashboard in Salesforce.", "Rationale": "[cite_start]This does not provide the seamless, embedded, or mobile-optimized experience required[cite: 1177]."}, {"Status": "Incorrect", "Choice": "B. Create a Dashboard in an external reporting tool, export data to the tool, and embed the dashboard in Salesforce using the Canval toolkit.", "Rationale": "[cite_start]This requires a complex integration (Canvas) and relies on the mobile functionality of a third-party tool[cite: 1179]."}, {"Status": "Incorrect", "Choice": "C. Create a standard Salesforce Dashboard and connect it to reports with the appropriate filters.", "Rationale": "[cite_start]Standard Salesforce dashboards lack the robust **ad-hoc exploration** and **drill-down** features needed by the sales managers[cite: 1181]."}], "meta": {"source": "QuestionBank", "original_number": 79}}, {"id": "Q80", "number": 80, "question": "DreamHouse Realty has a legacy system that captures Branch Offices and Transactions. DreamHouse Realty has 15 Branch Offices. Transactions can relate to any Branch Office. DreamHouse Realty has created hundreds of thousands of Transactions per year. What are two important considerations for the Data Architect in this scenario? (Choose two.)", "options": {"A": "Standard list view in-line editing.", "B": "Limitations on Org data storage.", "C": "Bulk API limitations on picklist fields.", "D": "Limitations on master-detail relationships."}, "answer": ["A", "C"], "explanations": [{"Status": "Correct", "Choice": "A. Standard list view in-line editing.", "Rationale": "Denormalizing the data (using a picklist instead of a lookup relationship) makes the Branch Office name a static value on the Transaction. If Branch Office names change, all historical transaction records would need to be updated. [cite_start]If this update is done via in-line editing, it can be cumbersome and error-prone, which is a major consideration for denormalization[cite: 1191]."}, {"Status": "Correct", "Choice": "C. Bulk API limitations on picklist fields.", "Rationale": "[cite_start]When loading high volumes of data (hundreds of thousands of transactions) using the Bulk API, there are known performance and error risks associated with processing data destined for picklist fields[cite: 1193]."}, {"Status": "Incorrect", "Choice": "B. Limitations on Org data storage.", "Rationale": "[cite_start]Since there are only 15 Branch Offices, the data volume is manageable, and the storage saved by denormalization is negligible[cite: 1195]."}, {"Status": "Incorrect", "Choice": "D. Limitations on master-detail relationships.", "Rationale": "The proposed change eliminates the separate Branch Office object and its relationship, so the limitations of the relationship type are no longer relevant."}], "meta": {"source": "QuestionBank", "original_number": 80}}, {"id": "Q81", "number": 81, "question": "Cloud Kicks is launching a Partner Community, which will allow users to register shipment requests that are then processed by Cloud Kicks employees. Shipment requests contain header information, and then a list of no more than 5 items being shipped. First, Cloud Kicks will introduce its community to 6,000 customers in North America, and then to 24,000 customers worldwide within the next two years. Cloud Kicks expects 12 shipment requests per week per customer, on average, and wants customers to be able to view up to three years of shipment requests and use Salesforce reports. What is the recommended solution for the Cloud Kicks Data Architect to address the requirements?", "options": {"A": "Create an external custom object to track shipment requests and a child external object to track shipment items. External objects are stored off-platform in Heroku\u2019s Postgres database.", "B": "Create an external custom object to track shipment requests with five lookup custom fields for each item being shipped. External objects are stored off-platform in Heroku\u2019s Postgres database.", "C": "Create a custom object to track shipment requests and a child custom object to track shipment items. Implement an archiving process that moves data off-platform after three years.", "D": "Create a custom object to track shipment requests with five lookup custom fields for each item being shipped Implement an archiving process that moves data off-platform after three years."}, "answer": ["C"], "explanations": [{"Status": "Correct", "Choice": "C. Create a custom object to track shipment requests and a child custom object to track shipment items. Implement an archiving process that moves data off-platform after three years.", "Rationale": "[cite_start]The key requirements are **Partner Community access, native reporting, and 3 years of data available in Salesforce**[cite: 1211]. [cite_start]This necessitates storing the data on the platform (`Custom Object`) [cite: 1212] [cite_start]to enable native reporting, combined with a crucial **archiving process** to manage the high data volume long-term[cite: 1212]."}, {"Status": "Incorrect", "Choice": "A. Create an external custom object to track shipment requests and a child external object to track shipment items. External objects are stored off-platform in Heroku\u2019s Postgres database.", "Rationale": "[cite_start]External objects perform poorly when used in Salesforce reporting, which is a key requirement[cite: 1215]."}, {"Status": "Incorrect", "Choice": "B. Create an external custom object to track shipment requests with five lookup custom fields for each item being shipped. External objects are stored off-platform in Heroku\u2019s Postgres database.", "Rationale": "[cite_start]This uses poor data modeling (denormalization) and performs poorly for native reporting[cite: 1218]."}, {"Status": "Incorrect", "Choice": "D. Create a custom object to track shipment requests with five lookup custom fields for each item being shipped Implement an archiving process that moves data off-platform after three years.", "Rationale": "[cite_start]This uses poor data modeling (denormalization) to model the list of 5 items, rather than a correct parent-child relationship[cite: 1220]."}], "meta": {"source": "QuestionBank", "original_number": 81}}, {"id": "Q82", "number": 82, "question": "Universal Containers has successfully migrated 50 million records into five different objects multiple times in a full copy sandbox. The Integration Engineer wants to re-run the test again a month before it goes live into Production. What is the recommended approach to re-run the test?", "options": {"A": "Truncate all 5 objects quickly and re-run the data migration test.", "B": "Refresh the full copy sandbox and re-run the data migration test.", "C": "Hard delete all 5 objects\u2019 data and re-run the data migration test.", "D": "Truncate all 5 objects and hard delete before running the migration test."}, "answer": ["B"], "explanations": [{"Status": "Correct", "Choice": "B. Refresh the full copy sandbox and re-run the data migration test.", "Rationale": "[cite_start]A **Sandbox Refresh** is the only way to quickly and reliably revert the environment to its **original state** (empty target objects, correct configuration, original IDs) [cite: 1229][cite_start], ensuring the test accurately simulates the production cutover[cite: 1229]."}, {"Status": "Incorrect", "Choice": "A. Truncate all 5 objects quickly and re-run the data migration test.", "Rationale": "**Truncate** is not available for standard objects or objects with specific features enabled. [cite_start]It does not completely reset the system state like a refresh[cite: 1232]."}, {"Status": "Incorrect", "Choice": "C. Hard delete all 5 objects\u2019 data and re-run the data migration test.", "Rationale": "[cite_start]**Hard Delete** (via API) is extremely time-consuming for 50 million records [cite: 1234] and does not guarantee a complete system reset."}, {"Status": "Incorrect", "Choice": "D. Truncate all 5 objects and hard delete before running the migration test.", "Rationale": "This is redundant, time-consuming, and still less effective than a refresh."}], "meta": {"source": "QuestionBank", "original_number": 82}}, {"id": "Q83", "number": 83, "question": "Universal Containers is integrating a new Opportunity engagement system with Salesforce. According to their Master Data Management strategy, Salesforce is the system of record for Account, Contact, and Opportunity data. However, there does seem to be valuable Opportunity data in the new system that potentially conflicts with what is stored in Salesforce. What is the recommended course of action to appropriately integrate this new system?", "options": {"A": "The MDM strategy defines Salesforce as the system of record, so Salesforce Opportunity values prevail in all conflicts.", "B": "A policy should be adopted so that the system whose record was most recently updated should prevail in conflicts.", "C": "The Opportunity engagement system should become the system of record for Opportunity records.", "D": "Stakeholders should be brought together to discuss the appropriate data strategy moving forward."}, "answer": ["D"], "explanations": [{"Status": "Correct", "Choice": "D. Stakeholders should be brought together to discuss the appropriate data strategy moving forward.", "Rationale": "[cite_start]The core problem is a **conflict in data ownership** where the existing MDM strategy is challenged[cite: 1247]. [cite_start]The architect's first step is a **governance action**: convene stakeholders (business owners, data stewards) to **redefine and agree** on the appropriate **System of Record (SOR)** and **data survivorship rules** before proceeding with the technical integration[cite: 1248]."}, {"Status": "Incorrect", "Choice": "A. The MDM strategy defines Salesforce as the system of record, so Salesforce Opportunity values prevail in all conflicts.", "Rationale": "[cite_start]This ignores the potential value of the new data and bypasses the necessary governance step of reviewing the policy[cite: 1250]."}, {"Status": "Incorrect", "Choice": "B. A policy should be adopted so that the system whose record was most recently updated should prevail in conflicts.", "Rationale": "[cite_start]The \"last write wins\" policy leads to data chaos and is what an MDM strategy is specifically designed to prevent[cite: 1252]."}, {"Status": "Incorrect", "Choice": "C. The Opportunity engagement system should become the system of record for Opportunity records.", "Rationale": "[cite_start]This is a decision that must be made by the business and governance team, not by the architect alone[cite: 1254]."}], "meta": {"source": "QuestionBank", "original_number": 83}}, {"id": "Q84", "number": 84, "question": "For a production cutover, a large number of Account records will be loaded into Salesforce from a legacy system. The legacy system does not have enough information to determine the Ownership for these Accounts upon initial load. Which two recommended options assign Account ownership to mitigate potential performance problems?", "options": {"A": "Let a \u201csystem user\u201d own all the Account records without assigning any role to this user in Role Hierarchy.", "B": "Let a \u201csystem user\u201d own the Account records and assign this user to the lowest-level role in the Role Hierarchy.", "C": "Let the VP of the Sales department, who will report directly to the senior VP, own all the Account records.", "D": "Let a \u201csystem user\u201d own all the Account records and make this user part of the highest-level role in the Role Hierarchy."}, "answer": ["B", "A"], "explanations": [{"Status": "Correct", "Choice": "B. Let a \u201csystem user\u201d own the Account records and assign this user to the lowest-level role in the Role Hierarchy.", "Rationale": "[cite_start]This is a strategy for mitigating **Owner Skew** performance issues[cite: 1262]. [cite_start]By assigning the system user to the **lowest level of the Role Hierarchy**, the system avoids unnecessary sharing rule calculations that traverse up the hierarchy[cite: 1263]."}, {"Status": "Correct", "Choice": "A. Let a \u201csystem user\u201d own all the Account records without assigning any role to this user in Role Hierarchy.", "Rationale": "[cite_start]Assigning a user without a role also minimizes sharing calculation complexity and is a valid method for mitigating performance issues[cite: 1265]."}, {"Status": "Incorrect", "Choice": "C. Let the VP of the Sales department, who will report directly to the senior VP, own all the Account records.", "Rationale": "Assigning records to a user high in the hierarchy maximizes the number of users whose sharing needs to be calculated and will lead to severe performance degradation."}, {"Status": "Incorrect", "Choice": "D. Let a \u201csystem user\u201d own all the Account records and make this user part of the highest-level role in the Role Hierarchy.", "Rationale": "Assigning a user at the highest level maximizes the role hierarchy traversal during sharing calculations, which guarantees poor performance."}], "meta": {"source": "QuestionBank", "original_number": 84}}, {"id": "Q85", "number": 85, "question": "Universal Containers (UC) is implementing its new Internet of Things technology, which consists of smart containers that provide information on container temperature and humidity updated every 10 minutes back to UC. There are roughly 10,000 containers equipped with this technology with the number expected to increase to 50,000 across the next five years. It is essential that Salesforce user have access to current and historical temperature and humidity data for each container. What is the recommended solution?", "options": {"A": "Create new custom fields for temperature and humidity in the existing Container custom object, as well as an external ID field that is unique for each container. These custom fields are updated when a new measure is received.", "B": "Create a new Container Reading custom object, which is created when a new measure is received for a specific container. The Container Reading custom object has a master-detail relationship to the container object.", "C": "Create a new Lightning Component that displays last humidity and temperature data for a specific container and can also display historical trends obtaining relevant data from UC\u2019s existing data warehouse.", "D": "Create a new Container Reading custom object with a master-detail relationship to Container which is created when a new measure is received for a specific container. Implement an archiving process that runs every hour."}, "answer": ["D"], "explanations": [{"Status": "Correct", "Choice": "D. Create a new Container Reading custom object with a master-detail relationship to Container which is created when a new measure is received for a specific container. Implement an archiving process that runs every hour.", "Rationale": "[cite_start]The volume is extremely high (131 million records over 5 years)[cite: 1282]. [cite_start]Storing **every reading** requires a transactional child object (`Container Reading`)[cite: 1282]. [cite_start]To prevent storage exhaustion, this must be paired with an immediate, aggressive **archiving process**[cite: 1283]."}, {"Status": "Incorrect", "Choice": "A. Create new custom fields for temperature and humidity in the existing Container custom object... These custom fields are updated when a new measure is received.", "Rationale": "[cite_start]This overwrites the previous reading and does not meet the requirement for **historical data** access[cite: 1285]."}, {"Status": "Incorrect", "Choice": "B. Create a new Container Reading custom object... The Container Reading custom object has a master-detail relationship to the container object.", "Rationale": "This is correct data modeling but ignores the **data volume** problem. [cite_start]Storing 131 million records would quickly exceed storage limits[cite: 1288]."}, {"Status": "Incorrect", "Choice": "C. Create a new Lightning Component that displays last humidity and temperature data for a specific container and can also display historical trends obtaining relevant data from UC\u2019s existing data warehouse.", "Rationale": "[cite_start]This is a valid architecture but ignores the requirement to model the data ingestion; it assumes the historical data is already outside of Salesforce[cite: 1290]."}], "meta": {"source": "QuestionBank", "original_number": 85}}, {"id": "Q88", "number": 88, "question": "Universal Containers (UC) has a multi-level master-detail relationship for opportunities, a custom opportunity line item object, and a custom discount request. UC has opportunity as master and custom line item object as detail in master-detail relationship. UC also has a custom line item object as master and a custom discount request object as detail in another master-detail relationship. UC has a requirement to show all sums of discounts across line items at an opportunity level. What is the recommended solution to address these requirements?", "options": {"A": "Use roll-up for the line-item-level summary and a trigger for the opportunity amount summary, as only one level roll-up is allowed.", "B": "Update the master-detail relationships to lookup relationships in order to allow the discount amount to roll up.", "C": "Remove the master-detail relationships and rely completely on workflow/triggers to summarize the discount amount.", "D": "Roll-up discount request amount at the line-item-level and line-item-level summary discount at the opportunity level."}, "answer": ["D"], "explanations": [{"Status": "Correct", "Choice": "D. Roll-up discount request amount at the line-item-level and line-item-level summary discount at the opportunity level.", "Rationale": "[cite_start]Salesforce **Roll-Up Summary fields** support multi-level aggregation, often called a \"double-roll-up\" or \"chain-roll-up\"[cite: 1340]. The solution is to use: 1. Roll up Discount Request Amount (child) to the **Opportunity Line Item** (master). 2. Roll up the summarized Line Item Discount (child) to the **Opportunity** (master)."}, {"Status": "Incorrect", "Choice": "A. Use roll-up for the line-item-level summary and a trigger for the opportunity amount summary, as only one level roll-up is allowed.", "Rationale": "[cite_start]This is a misunderstanding; multi-level roll-ups *are* natively supported by Salesforce[cite: 1344]."}, {"Status": "Incorrect", "Choice": "B. Update the master-detail relationships to lookup relationships in order to allow the discount amount to roll up.", "Rationale": "[cite_start]Roll-Up Summary fields are **only supported by Master-Detail relationships**[cite: 1346]."}, {"Status": "Incorrect", "Choice": "C. Remove the master-detail relationships and rely completely on workflow/triggers to summarize the discount amount.", "Rationale": "[cite_start]Relying on custom code (triggers) is inefficient when a native declarative feature (Roll-Up Summary) is sufficient[cite: 1348]."}], "meta": {"source": "QuestionBank", "original_number": 88}}, {"id": "Q89", "number": 89, "question": "Which two best practices should be followed when using SOSL for searching?", "options": {"A": "Use searches against single Objects for greater speed and accuracy.", "B": "Keep searches specific and avoid wildcards where possible.", "C": "Use SOSL option to ignore custom indexes as search fields are pre-indexed.", "D": "Use Find in \u201cALL FIELDS\u201d for faster searches."}, "answer": ["D"], "explanations": [{"Status": "Correct", "Choice": "B. Keep searches specific and avoid wildcards where possible.", "Rationale": "Wildcards, especially leading wildcards (`%Text`), force the database to perform full-text searches, which are slow. [cite_start]Keeping searches specific and avoiding unnecessary wildcards improves performance[cite: 1356]."}, {"Status": "Correct", "Choice": "D. Use Find in \u201cALL FIELDS\u201d for faster searches.", "Rationale": "SOSL (Salesforce Object Search Language) is a full-text search engine. Using `FIND` across all indexed fields is its intended and fastest mechanism for broad, text-based searches."}, {"Status": "Incorrect", "Choice": "A. Use searches against single Objects for greater speed and accuracy.", "Rationale": "[cite_start]When you know the object, **SOQL** is often preferred over SOSL, but SOSL is designed for cross-object searches[cite: 1360]."}, {"Status": "Incorrect", "Choice": "C. Use SOSL option to ignore custom indexes as search fields are pre-indexed.", "Rationale": "[cite_start]SOSL automatically uses search indexes, and there is no option to ignore them[cite: 1362]."}], "meta": {"source": "QuestionBank", "original_number": 89}}, {"id": "Q90", "number": 90, "question": "Universal Containers (UC) maintains a collection of several million Account records that represent business in the United Sates. As a logistics company, this list is one of the most valuable and important components of UC's business, and the accuracy of shipping addresses is paramount. Recently it has been noticed that too many of the addresses of these businesses are inaccurate, or the businesses don't exist. Which two scalable strategies should UC consider to improve the quality of their Account addresses?", "options": {"A": "Contact each business on the list and ask them to review and update their address information.", "B": "Build a team of employees that validate Accounts by searching the web and making phone calls.", "C": "Integrate with a third-party database or services for address validation and enrichment.", "D": "Leverage Data.com Clean to clean up Account address fields with the D&B database."}, "answer": ["C", "D"], "explanations": [{"Status": "Correct", "Choice": "C. Integrate with a third-party database or services for address validation and enrichment.", "Rationale": "[cite_start]Manual methods are not **scalable** for **several million** records[cite: 1373]. [cite_start]Integrating with an automated third-party data service (like an address verification API) is the most scalable method to validate and correct data[cite: 1374]."}, {"Status": "Correct", "Choice": "D. Leverage Data.com Clean to clean up Account address fields with the D&B database.", "Rationale": "[cite_start]**Data.com Clean** (or its modern equivalent) is the native/AppExchange declarative solution designed for **ongoing cleansing and enrichment** of Account addresses by leveraging external, trusted business databases[cite: 1376]."}, {"Status": "Incorrect", "Choice": "A. Contact each business on the list and ask them to review and update their address information.", "Rationale": "[cite_start]This is a manual process that is not scalable for several million records[cite: 1378]."}, {"Status": "Incorrect", "Choice": "B. Build a team of employees that validate Accounts by searching the web and making phone calls.", "Rationale": "[cite_start]This is a manual process that is not scalable for several million records[cite: 1380]."}], "meta": {"source": "QuestionBank", "original_number": 90}}, {"id": "Q91", "number": 91, "question": "Universal Containers (UC) loads bulk leads and campaigns from third-party lead aggregators on a weekly and monthly basis. The expected lead record volume is SOOK records per week, and the expected campaign records volume is 10K campaigns per week. After the upload, Lead records are shared with various sales agents via sharing rules and added as Campaign members via Apex triggers on Lead creation. UC agents work on leads for 6 months, but want to keep the records in the system for at least 1 year for reference. Compliance requires them to be stored for a minimum of 3 years. After that, data can be deleted. What statement is true with respect to a data archiving strategy for UC?", "options": {"A": "UC can store long-term lead records in custom storage objects to avoid counting against storage limits.", "B": "UC can leverage the Salesforce Data Backup and Recovery feature for data archival needs.", "C": "UC can leverage recycle bin capability, which guarantees record storage for 15 days after deletion.", "D": "UC can leverage a \u201ctier\u201d-based approach to classify the record storage need."}, "answer": ["D"], "explanations": [{"Status": "Correct", "Choice": "D. UC can leverage a \u201ctier\u201d-based approach to classify the record storage need.", "Rationale": "[cite_start]This is the correct high-level strategy for managing high-volume data with different access needs[cite: 1394]. **Tier 1 (Operational/Active):** 0-6 months in Salesforce. **Tier 2 (Reference):** 6 months - 1 year in Salesforce. [cite_start]**Tier 3 (Compliance/Archival):** 1-3 years stored off-platform[cite: 1396]. This classification (tiering) is fundamental to defining the strategy."}, {"Status": "Incorrect", "Choice": "A. UC can store long-term lead records in custom storage objects to avoid counting against storage limits.", "Rationale": "[cite_start]This is incorrect; all custom objects inside Salesforce count toward storage limits[cite: 1398]."}, {"Status": "Incorrect", "Choice": "B. UC can leverage the Salesforce Data Backup and Recovery feature for data archival needs.", "Rationale": "[cite_start]Backup is for recovery; it is not a feature for operational data archival (on-demand access for compliance or reference)[cite: 1400]."}, {"Status": "Incorrect", "Choice": "C. UC can leverage recycle bin capability, which guarantees record storage for 15 days after deletion.", "Rationale": "[cite_start]The Recycle Bin is for temporary deletion and does not meet the 3-year compliance requirement[cite: 1402]."}], "meta": {"source": "QuestionBank", "original_number": 91}}, {"id": "Q92", "number": 92, "question": "Universal Containers (UC) is implementing Salesforce Sales Cloud and Service Cloud. As part of their implementation, they are planning to create a new custom object (Shipments), which will have a lookup relationship to Opportunities. When creating shipment records, Salesforce users need to manually input a customer reference, which is provided by customers, and will be stored in the Customer_Reference_c text custom field. Support agents will likely use this customer reference to search for Shipment records when resolving shipping issues. UC is expecting to have around 5 million shipment records created per year. What is the recommended solution to ensure that support agents using global search and reports can quickly find shipment records?", "options": {"A": "Implement an archiving process for shipment records created after five years.", "B": "Implement an archiving process for shipment records created after three years.", "C": "Set Customer-Reference_c as an External ID (non-unique).", "D": "Set Customer-Reference_c as an External ID (unique)."}, "answer": ["D"], "explanations": [{"Status": "Correct", "Choice": "D. Set Customer-Reference_c as an External ID (unique).", "Rationale": "To ensure quick and efficient retrieval of specific records from a large object (5 million/year) using a non-standard field (Customer Reference), the field must be **indexed**. [cite_start]Setting a field as an **External ID** automatically creates a standard, high-performance index, which optimizes search and query performance[cite: 1414]."}, {"Status": "Incorrect", "Choice": "A. Implement an archiving process for shipment records created after five years.", "Rationale": "[cite_start]Archiving is for managing storage limits, not for optimizing the immediate performance of searching on a field[cite: 1416]."}, {"Status": "Incorrect", "Choice": "B. Implement an archiving process for shipment records created after three years.", "Rationale": "Archiving does not optimize search performance."}, {"Status": "Incorrect", "Choice": "C. Set Customer-Reference_c as an External ID (non-unique).", "Rationale": "While it creates an index, the unique designation is preferred for a reference number that agents will use for quick, single-record lookup."}], "meta": {"source": "QuestionBank", "original_number": 92}}, {"id": "Q94", "number": 94, "question": "Universal Containers (UC) wants to store product data in Salesforce, but the standard Product object does not support the more complex hierarchical structure which is currently being used in the product master system. How can UC modify the standard Product object model to support a hierarchical data structure in order to synchronize product data from the source system to Salesforce?", "options": {"A": "Create a custom lookup filed on the standard Product to reference the child record in the hierarchy.", "B": "Create a custom lookup field on the standard Product to reference the parent record in the hierarchy.", "C": "Create a custom master-detail field on the standard Product to reference the child record in the hierarchy.", "D": "Create an Apex trigger to synchronize the Product Family standard picklist field on the Product object."}, "answer": ["B"], "explanations": [{"Status": "Correct", "Choice": "B. Create a custom lookup field on the standard Product to reference the parent record in the hierarchy.", "Rationale": "[cite_start]To create a self-referencing, multi-level hierarchy, you must create a custom field on the object that points back to itself[cite: 1442]. Creating a **custom Lookup field on the Product object** that references the **Product object itself** establishes the parent-child relationship needed for the hierarchical structure."}, {"Status": "Incorrect", "Choice": "A. Create a custom lookup filed on the standard Product to reference the child record in the hierarchy.", "Rationale": "The Lookup should point from the child to the parent to form the hierarchy structure."}, {"Status": "Incorrect", "Choice": "C. Create a custom master-detail field on the standard Product to reference the child record in the hierarchy.", "Rationale": "[cite_start]You cannot create a self-referencing Master-Detail relationship[cite: 1447]."}, {"Status": "Incorrect", "Choice": "D. Create an Apex trigger to synchronize the Product Family standard picklist field on the Product object.", "Rationale": "[cite_start]The `Product Family` field is a single picklist and cannot be used to model a complex, multi-level hierarchy[cite: 1449]."}], "meta": {"source": "QuestionBank", "original_number": 94}}, {"id": "Q95", "number": 95, "question": "UC is having issues using Informatica Cloud Louder to export 10M Order records. Each Order record has 10 Order Line Items. What two steps can you take to help correct this? Choose two answers.", "options": {"A": "Export in multiple batches", "B": "Export Bulk API in parallel mode", "C": "Use PK Chunking", "D": "Limit Batch to 10K records"}, "answer": ["A", "C"], "explanations": [{"Status": "Correct", "Choice": "C. Use PK Chunking", "Rationale": "[cite_start]The problem is a query issue when exporting a massive volume of data (10M Orders)[cite: 1454]. [cite_start]**PK Chunking** (Primary Key Chunking) is the highly effective solution for breaking down a large query into smaller, selective queries based on the record ID, which prevents timeouts[cite: 1455]."}, {"Status": "Correct", "Choice": "A. Export in multiple batches", "Rationale": "Using an ETL tool to export the data in **multiple batches** (e.g., based on creation date or a custom indexed field) is a general best practice for handling large exports that helps avoid query timeouts."}, {"Status": "Incorrect", "Choice": "B. Export Bulk API in parallel mode", "Rationale": "The Bulk API automatically runs in parallel mode when possible; the failure is due to the non-selective query, not the processing mode."}, {"Status": "Incorrect", "Choice": "D. Limit Batch to 10K records", "Rationale": "The batch size is a setting for the *ETL tool* and does not solve the underlying non-selective query issue that is causing the initial time-out failure."}], "meta": {"source": "QuestionBank", "original_number": 95}}, {"id": "Q96", "number": 96, "question": "Universal Containers would like to have a Service-Level Agreement (SLA) of 1 day for any data loss due to unintentional or malicious updates of records in Salesforce. What approach should be suggested to address this requirement?", "options": {"A": "Build a daily extract job and extract data to on-premise systems for long-term backup and archival purposes.", "B": "Schedule a Weekly Extract Service for key objects and extract data in XL sheets to on-premise systems.", "C": "Store all data in shadow custom objects on any updates and deletes, and extract them as needed.", "D": "Evaluate a third-party AppExchange app, such as OwnBackup or Spanning, etc., for backup and archival purposes."}, "answer": ["D"], "explanations": [{"Status": "Correct", "Choice": "D. Evaluate a third-party AppExchange app, such as OwnBackup or Spanning, etc., for backup and archival purposes.", "Rationale": "[cite_start]The requirement is for a tight **SLA of 1 day for recovery** from data loss[cite: 1469]. [cite_start]Only a dedicated, automated, third-party **Backup and Restore solution** (AppExchange) provides the features for daily backup, fast indexing, granular field-level comparison, and point-in-time recovery to meet a strict SLA[cite: 1470]."}, {"Status": "Incorrect", "Choice": "A. Build a daily extract job and extract data to on-premise systems for long-term backup and archival purposes.", "Rationale": "[cite_start]A custom extract job only provides the raw backup *data*; it does not provide the automated recovery *process* or the sophisticated tools needed to compare data and restore records[cite: 1473]."}, {"Status": "Incorrect", "Choice": "B. Schedule a Weekly Extract Service for key objects and extract data in XL sheets to on-premise systems.", "Rationale": "[cite_start]A *weekly* extract violates the *daily* recovery SLA[cite: 1475]."}, {"Status": "Incorrect", "Choice": "C. Store all data in shadow custom objects on any updates and deletes, and extract them as needed.", "Rationale": "[cite_start]This consumes double the storage and requires complex custom development (triggers) for a recovery solution, which is inefficient[cite: 1477]."}], "meta": {"source": "QuestionBank", "original_number": 96}}, {"id": "Q97", "number": 97, "question": "Universal Containers (UC) has 1,000 accounts and 50,000 opportunities. UC has an enterprise security requirement to export all sales data outside of Salesforce on a weekly basis. The security requirement also calls for exporting key operational data that includes events such as file downloads, logins, logouts, etc. Which two recommended approaches would address the above requirement?", "options": {"A": "Use Field Audit History to capture operational data and extract it to on-premise systems.", "B": "Use Weekly Export to extract transactional data to on-premise systems.", "C": "Use a custom built extract job to extract operational data to on-premise systems.", "D": "Use Event Monitoring to extract event data to on-premise systems."}, "answer": ["B", "D"], "explanations": [{"Status": "Correct", "Choice": "B. Use Weekly Export to extract transactional data to on-premise systems.", "Rationale": "[cite_start]The **Weekly Export** service (Data Export Wizard) is the native, UI-configurable tool for a weekly export of all transactional data (Accounts, Opportunities), meeting the sales data requirement[cite: 1486]."}, {"Status": "Correct", "Choice": "D. Use Event Monitoring to extract event data to on-premise systems.", "Rationale": "[cite_start]**Event Monitoring** is the dedicated Salesforce feature that captures operational data like **file downloads, logins, and logouts**[cite: 1488]. This data can be extracted (via API) to meet the security/operational data requirement."}, {"Status": "Incorrect", "Choice": "A. Use Field Audit History to capture operational data and extract it to on-premise systems.", "Rationale": "[cite_start]Field Audit History tracks *data changes* on records, not the *operational events* (logins, downloads) required[cite: 1491]."}, {"Status": "Incorrect", "Choice": "C. Use a custom built extract job to extract operational data to on-premise systems.", "Rationale": "[cite_start]A custom-built job is unnecessary when Event Monitoring provides the data source directly[cite: 1492]."}], "meta": {"source": "QuestionBank", "original_number": 97}}, {"id": "Q98", "number": 98, "question": "Universal Containers has a public website with several forms that create Lead records in Salesforce using the REST API. When designing these forms, which two techniques will help maintain a high level of data quality?", "options": {"A": "Do client-side validation of phone number and email field formats.", "B": "Prefer picklist form fields over free text fields, where possible.", "C": "Ensure the website visitor is browsing using an HTTPS connection.", "D": "Use cookies to track when visitors submit multiple forms."}, "answer": ["A", "B"], "explanations": [{"Status": "Correct", "Choice": "B. Prefer picklist form fields over free text fields, where possible.", "Rationale": "[cite_start]**Picklists** enforce data consistency and prevent human input errors (typos, abbreviations), which is the most effective way to maintain data quality on input forms[cite: 1502]."}, {"Status": "Correct", "Choice": "A. Do client-side validation of phone number and email field formats.", "Rationale": "[cite_start]Client-side validation (in the web form's code) provides an immediate check for the user, improving the **quality and format** of the data before it is sent to Salesforce[cite: 1504]. This should be complemented by server-side validation (Validation Rules) in Salesforce."}, {"Status": "Incorrect", "Choice": "C. Ensure the website visitor is browsing using an HTTPS connection.", "Rationale": "[cite_start]HTTPS is a **security** measure (encryption in transit), not a **data quality** measure[cite: 1507]."}, {"Status": "Incorrect", "Choice": "D. Use cookies to track when visitors submit multiple forms.", "Rationale": "[cite_start]Cookies are a **tracking/analytics** technique and do not directly improve the quality of the data entered into the fields[cite: 1509]."}], "meta": {"source": "QuestionBank", "original_number": 98}}, {"id": "Q99", "number": 99, "question": "Universal Containers is exporting 40 million Account records from Salesforce using Informatica Cloud. The ETL tool fails and the query log indicates a full table scan time-out failure. What is the recommended solution?", "options": {"A": "Modify the export job header to specify Export-in-Parallel.", "B": "Modify the export job header to specify Sforce-Enable-PKChunking.", "C": "Modify the export query that includes standard index fields(s).", "D": "Modify the export query with LIMIT clause with Batch size 10,000."}, "answer": ["B"], "explanations": [{"Status": "Correct", "Choice": "B. Modify the export job header to specify Sforce-Enable-PKChunking.", "Rationale": "[cite_start]The error is a **full table scan time-out failure** on a large object (40M records)[cite: 1516]. [cite_start]**PK Chunking** (Primary Key Chunking) is the highly effective solution for breaking down a large query into smaller, selective queries based on the record ID, which avoids the time-out[cite: 1517]."}, {"Status": "Incorrect", "Choice": "A. Modify the export job header to specify Export-in-Parallel.", "Rationale": "[cite_start]The Bulk API is already running in parallel; the failure is due to the non-selective query, not the processing mode[cite: 1519]."}, {"Status": "Incorrect", "Choice": "C. Modify the export query that includes standard index fields(s).", "Rationale": "[cite_start]The query is likely already using indexed fields, but the volume prevents the index from being selective enough to avoid a full table scan[cite: 1521]. PK Chunking is the proper scaling solution."}, {"Status": "Incorrect", "Choice": "D. Modify the export query with LIMIT clause with Batch size 10,000.", "Rationale": "[cite_start]This does not solve the underlying non-selective query issue that is causing the initial time-out failure[cite: 1524]."}], "meta": {"source": "QuestionBank", "original_number": 99}}, {"id": "Q100", "number": 100, "question": "Universal Containers (UC) wants to capture information on how data entities are stored within the different applications and systems used within the company. For that purpose, the architecture team decided to create a data dictionary covering the main business domains within UC. Which two common techniques are used building a data dictionary to store information on how business entities are defined?", "options": {"A": "Use Salesforce Object Query Language.", "B": "Use a data definition language.", "C": "Use an entity relationship diagram.", "D": "Use the Salesforce Metadata API"}, "answer": ["C", "D"], "explanations": [{"Status": "Correct", "Choice": "C. Use an entity relationship diagram.", "Rationale": "A **Data Dictionary** is a document that describes the data structure. [cite_start]An **Entity Relationship Diagram (ERD)** is a common technique used to visually represent the entity relationships, which is a core component of the data dictionary[cite: 1532]."}, {"Status": "Correct", "Choice": "D. Use the Salesforce Metadata API", "Rationale": "[cite_start]The **Metadata API** is the programmatic tool used to **extract the field definitions, object schema, and picklist values** from Salesforce, which are the raw materials needed to populate the data dictionary[cite: 1534]."}, {"Status": "Incorrect", "Choice": "A. Use Salesforce Object Query Language.", "Rationale": "[cite_start]SOQL is used to query *data* (records), not to extract *metadata* (schema/definitions) for a data dictionary[cite: 1536]."}, {"Status": "Incorrect", "Choice": "B. Use a data definition language.", "Rationale": "Data Definition Language (DDL) is used to define the schema in SQL databases; it is not the common technique for compiling an enterprise data dictionary document."}], "meta": {"source": "QuestionBank", "original_number": 100}}, {"id": "Q101", "number": 101, "question": "Universal Containers has received complaints that customers are being called by multiple Sales Reps where the second Sales Rep that calls is unaware of the previous call by their coworker. What is a data quality problem that could cause this?", "options": {"A": "Missing phone number on the Contact record.", "B": "Customer phone number has changed on the Contact record.", "C": "Duplicate Contact records exist in the system.", "D": "Duplicate Activity records on a Contact."}, "answer": ["C"], "explanations": [{"Status": "Correct", "Choice": "C. Duplicate Contact records exist in the system.", "Rationale": "[cite_start]The core problem is that two Sales Reps are calling the same person, which means they are interacting with two separate records for the same customer[cite: 1544]. [cite_start]**Duplicate Contact records** is the data quality problem that leads to inconsistent service, wasted effort, and poor customer experience[cite: 1545]."}, {"Status": "Incorrect", "Choice": "A. Missing phone number on the Contact record.", "Rationale": "A missing phone number would prevent the first call, not cause a second, duplicate call."}, {"Status": "Incorrect", "Choice": "B. Customer phone number has changed on the Contact record.", "Rationale": "A changed phone number is a **data accuracy** issue, but doesn't explain why a second, unaware rep is contacting the person."}, {"Status": "Incorrect", "Choice": "D. Duplicate Activity records on a Contact.", "Rationale": "Duplicate activities on one contact would confuse the current rep, but wouldn't cause a different, unaware rep to call the customer."}], "meta": {"source": "QuestionBank", "original_number": 101}}, {"id": "Q102", "number": 102, "question": "Universal Containers (UC) is a major supplier of office supplies. Some products are produced by UC and some by other manufacturers. Recently, a number of customers have complained that product descriptions on the invoices do not match the descriptions in the online catalog and on some of the order confirmations (e.g., \"ballpoint pen\" in the catalog and \"pen\" on the invoice, and item color labels are inconsistent: \"what vs. \"White\" or \"blk\" vs. \"Black\"). All product data is consolidated in the company data warehouse and pushed to Salesforce to generate quotes and invoices. The online catalog and webshop is a Salesforce Customer Community solution. What is a correct technique UC should use to solve the data inconsistency?", "options": {"A": "Change integration to let product master systems update product data directly in Salesforce via the Salesforce API.", "B": "Add custom fields to the Product standard object in Salesforce to store data from the different source systems.", "C": "Define a data taxonomy for product data and apply the taxonomy to the product data in the data warehouse.", "D": "Build Apex Triggers in Salesforce that ensure products have the correct names and labels after data is loaded into salesforce."}, "answer": ["C"], "explanations": [{"Status": "Correct", "Choice": "C. Define a data taxonomy for product data and apply the taxonomy to the product data in the data warehouse.", "Rationale": "[cite_start]The problem is **data inconsistency** (e.g., \"pen\" vs. \"ballpoint pen\" or \"blk\" vs. \"Black\")[cite: 1558]. [cite_start]Since all product data is consolidated and pushed from the **data warehouse**[cite: 1559], the best solution is to fix the data at the source by defining a **data taxonomy** (a set of common data rules and definitions) and applying it to standardize the values *before* they are distributed to Salesforce or any other system."}, {"Status": "Incorrect", "Choice": "A. Change integration to let product master systems update product data directly in Salesforce via the Salesforce API.", "Rationale": "This changes the integration flow but doesn't solve the fundamental problem of inconsistent data coming from the source systems."}, {"Status": "Incorrect", "Choice": "B. Add custom fields to the Product standard object in Salesforce to store data from the different source systems.", "Rationale": "This only pushes the inconsistency into Salesforce, complicating the data model and reporting."}, {"Status": "Incorrect", "Choice": "D. Build Apex Triggers in Salesforce that ensure products have the correct names and labels after data is loaded into salesforce.", "Rationale": "This treats the symptom by using Apex, which is inefficient. The correction should occur at the authoritative data source (the data warehouse)."}], "meta": {"source": "QuestionBank", "original_number": 102}}, {"id": "Q104", "number": 104, "question": "Universal Containers is creating a new B2C service offering for consumers to ship goods across continents. This is in addition to their well-established B2B offering. Their current Salesforce org uses the standard Account object to track B2B customers. They are expecting to have over 50,000,000 consumers over the next five years across their 50 business regions. B2C customers will be individuals. Household data is not required to be stored. What is the recommended data model for consumer account data to be stored in Salesforce?", "options": {"A": "Use the Account object with Person Accounts and a new B2C page layout.", "B": "Use the Account object with a newly created Record Type for B2C customers.", "C": "Create a new picklist value for B2C customers on the Account Type field.", "D": "Use 50 umbrella Accounts for each region, with customers as associated Contacts."}, "answer": ["A"], "explanations": [{"Status": "Correct", "Choice": "A. Use the Account object with Person Accounts and a new B2C page layout.", "Rationale": "[cite_start]**Person Accounts** are the standard, specialized Salesforce feature for modeling individual consumers (B2C)[cite: 1589]. [cite_start]They combine the Account and Contact objects into a single record, which is optimal for a high volume of individual customers (50 million consumers)[cite: 1590]."}, {"Status": "Incorrect", "Choice": "B. Use the Account object with a newly created Record Type for B2C customers.", "Rationale": "This is a standard B2B model that uses the Contact object for the individual. The Person Account model is better suited for B2C."}, {"Status": "Incorrect", "Choice": "C. Create a new picklist value for B2C customers on the Account Type field.", "Rationale": "This does not change the underlying data model structure and is an insufficient method for distinguishing complex business processes."}, {"Status": "Incorrect", "Choice": "D. Use 50 umbrella Accounts for each region, with customers as associated Contacts.", "Rationale": "This creates unnecessary complexity and is inefficient. [cite_start]Furthermore, linking millions of contacts to a handful of \"umbrella\" accounts creates a severe **Account Skew** problem, degrading performance[cite: 1597]."}], "meta": {"source": "QuestionBank", "original_number": 104}}, {"id": "Q105", "number": 105, "question": "Universal Containers (UC) is implementing a new customer categorization process where customers should be assigned to a Gold, Silver, or Bronze category if they've purchased UC\u2019s new support service. Customers are expected to be evenly distributed across all three categories. Currently, UC has around 500,000 customers, and is expecting 1% of existing non-categorized customers to purchase UC's new support service every month over the next five years. What is the recommended solution to ensure long-term performance, bearing in mind the above requirements?", "options": {"A": "Implement a new global picklist custom field with Gold, Silver, and Bronze values and enable it in Account.", "B": "Implement a new picklist custom field in the Account object with Gold, Silver, and Bronze values.", "C": "Implement a new Categories custom object and a master-detail relationship from Account to Category.", "D": "Implement a new Categories custom object and create a lookup field from Account to Category."}, "answer": ["B"], "explanations": [{"Status": "Correct", "Choice": "B. Implement a new picklist custom field in the Account object with Gold, Silver, and Bronze values.", "Rationale": "[cite_start]Since there are **only three category values** and the accounts will be **evenly distributed**[cite: 1606], the data modeling is simple. A direct **picklist field** on the Account object is the simplest, most declarative, and best-performing solution. It avoids unnecessary relationships and object overhead."}, {"Status": "Incorrect", "Choice": "A. Implement a new global picklist custom field with Gold, Silver, and Bronze values and enable it in Account.", "Rationale": "A global picklist is an unnecessary layer of complexity when only one object (Account) is using the field."}, {"Status": "Incorrect", "Choice": "C. Implement a new Categories custom object and a master-detail relationship from Account to Category.", "Rationale": "Creating a separate object for only three stable values is over-modeling. If a Master-Detail relationship is used, it could lead to **Lookup Skew** if too many Account records point to one Category record (though unlikely here, it's a risk of the model)."}, {"Status": "Incorrect", "Choice": "D. Implement a new Categories custom object and create a lookup field from Account to Category.", "Rationale": "This is over-modeling. Separate objects are typically only warranted if the categories change frequently or have many related fields."}], "meta": {"source": "QuestionBank", "original_number": 105}}, {"id": "Q106", "number": 106, "question": "Ursa Major Solar's legacy system has a quarterly accounts receivable report that compiles data from the following: - Accounts - Contacts - Opportunities - Orders - Order Line Items Which issue will an architect have when implementing this in Salesforce?", "options": {"A": "Custom report types CANNOT contain Opportunity data.", "B": "Salesforce does NOT support Orders or Order Line Items.", "C": "Salesforce does NOT allow more than four objects in a single report type.", "D": "A report CANNOT contain data from Accounts and Contacts."}, "answer": ["C"], "explanations": [{"Status": "Correct", "Choice": "C. Salesforce does NOT allow more than four objects in a single report type.", "Rationale": "[cite_start]The report requires data from **five** objects (Account, Contact, Opportunity, Order, Order Line Item)[cite: 1622]. [cite_start]**Custom Report Types** are limited to a maximum of four objects that can be joined together (Primary Object + 3 linked objects)[cite: 1623]."}, {"Status": "Incorrect", "Choice": "A. Custom report types CANNOT contain Opportunity data.", "Rationale": "This is false; Opportunity is a standard object and can be the primary object in a Custom Report Type."}, {"Status": "Incorrect", "Choice": "B. Salesforce does NOT support Orders or Order Line Items.", "Rationale": "This is false; Salesforce has standard objects for both `Order` and `Order Product`."}, {"Status": "Incorrect", "Choice": "D. A report CANNOT contain data from Accounts and Contacts.", "Rationale": "This is false; Accounts and Contacts have a standard lookup relationship and can easily be reported on together."}], "meta": {"source": "QuestionBank", "original_number": 106}}, {"id": "Q107", "number": 107, "question": "Two million Opportunities need to be loaded in different batches into Salesforce using the Bulk API in parallel mode. What should an Architect consider when loading the Opportunity records?", "options": {"A": "Use the Name field values to sort batches.", "B": "Order batches by Auto-number field.", "C": "Create indexes on Opportunity object text fields.", "D": "Group batches by the Accountid field."}, "answer": ["D"], "explanations": [{"Status": "Correct", "Choice": "D. Group batches by the Accountid field.", "Rationale": "When loading child records (**Opportunity**) in **parallel mode**, multiple threads may try to update the Roll-Up Summary fields on the same parent **Account** simultaneously. This causes **row-locking contention**. [cite_start]**Grouping by the Parent ID (`AccountId`)** ensures that all records referencing the same parent are processed together, mitigating locking issues[cite: 1636]."}, {"Status": "Incorrect", "Choice": "A. Use the Name field values to sort batches.", "Rationale": "Sorting by the `Name` field (a non-ID, non-indexed field) provides no performance benefit and will not mitigate row-locking issues."}, {"Status": "Incorrect", "Choice": "B. Order batches by Auto-number field.", "Rationale": "An auto-number field is generated upon insertion and cannot be used for ordering during the load planning stage."}, {"Status": "Incorrect", "Choice": "C. Create indexes on Opportunity object text fields.", "Rationale": "Custom indexes slow down **DML operations (inserts)** and should be removed or deferred during bulk loading."}], "meta": {"source": "QuestionBank", "original_number": 107}}, {"id": "Q108", "number": 108, "question": "Ursa Major Solar has defined a new Data Quality Plan for their Salesforce data. Which two approaches should an Architect recommend to enforce the plan throughout the organization? (Choose two.)", "options": {"A": "Ensure all data is stored in an external system and set up an integration to Salesforce for view-only access.", "B": "Schedule reports that will automatically catch duplicates and merge or delete the records every week.", "C": "Enforce critical business processes by using Workflow, Validation Rules, and Apex code.", "D": "Schedule a weekly dashboard displaying records that are missing information to be sent to managers for review."}, "answer": ["C", "D"], "explanations": [{"Status": "Correct", "Choice": "C. Enforce critical business processes by using Workflow, Validation Rules, and Apex code.", "Rationale": "This is the architectural foundation for **enforcement**. **Validation Rules** enforce data integrity upon save, and **Workflow/Apex** enforce the standardized business processes."}, {"Status": "Correct", "Choice": "D. Schedule a weekly dashboard displaying records that are missing information to be sent to managers for review.", "Rationale": "[cite_start]A dashboard that highlights data quality issues and is delivered to **managers** creates accountability and a crucial feedback loop for **enforcing** the plan organization-wide[cite: 1653]."}, {"Status": "Incorrect", "Choice": "A. Ensure all data is stored in an external system and set up an integration to Salesforce for view-only access.", "Rationale": "This is an integration strategy that does not address how to **enforce** data quality on the data that *is* created or updated within Salesforce."}, {"Status": "Incorrect", "Choice": "B. Schedule reports that will automatically catch duplicates and merge or delete the records every week.", "Rationale": "Automated merging or deletion is too high-risk to be recommended as a standard process for enforcement."}], "meta": {"source": "QuestionBank", "original_number": 108}}, {"id": "Q109", "number": 109, "question": "Dreambouse Realty has a Salesforce deployment that manages Sales, Support, and Marketing efforts in a multi-system ERP environment. The company recently reached the limits of native reports and dashboards and needs options for providing more analytical insights. Which two approaches an Architect should recommend? (Choose two.)", "options": {"A": "Weekly Snapshots", "B": "Einstein Analytics", "C": "Setup Audit Trails", "D": "AppExchange Apps"}, "answer": ["B", "D"], "explanations": [{"Status": "Correct", "Choice": "B. Einstein Analytics", "Rationale": "[cite_start]**Einstein Analytics** (CRM Analytics/Tableau CRM) is the native, dedicated, advanced Business Intelligence platform designed to overcome the limits of native reports and dashboards by handling large datasets and performing complex data preparation and analysis[cite: 1662]."}, {"Status": "Correct", "Choice": "D. AppExchange Apps", "Rationale": "[cite_start]**AppExchange Apps** (specifically third-party BI or reporting solutions) are a standard architectural recommendation to extend native reporting limits and provide more analytical insights[cite: 1664]."}, {"Status": "Incorrect", "Choice": "A. Weekly Snapshots", "Rationale": "Weekly Snapshots are an internal tool used to store historical data. They still run on the native platform with the same limits and are not a solution for providing *more analytical insights*."}, {"Status": "Incorrect", "Choice": "C. Setup Audit Trails", "Rationale": "The Setup Audit Trail tracks **metadata changes**, not the sales or operational data needed for analytical insights."}], "meta": {"source": "QuestionBank", "original_number": 109}}, {"id": "Q110", "number": 110, "question": "Cloud Kicks currently has a Public Read/Write sharing model for the company's Contacts. Cloud Kicks management team requests that only the owner of a contact record be allowed to delete that contact. What should an Architect do to meet these requirements?", "options": {"A": "Set the profile of the users to remove delete permission from the Contact object.", "B": "Check if the current user is NOT the owner by creating a \"before delete\u201d trigger.", "C": "Set the Sharing settings as Public Read Only for the Contact object.", "D": "Check if the current user is NOT the owner by creating a validation rule on the Contact object."}, "answer": ["B"], "explanations": [{"Status": "Correct", "Choice": "B. Check if the current user is NOT the owner by creating a \"before delete\u201d trigger.", "Rationale": "[cite_start]Since the OWD is open (`Public Read/Write`) allowing everyone to **edit**, the only way to block **deletion** conditionally (non-owner cannot delete) is via a **Before Delete Apex Trigger**[cite: 1675]. This code checks the running user ID against the record owner ID and prevents the DML operation if they don't match."}, {"Status": "Incorrect", "Choice": "A. Set the profile of the users to remove delete permission from the Contact object.", "Rationale": "This would prevent *all* users, including the owner, from deleting the record, violating the requirement that the owner be allowed to delete."}, {"Status": "Incorrect", "Choice": "C. Set the Sharing settings as Public Read Only for the Contact object.", "Rationale": "This would prevent non-owners from *editing* the contact, which violates the requirement for an open sharing model allowing all internal users to edit."}, {"Status": "Incorrect", "Choice": "D. Check if the current user is NOT the owner by creating a validation rule on the Contact object.", "Rationale": "**Validation Rules cannot be used to block delete operations**; they only fire on insert/update."}], "meta": {"source": "QuestionBank", "original_number": 110}}, {"id": "Q111", "number": 111, "question": "An Architect needs information about who is creating, changing, or deleting certain fields within the past four months. How can the Architect access this information?", "options": {"A": "Create a field history report for the fields in question.", "B": "After exporting the setup audit trail, find the fields in question.", "C": "After exporting the metadata, search it for the fields in question.", "D": "Remove \"customize application\" permissions from everyone else."}, "answer": ["B"], "explanations": [{"Status": "Correct", "Choice": "B. After exporting the setup audit trail, find the fields in question.", "Rationale": "[cite_start]The **Setup Audit Trail** tracks all **metadata changes** (creation, modification, deletion of objects, fields, etc.) for the last 180 days, including the user who made the change and the timestamp[cite: 1690]."}, {"Status": "Incorrect", "Choice": "A. Create a field history report for the fields in question.", "Rationale": "**Field History Tracking** tracks *data changes* on the *record* (e.g., Contact Name changed), not changes to the *field definition* (e.g., a field was deleted)."}, {"Status": "Incorrect", "Choice": "C. After exporting the metadata, search it for the fields in question.", "Rationale": "Exporting metadata gives you the *current state* of the fields, but not the *history* of who changed them."}, {"Status": "Incorrect", "Choice": "D. Remove \"customize application\" permissions from everyone else.", "Rationale": "This is a security action and does not help find *historical* information."}], "meta": {"source": "QuestionBank", "original_number": 111}}, {"id": "Q112", "number": 112, "question": "Universal Containers has more than 10 million records in the Order_c object. The query has timed out when running a bulk query. What should be considered to resolve query timeout?", "options": {"A": "Tooling API", "B": "PK Chunking", "C": "Metadata API", "D": "Streaming API"}, "answer": ["B"], "explanations": [{"Status": "Correct", "Choice": "B. PK Chunking", "Rationale": "[cite_start]The most effective solution for query timeouts on massive data volumes (10 million records) using the Bulk API is **PK Chunking** (Primary Key Chunking)[cite: 1701]. It automatically splits a bulk query on a large table into multiple batches based on the record ID, ensuring each smaller query remains selective and avoids timeouts."}, {"Status": "Incorrect", "Choice": "A. Tooling API", "Rationale": "The Tooling API is used for development tools and metadata, not for high-volume data extraction."}, {"Status": "Incorrect", "Choice": "C. Metadata API", "Rationale": "The Metadata API is used for metadata deployment, not for data extraction."}, {"Status": "Incorrect", "Choice": "D. Streaming API", "Rationale": "Streaming API is for real-time event processing, not for extracting large volumes of existing data."}], "meta": {"source": "QuestionBank", "original_number": 112}}, {"id": "Q113", "number": 113, "question": "Universal Containers (UC) has a data model as shown in the image. The Project object has a private sharing model, and it has Roll-Up summary fields to calculate the number of resources assigned to the project, total hours for the project, and the number of work items associated to the project. What should the architect consider, knowing there will be a large amount of time entry records to be loaded regularly from an external system into Salesforce.com?", "options": {"A": "Load all data using external IDs to link to parent records.", "B": "Use workflow to calculate summary values instead of Roll-Up.", "C": "Use triggers to calculate summary values instead of Roll-Up.", "D": "Load all data after deferring sharing calculations."}, "answer": ["D"], "explanations": [{"Status": "Correct", "Choice": "D. Load all data after deferring sharing calculations.", "Rationale": "The Project object has a **private sharing model** and **Roll-Up Summary fields**. Loading a large amount of child records will trigger complex and resource-intensive **Sharing Recalculations**. [cite_start]**Deferring sharing calculations** (a feature enabled by Salesforce Support) is the best practice to minimize load time and avoid `UNABLE_TO_LOCK_ROW` errors in orgs with complex sharing[cite: 1716]."}, {"Status": "Incorrect", "Choice": "A. Load all data using external IDs to link to parent records.", "Rationale": "This is standard migration practice, but it does not mitigate the performance hit caused by the Roll-Up Summaries and Sharing Recalculations."}, {"Status": "Incorrect", "Choice": "B. Use workflow to calculate summary values instead of Roll-Up.", "Rationale": "Workflow/Flow cannot calculate aggregate summary values (SUM, COUNT) from child records."}, {"Status": "Incorrect", "Choice": "C. Use triggers to calculate summary values instead of Roll-Up.", "Rationale": "This is custom code and does not solve the fundamental performance bottleneck caused by sharing calculations."}], "meta": {"source": "QuestionBank", "original_number": 113}}, {"id": "Q114", "number": 114, "question": "Which two aspects of data does an Enterprise data governance program aim to improve?", "options": {"A": "Data integrity", "B": "Data distribution", "C": "Data usability", "D": "Data modeling"}, "answer": ["A", "C"], "explanations": [{"Status": "Correct", "Choice": "A. Data integrity", "Rationale": "**Data Integrity** (accuracy, consistency, quality) is a core aspect of data governance. Governance defines the rules to ensure data is trustworthy."}, {"Status": "Correct", "Choice": "C. Data usability", "Rationale": "**Data Usability** (accessibility, discoverability, relevance) is the other core aspect. Governance ensures data is fit for purpose for business consumption."}, {"Status": "Incorrect", "Choice": "B. Data distribution", "Rationale": "Data distribution is a function of **Integration Architecture**, not a core strategic goal of **Data Governance**."}, {"Status": "Incorrect", "Choice": "D. Data modeling", "Rationale": "Data modeling is a function of **Data Architecture**, which is guided by governance, but is not a primary goal *of* the governance program itself."}], "meta": {"source": "QuestionBank", "original_number": 114}}, {"id": "Q115", "number": 115, "question": "Universal Containers (UC) has over 10 million accounts with an average of 20 opportunities with each account. A Sales Executive at UC needs to generate a daily report for all opportunities in a specific opportunity stage. Which two key considerations should be made to make sure the performance of the report is not degraded due to large data volume?", "options": {"A": "Number of queries running at a time.", "B": "Number of joins used in report query.", "C": "Number of records returned by report query.", "D": "Number of characters in report query."}, "answer": ["B", "C"], "explanations": [{"Status": "Correct", "Choice": "B. Number of joins used in report query.", "Rationale": "The number of objects joined in a report query (the complexity of the join) is a major factor in **query degradation** on large data volumes. [cite_start]Fewer joins improve performance[cite: 1738]."}, {"Status": "Correct", "Choice": "C. Number of records returned by report query.", "Rationale": "The total **size of the result set** directly impacts report run time. [cite_start]If the query returns a massive number of records, it increases processing time and risks timeouts[cite: 1740]."}, {"Status": "Incorrect", "Choice": "A. Number of queries running at a time.", "Rationale": "This affects the overall **system capacity**, but is not a key consideration for the degradation of a *single* report's performance."}, {"Status": "Incorrect", "Choice": "D. Number of characters in report query.", "Rationale": "The length of the query text has a negligible impact on report performance."}], "meta": {"source": "QuestionBank", "original_number": 115}}, {"id": "Q119", "number": 119, "question": "Patients: They are individuals who need care. A data architect needs to map the actor to SF objects.", "options": {"A": "Patients as Contacts, Payment providers as Accounts, & Doctors as Accounts", "B": "Patients as Person Accounts, Payment providers as Accounts, & Doctors as Contacts", "C": "Patients as Person Accounts, Payment providers as Accounts, & Doctors as Person Account", "D": "Patients as Accounts, Payment providers as Accounts, & Doctors as Person Accounts"}, "answer": ["C"], "explanations": [{"Status": "Correct", "Choice": "C. Patients as Person Accounts, Payment providers as Accounts, & Doctors as Person Account", "Rationale": "In a B2C healthcare context: **Patients** are individuals (Person Accounts). **Payment Providers** are organizations (Accounts). [cite_start]**Doctors** are individuals involved in the care plan who need access to patient information (Person Accounts)[cite: 1767]."}, {"Status": "Incorrect", "Choice": "A. Patients as Contacts, Payment providers as Accounts, & Doctors as Accounts", "Rationale": "Patients should be Person Accounts in B2C. Doctors should be individuals (Person Accounts/Contacts), not corporate Accounts."}, {"Status": "Incorrect", "Choice": "B. Patients as Person Accounts, Payment providers as Accounts, & Doctors as Contacts", "Rationale": "This is a viable model, but Person Accounts for Doctors is often the optimal choice for individual professionals who may have their own direct records."}, {"Status": "Incorrect", "Choice": "D. Patients as Accounts, Payment providers as Accounts, & Doctors as Person Accounts", "Rationale": "Patients should be Person Accounts, not corporate Accounts."}], "meta": {"source": "QuestionBank", "original_number": 119}}, {"id": "Q121", "number": 121, "question": "Universal Containers (UC) requires 2 years of customer related cases to be available on SF for operational reporting. Any cases older than 2 years and upto 7 years need to be available on demand to the Service agents. UC creates 5 million cases per yr. Which 2 data archiving strategies should a data architect recommend? Choose 2 options:", "options": {"A": "Use custom objects for cases older than 2 years and use nightly batch to move them.", "B": "Sync cases older than 2 years to an external database, and provide access to Service agents to the database.", "C": "Use Big objects for cases older than 2 years, and use nightly batch to move them.", "D": "Use Heroku and external objects to display cases older than 2 years and bulk API to hard delete from Salesforce."}, "answer": ["B", "C"], "explanations": [{"Status": "Correct", "Choice": "B. Sync cases older than 2 years to an external database, and provide access to Service agents to the database", "Rationale": "**Moving data to an external database (off-platform)** is the scalable solution to handle the high volume (5M cases/year) and long retention period (7 years) without consuming expensive Salesforce storage. Service agents can access this external database directly or via a mashup."}, {"Status": "Correct", "Choice": "C. Use Big objects for cases older than 2 years, and use nightly batch to move them.", "Rationale": "[cite_start]**Big Objects** are designed for storing massive amounts of data (archival) within the Salesforce platform efficiently[cite: 1798]. [cite_start]They meet the requirement for historical data to be \"available on demand\" to Service agents within the Salesforce UI, which is a key benefit over external storage[cite: 1799]."}, {"Status": "Incorrect", "Choice": "A. Use custom objects for cases older than 2 years and use nightly batch to move them.", "Rationale": "Storing 25-35 million records (5M/yr * 5-7 yrs) in a standard custom object would quickly exceed storage limits."}, {"Status": "Incorrect", "Choice": "D. Use Heroku and external objects to display cases older than 2 years and bulk API to hard delete from Salesforce.", "Rationale": "This is a flawed hybrid approach. External Objects display data from an external database; Heroku Connect is a syncing solution, not an archival strategy."}], "meta": {"source": "QuestionBank", "original_number": 121}}, {"id": "Q122", "number": 122, "question": "NTO would like to retrieve their SF orgs meta data programmatically for backup within a various external. Which API is the best fit for accomplishing this task?", "options": {"A": "Metadata API", "B": "Tooling API", "C": "Bulk API in serial mode", "D": "SOAP API"}, "answer": ["A"], "explanations": [{"Status": "Correct", "Choice": "A. Metadata API", "Rationale": "[cite_start]The **Metadata API** is the dedicated, official API for retrieving, deploying, and managing **setup and configuration information (metadata)**, including for backup purposes[cite: 1809]."}, {"Status": "Incorrect", "Choice": "B. Tooling API", "Rationale": "The Tooling API is primarily for building custom development tools and interacting with metadata in a more granular way; the Metadata API is better for large-scale programmatic backups."}, {"Status": "Incorrect", "Choice": "C. Bulk API in serial mode", "Rationale": "The Bulk API is used for large volumes of **data** (records), not **metadata**."}, {"Status": "Incorrect", "Choice": "D. SOAP API", "Rationale": "The SOAP API is generally used for transactional DML operations and retrieving small amounts of data, not for bulk metadata retrieval."}], "meta": {"source": "QuestionBank", "original_number": 122}}, {"id": "Q126", "number": 126, "question": "UC has been using SF for 10 years. Lately, users have noticed, that the pages load slowly when viewing Customer and Account list view. To mitigate, UC will implement a data archive strategy to reduce the amount of data actively loaded. Which 2 tasks are required to define the strategy? Choose 2 answers:", "options": {"A": "Identify the recovery point objective.", "B": "Identify how the archive data will be accessed and used.", "C": "Identify the recovery time objective.", "D": "Identify the data retention requirements"}, "answer": ["B", "D"], "explanations": [{"Status": "Correct", "Choice": "B. Identify how the archive data will be accessed and used.", "Rationale": "[cite_start]This task defines the target system for the archived data (e.g., Data Warehouse, Big Objects, Flat Files) and dictates the retrieval mechanism (e.g., real-time, on-demand, reporting)[cite: 1861]."}, {"Status": "Correct", "Choice": "D. Identify the data retention requirements", "Rationale": "[cite_start]This task establishes the minimum amount of time data must be legally or functionally kept (e.g., 7 years for compliance), which drives the entire archiving schedule and purge criteria[cite: 1863]."}, {"Status": "Incorrect", "Choice": "A. Identify the recovery point objective.", "Rationale": "RPO (Recovery Point Objective) and RTO (Recovery Time Objective - option C) are terms associated with **Disaster Recovery (DR)** and **Backup**, not data **archiving and purging**."}, {"Status": "Incorrect", "Choice": "C. Identify the recovery time objective.", "Rationale": "RTO (Recovery Time Objective) is a **Disaster Recovery (DR)** and **Backup** term, not an archiving requirement."}], "meta": {"source": "QuestionBank", "original_number": 126}}, {"id": "Q127", "number": 127, "question": "UC has a classic encryption for Custom fields and is leveraging weekly data reports for data backups. During the data validation of exported data UC discovered that encrypted field values are still being exported as part of data exported. What should a data architect recommend to make sure decrypted values are exported during data export?", "options": {"A": "Set a standard profile for Data Migration user, and assign view encrypted data", "B": "Create another field to copy data from encrypted field and use this field in export", "C": "Leverage Apex class to decrypt data before exporting it.", "D": "Set up a custom profile for data migration user and assign view encrypted data."}, "answer": ["D"], "explanations": [{"Status": "Correct", "Choice": "D. Set up a custom profile for data migration user and assign view encrypted data.", "Rationale": "For fields protected by **Classic Encryption**, the user performing the export must have the **\"View Encrypted Data\"** permission enabled in their Profile or a Permission Set. This allows the user to see the decrypted value upon export."}, {"Status": "Incorrect", "Choice": "A. Set a standard profile for Data Migration user, and assign view encrypted data", "Rationale": "The permission is needed, but best practice is to assign necessary permissions via a dedicated **Custom Profile or Permission Set** for the integration/migration user."}, {"Status": "Incorrect", "Choice": "B. Create another field to copy data from encrypted field and use this field in export", "Rationale": "This is redundant and would create security risks."}, {"Status": "Incorrect", "Choice": "C. Leverage Apex class to decrypt data before exporting it.", "Rationale": "While Apex can decrypt data, enabling the declarative \"View Encrypted Data\" permission is the simplest and most direct solution for bulk export."}], "meta": {"source": "QuestionBank", "original_number": 127}}, {"id": "Q128", "number": 128, "question": "Universal containers is implementing Salesforce lead management. UC Procure lead data from multiple sources and would like to make sure lead data as company profile and location information. Which solution should a data architect recommend to make sure lead data has both profile and location information?", "options": {"A": "Ask sales people to search for populating company profile and location data", "B": "Run reports to identify records which does not have company profile and location data", "C": "Leverage external data providers populate company profile and location data", "D": "Export data out of Salesforce and send to another team to populate company profile and location data"}, "answer": ["C"], "explanations": [{"Status": "Correct", "Choice": "C. Leverage external data providers populate company profile and location data", "Rationale": "To ensure lead data is consistently enriched with **company profile and location information** (firmographic data), the most scalable and accurate method is to use **external data providers** (like those from the AppExchange or integrated services) to automatically cleanse and augment the data upon entry."}, {"Status": "Incorrect", "Choice": "A. Ask sales people to search for populating company profile and location data", "Rationale": "This is a manual process that is inefficient and leads to inconsistent data quality."}, {"Status": "Incorrect", "Choice": "B. Run reports to identify records which does not have company profile and location data", "Rationale": "This is a step for *monitoring* the data quality gap, not a solution for *populating* the missing data."}, {"Status": "Incorrect", "Choice": "D. Export data out of Salesforce and send to another team to populate company profile and location data", "Rationale": "This is a highly inefficient, manual, and slow process."}], "meta": {"source": "QuestionBank", "original_number": 128}}, {"id": "Q129", "number": 129, "question": "UC has millions of case records with case history and SLA data. UC\u2019s compliance team would like historical cases to be accessible for 10 years for Audit purpose. What solution should a data architect recommend?", "options": {"A": "Archive Case data using Salesforce Archiving process", "B": "Purchase more data storage to support case object", "C": "Use a custom object to store archived case data.", "D": "Use a custom Big object to store archived case data."}, "answer": ["D"], "explanations": [{"Status": "Correct", "Choice": "D. Use a custom Big object to store archived case data.", "Rationale": "[cite_start]**Big Objects** are specifically designed to store and manage massive volumes of historical data (millions of records) in a scalable way within the Salesforce platform for **audit and compliance** purposes (10 years)[cite: 1901]."}, {"Status": "Incorrect", "Choice": "A. Archive Case data using Salesforce Archiving process", "Rationale": "Salesforce does not have a general, built-in archival process; it requires custom implementation (e.g., Batch Apex)."}, {"Status": "Incorrect", "Choice": "B. Purchase more data storage to support case object", "Rationale": "This is an expensive solution that does not address the performance degradation that comes with querying massive tables."}, {"Status": "Incorrect", "Choice": "C. Use a custom object to store archived case data.", "Rationale": "Storing millions of records for 10 years in a standard custom object would exceed standard storage limits and cause significant performance issues."}], "meta": {"source": "QuestionBank", "original_number": 129}}, {"id": "Q130", "number": 130, "question": "NTO need to extract 50 million records from a custom object everyday from its Salesforce org. NTO is facing query timeout issues while extracting these records. What should a data architect recommend in order to get around the time out issue?", "options": {"A": "Use a custom auto number and formula field and use that to chunk records while extracting data.", "B": "The REST API to extract data as it automatically chunks records by 200.", "C": "Use ETL tool for extraction of records.", "D": "Ask SF support to increase the query timeout value."}, "answer": ["C"], "explanations": [{"Status": "Correct", "Choice": "C. Use ETL tool for extraction of records.", "Rationale": "While the answer in the source is C, the true technical solution is **PK Chunking with the Bulk API**. [cite_start]However, an **ETL tool** (like Informatica, Mulesoft, etc.) is the correct **architectural component** to handle the orchestration of high-volume extracts (50 million records) daily, and it will automatically leverage the Bulk API with PK Chunking to bypass timeouts[cite: 1916]."}, {"Status": "Incorrect", "Choice": "A. Use a custom auto number and formula field and use that to chunk records while extracting data.", "Rationale": "Custom formula/auto-number fields are not efficient for manual chunking compared to PK Chunking."}, {"Status": "Incorrect", "Choice": "B. The REST API to extract data as it automatically chunks records by 200.", "Rationale": "The REST API is synchronous and limited in volume. Extracting 50 million records daily is highly inefficient and risks hitting API limits."}, {"Status": "Incorrect", "Choice": "D. Ask SF support to increase the query timeout value.", "Rationale": "Salesforce will not increase the standard query timeout limit, as it risks impacting the shared multi-tenant infrastructure."}], "meta": {"source": "QuestionBank", "original_number": 130}}, {"id": "Q132", "number": 132, "question": "UC has a roll-up summary field on Account to calculate the count of contacts associated with an account. During the account load, SF is throwing an \u201cUnable to lock a row\u201d error. Which solution should a data architect recommend, to resolve the error?", "options": {"A": "Leverage data loader platform API to load data.", "B": "Perform Batch job in parallel mode and reduce Batch size", "C": "Perform Batch job in serial mode and reduce batch size", "D": "Defer roll-up summary fields calculation during data migration."}, "answer": ["C"], "explanations": [{"Status": "Correct", "Choice": "C. Perform Batch job in serial mode and reduce batch size", "Rationale": "The \"Unable to lock a row\" error occurs when multiple threads (parallel batches) try to insert child records (Contacts) under the same parent (Account) and simultaneously update the Roll-Up Summary field. [cite_start]Switching to **Serial Mode** and reducing the batch size forces the processing to occur sequentially, eliminating the concurrent row-locking issue[cite: 1948]."}, {"Status": "Incorrect", "Choice": "A. Leverage data loader platform API to load data.", "Rationale": "This is just a different API; it doesn't solve the underlying concurrency issue."}, {"Status": "Incorrect", "Choice": "B. Perform Batch job in parallel mode and reduce Batch size", "Rationale": "Parallel mode is the *cause* of the locking issue in this specific scenario. Reducing the batch size may help slightly, but serial mode is the solution."}, {"Status": "Incorrect", "Choice": "D. Defer roll-up summary fields calculation during data migration.", "Rationale": "Roll-Up Summary fields cannot be deferred (disabled) during migration. Only sharing calculations can be deferred."}], "meta": {"source": "QuestionBank", "original_number": 132}}, {"id": "Q133", "number": 133, "question": "UC is migrating individual customers (B2C) data from legacy systems to SF. There are millions of customers stored as accounts and contacts in legacy database. Which object model should a data architect configure within SF ?", "options": {"A": "Leverage person account object in Salesforce", "B": "Leverage custom person account object in SF", "C": "Leverage custom account and contact object in SF", "D": "Leverage standard account and contact object in SF"}, "answer": ["A"], "explanations": [{"Status": "Correct", "Choice": "A. Leverage person account object in Salesforce", "Rationale": "[cite_start]**Person Accounts** are the standard, specialized Salesforce feature for modeling individual consumers (B2C)[cite: 1960]. They combine the Account and Contact objects into a single record, which is the most effective data model for B2C organizations with high customer volume."}, {"Status": "Incorrect", "Choice": "B. Leverage custom person account object in SF", "Rationale": "There is no \"custom person account object\"; Person Accounts must be enabled on the standard Account object."}, {"Status": "Incorrect", "Choice": "C. Leverage custom account and contact object in SF", "Rationale": "Standard objects should always be used over custom objects if they meet the requirements."}, {"Status": "Incorrect", "Choice": "D. Leverage standard account and contact object in SF", "Rationale": "This is the standard B2B model and is less optimized for B2C than the Person Account model."}], "meta": {"source": "QuestionBank", "original_number": 133}}, {"id": "Q135", "number": 135, "question": "UC is migrating data from legacy system to SF. UC would like to preserve the following information on records being migrated: Date time stamps for created date and last modified date. Ownership of records belonging to inactive users being migrated to Salesforce. Which 2 solutions should a data architect recommends to preserve the date timestamps and ownership on records? Choose 2 answers.", "options": {"A": "Log a case with SF to update these fields", "B": "Enable update records with Inactive Owners Permission", "C": "Enable Set Audit fields upon Record Creation Permission", "D": "Enable modify all and view all permission."}, "answer": ["B", "C"], "explanations": [{"Status": "Correct", "Choice": "B. Enable update records with Inactive Owners Permission", "Rationale": "[cite_start]This permission is necessary to allow the migration user to assign ownership to users who are currently inactive in Salesforce, preserving the historical ownership of records[cite: 1986]."}, {"Status": "Correct", "Choice": "C. Enable Set Audit fields upon Record Creation Permission", "Rationale": "[cite_start]This permission is necessary to allow the migration user to set the `CreatedDate` and `LastModifiedDate` fields on insert, preserving the date time stamps[cite: 1988]."}, {"Status": "Incorrect", "Choice": "A. Log a case with SF to update these fields", "Rationale": "These fields can be set by the migration user declaratively via permission, making a support case unnecessary."}, {"Status": "Incorrect", "Choice": "D. Enable modify all and view all permission.", "Rationale": "These permissions grant broad access to data but do not enable setting the system audit fields or assigning inactive owners."}], "meta": {"source": "QuestionBank", "original_number": 135}}, {"id": "Q136", "number": 136, "question": "UC has migrated its Back-office data into an on-premise database with REST API access. UC recently implemented Sales cloud for its sales organization. But users are complaining about a lack of order data inside SF. UC is concerned about SF storage limits but would still like Sales cloud to have access to the data. Which design patterns should a data architect select to satisfy the requirement?", "options": {"A": "Migrate and persist the data in SF to take advantage of native functionality.", "B": "Use SF Connect to virtualize the data in SF and avoid storage limits.", "C": "Develop a bidirectional integration between the on-premise system and Salesforce.", "D": "Build a UI for the on-premise system and iframe it in Salesforce"}, "answer": ["B"], "explanations": [{"Status": "Correct", "Choice": "B. Use SF Connect to virtualize the data in SF and avoid storage limits.", "Rationale": "[cite_start]**Salesforce Connect** (using External Objects) is the ideal pattern for meeting two key constraints: providing access to data *in Salesforce* while avoiding *Salesforce storage limits* by keeping the data in the external system and accessing it in real-time[cite: 2003]."}, {"Status": "Incorrect", "Choice": "A. Migrate and persist the data in SF to take advantage of native functionality.", "Rationale": "This violates the constraint that UC is concerned about **SF storage limits**."}, {"Status": "Incorrect", "Choice": "C. Develop a bidirectional integration between the on-premise system and Salesforce.", "Rationale": "The requirement is for Sales Cloud to *access* the data, not necessarily to update it bidirectionally. This is an overly complex solution."}, {"Status": "Incorrect", "Choice": "D. Build a UI for the on-premise system and iframe it in Salesforce", "Rationale": "This is the \"mashup\" pattern, which avoids storage limits but provides a clumsy user experience compared to native Salesforce Connect integration."}], "meta": {"source": "QuestionBank", "original_number": 136}}, {"id": "Q137", "number": 137, "question": "NTO has decided to franchise its brand. Upon implementation, 1000 franchisees will be able to access BTO\u2019s product information and track large customer sales and opportunities through a portal. The Franchisees will also be able to run monthly and quarterly sales reports and projections as well as view the reports in dashboards. Which licenses does NTO need to provide these features to the Franchisees?", "options": {"A": "Salesforce Sales Cloud license", "B": "Lightning Platform license", "C": "Customer Community license", "D": "Partner Community license"}, "answer": ["D"], "explanations": [{"Status": "Correct", "Choice": "D. Partner Community license", "Rationale": "The users are **Franchisees (channel resellers)**, who need access to CRM data (Opportunities, Products) to manage their sales channel and run reports. [cite_start]The **Partner Community** license is specifically designed for channel partners and resellers[cite: 2017]."}, {"Status": "Incorrect", "Choice": "A. Salesforce Sales Cloud license", "Rationale": "This is an expensive license intended for internal, full-time employees, not external partners."}, {"Status": "Incorrect", "Choice": "B. Lightning Platform license", "Rationale": "This license provides basic functionality but does not include access to the standard Sales/CRM objects required for the process."}, {"Status": "Incorrect", "Choice": "C. Customer Community license", "Rationale": "This license is for end-users/customers and does not typically include access to Opportunity records or robust reporting capabilities required for channel sales."}], "meta": {"source": "QuestionBank", "original_number": 137}}, {"id": "Q138", "number": 138, "question": "A customer needs a sales model that allows the following: Opportunities need to be assigned to sales people based on the zip code. Each sales person can be assigned to multiple zip codes. Each zip code is assigned to a sales area definition. Sales is aggregated by sales area for reporting. What should a data architect recommend?", "options": {"A": "Assign opportunities using list views using zip code.", "B": "Add custom fields in opportunities for zip code and use assignment rules.", "C": "Allow sales users to manually assign opportunity ownership based on zip code.", "D": "Configure territory management feature to support opportunity assignment."}, "answer": ["D"], "explanations": [{"Status": "Correct", "Choice": "D. Configure territory management feature to support opportunity assignment.", "Rationale": "[cite_start]**Enterprise Territory Management** is the standard feature designed to model complex sales structures based on criteria like **Zip Code**, where a user can belong to **multiple territories**, and territories can be used for reporting and assignment[cite: 2029]."}, {"Status": "Incorrect", "Choice": "A. Assign opportunities using list views using zip code.", "Rationale": "This is a manual, non-scalable process."}, {"Status": "Incorrect", "Choice": "B. Add custom fields in opportunities for zip code and use assignment rules.", "Rationale": "Standard Assignment Rules can only assign based on basic criteria; they cannot handle the complexity of one user assigned to multiple, non-contiguous zip codes."}, {"Status": "Incorrect", "Choice": "C. Allow sales users to manually assign opportunity ownership based on zip code.", "Rationale": "This is a manual, non-scalable process."}], "meta": {"source": "QuestionBank", "original_number": 138}}, {"id": "Q139", "number": 139, "question": "US is implementing salesforce and will be using salesforce to track customer complaints, provide white papers on products and provide subscription (Fee) based support. Which license type will US users need to fulfil US's requirements?", "options": {"A": "Lightning platform starter license.", "B": "Service cloud license.", "C": "Salesforce license.", "D": "Sales cloud license"}, "answer": ["B"], "explanations": [{"Status": "Correct", "Choice": "B. Service cloud license.", "Rationale": "The requirements revolve around **customer complaints** (Cases), **support** (Service Cloud), and access to **products** (Knowledge). [cite_start]The **Service Cloud** license is the primary license that grants access to the core Service Cloud objects and features needed to manage a support organization[cite: 2040]."}, {"Status": "Incorrect", "Choice": "A. Lightning platform starter license.", "Rationale": "This license is for custom apps and does not include access to the core standard Service Cloud objects (Case)."}, {"Status": "Incorrect", "Choice": "C. Salesforce license.", "Rationale": "This is a general license that typically refers to the full Sales Cloud license. While viable, the Service Cloud license is specifically aligned with the requirements."}, {"Status": "Incorrect", "Choice": "D. Sales cloud license", "Rationale": "The core requirements are focused on **support/service**, not sales."}], "meta": {"source": "QuestionBank", "original_number": 139}}, {"id": "Q140", "number": 140, "question": "US has released a new disaster recovery (DR)policy that states that cloud solutions need a business continuity plan in place separate from the cloud providers built in data recovery solution. Which solution should a data architect use to comply with the DR policy?", "options": {"A": "Leverage a 3rd party tool that extract salesforce data/metadata and stores the information in an external protected system.", "B": "Leverage salesforce weekly exports, and store data in Flat files on a protected system.", "C": "Utilize an ETL tool to migrate data to an on-premise archive solution.", "D": "Write a custom batch job to extract data changes nightly, and store in an external protected system."}, "answer": ["A"], "explanations": [{"Status": "Correct", "Choice": "A. Leverage a 3rd party tool that extract salesforce data/metadata and stores the information in an external protected system.", "Rationale": "[cite_start]A dedicated, third-party **Backup and Restore solution** (AppExchange) is the only recommended solution that provides a robust, fully automated, and separate business continuity plan that includes both **data and metadata** extraction with recovery capability[cite: 2054]."}, {"Status": "Incorrect", "Choice": "B. Leverage salesforce weekly exports, and store data in Flat files on a protected system.", "Rationale": "Weekly exports violate the daily backup best practice and do not include **metadata**, which is required for a full recovery/continuity plan."}, {"Status": "Incorrect", "Choice": "C. Utilize an ETL tool to migrate data to an on-premise archive solution.", "Rationale": "This is an archiving strategy, not a backup and disaster recovery solution with restore capabilities."}, {"Status": "Incorrect", "Choice": "D. Write a custom batch job to extract data changes nightly, and store in an external protected system.", "Rationale": "Writing a custom solution is high effort and high risk, and still lacks the certified restore capabilities of a dedicated third-party tool."}], "meta": {"source": "QuestionBank", "original_number": 140}}, {"id": "Q141", "number": 141, "question": "NTO (Northern Trail Outfitters) has multiple salesforce orgs based on geographical reports (AMER, EMEA, APAC). NTO products are in the AMER org and need to be created in the EMEA and APAC after the products are approved. Which two features should a data architect recommend to share records between salesforce orgs? Choose 2.", "options": {"A": "Change data capture (CDC)", "B": "Salesforce connect.", "C": "Federation search", "D": "Salesforce 2 Salesforce"}, "answer": ["A", "D"], "explanations": [{"Status": "Correct", "Choice": "A. Change data capture (CDC)", "Rationale": "[cite_start]**CDC** is the high-performance, real-time method for Salesforce to **push** notifications of record changes (new Products) to external subscribers (the EMEA and APAC orgs, likely via a middleware/Integration Hub), which can then create the records[cite: 2067]."}, {"Status": "Correct", "Choice": "D. Salesforce 2 Salesforce", "Rationale": "[cite_start]**Salesforce to Salesforce** is the native, declarative feature designed specifically to **share data** between two different Salesforce organizations, meeting the requirement directly[cite: 2068]."}, {"Status": "Incorrect", "Choice": "B. Salesforce connect.", "Rationale": "Salesforce Connect is for *real-time viewing* of external data, not for *replicating* a copy of the Product record into the other orgs."}, {"Status": "Incorrect", "Choice": "C. Federation search", "Rationale": "Federation Search is for searching across systems, not for replicating data."}], "meta": {"source": "QuestionBank", "original_number": 141}}, {"id": "Q142", "number": 142, "question": "NTO has been using salesforce for sales and service for 10 years. For the past 2 years, the marketing group has noticed a raise from 0 to 35 % in returned mail when sending mail using the contact information stored in salesforce. Which solution should the data architect use to reduce the amount of returned mails?", "options": {"A": "Use a 3rd-party data source to update contact information in salesforce.", "B": "Email all customer and asked them to verify their information and to call NTO if their address is incorrect.", "C": "Delete contacts when the mail is returned to save postal cost to NTO.", "D": "Have the sales team to call all existing customers and ask to verify the contact details."}, "answer": ["A"], "explanations": [{"Status": "Correct", "Choice": "A. Use a 3rd-party data source to update contact information in salesforce.", "Rationale": "[cite_start]The most scalable and effective way to ensure high **data accuracy** (correct addresses) and reduce returned mail is to integrate with a **third-party data service** that validates, corrects, and augments addresses (cleansing) on an ongoing basis[cite: 2080]."}, {"Status": "Incorrect", "Choice": "B. Email all customer and asked them to verify their information and to call NTO if their address is incorrect.", "Rationale": "This relies on manual effort and customer compliance, which is not a guaranteed or scalable solution."}, {"Status": "Incorrect", "Choice": "C. Delete contacts when the mail is returned to save postal cost to NTO.", "Rationale": "Deleting records due to poor data quality is poor governance and risks losing valuable customer data."}, {"Status": "Incorrect", "Choice": "D. Have the sales team to call all existing customers and ask to verify the contact details.", "Rationale": "This is a manual, non-scalable process that is costly and inefficient."}], "meta": {"source": "QuestionBank", "original_number": 142}}, {"id": "Q145", "number": 145, "question": "What 2 data management policies does the data classification feature allow customers to classify in salesforce? Choose 2 answers:", "options": {"A": "Reference data policy.", "B": "Data governance policy.", "C": "Data sensitivity policy.", "D": "Compliance categorization policy."}, "answer": ["C", "D"], "explanations": [{"Status": "Correct", "Choice": "C. Data sensitivity policy.", "Rationale": "[cite_start]The Data Classification feature (available on standard/custom fields) allows fields to be classified by **Data Sensitivity Level** (e.g., Public, Confidential, Restricted)[cite: 2132]."}, {"Status": "Correct", "Choice": "D. Compliance categorization policy.", "Rationale": "[cite_start]The feature also allows fields to be categorized by **Compliance Category** (e.g., PII, HIPAA, GDPR), which is necessary for compliance reporting[cite: 2133]."}, {"Status": "Incorrect", "Choice": "A. Reference data policy.", "Rationale": "Reference data is not a classification policy type."}, {"Status": "Incorrect", "Choice": "B. Data governance policy.", "Rationale": "Data governance is the overarching framework, not a field classification category."}], "meta": {"source": "QuestionBank", "original_number": 145}}, {"id": "Q148", "number": 148, "question": "The MDM solution provides de-duplication features, so it acts as the single source of truth. How should a data architect implement the storage of master key within salesforce?", "options": {"A": "Store the master key in Heroku postgres and use Heroku connect for synchronization.", "B": "Create a custom object to store the master key with a lookup field to contact.", "C": "Create an external object to store the master key with a lookup field to contact.", "D": "Store the master key on the contact object as an external ID (Field for referential imports)"}, "answer": ["D"], "explanations": [{"Status": "Correct", "Choice": "D. Store the master key on the contact object as an external ID (Field for referential imports)", "Rationale": "The **MDM Master Key** is the unique identifier (Global ID) for the customer across the enterprise. [cite_start]It should be stored directly on the corresponding Salesforce record (**Contact**) as an **External ID** field[cite: 2157]. This enables easy de-duplication, searching, and provides the key for integration lookups."}, {"Status": "Incorrect", "Choice": "A. Store the master key in Heroku postgres and use Heroku connect for synchronization.", "Rationale": "This would complicate lookups from Salesforce and is inefficient for a master key."}, {"Status": "Incorrect", "Choice": "B. Create a custom object to store the master key with a lookup field to contact.", "Rationale": "Creating a custom object for a single ID field is unnecessary data modeling overhead."}, {"Status": "Incorrect", "Choice": "C. Create an external object to store the master key with a lookup field to contact.", "Rationale": "An External Object is for real-time external data *viewing*, not for persistent storage of the customer's master reference key."}], "meta": {"source": "QuestionBank", "original_number": 148}}, {"id": "Q149", "number": 149, "question": "UC has large amount of orders coming in from its online portal. Historically all order are assigned to a generic user. Which 2 measures should data architect recommend to avoid any performance issues while working with large number of order records? Choose 2 answers:", "options": {"A": "Clear the role field in the generic user record.", "B": "Salesforce handles the assignment of orders automatically and there is no performance impact.", "C": "Create a role at top of role hierarchy and assign the role to the generic user.", "D": "Create a pool of generic users and distribute the assignment of memory to the pool of users."}, "answer": ["A", "D"], "explanations": [{"Status": "Correct", "Choice": "A. Clear the role field in the generic user record.", "Rationale": "[cite_start]If the generic user is placed **without a role**, the complexity of the sharing calculation is minimized, which mitigates performance issues caused by **Owner Skew**[cite: 2173]."}, {"Status": "Correct", "Choice": "D. Create a pool of generic users and distribute the assignment of memory to the pool of users.", "Rationale": "To avoid severe **Owner Skew** (one user owning all records), the records must be distributed among a **pool of generic users**. [cite_start]This spreads the load of sharing calculations and DML operations across multiple users[cite: 2176]."}, {"Status": "Incorrect", "Choice": "C. Create a role at top of role hierarchy and assign the role to the generic user.", "Rationale": "Assigning the user to a role high in the hierarchy **maximizes** the number of sharing calculations, leading to the *worst* possible performance."}, {"Status": "Incorrect", "Choice": "B. Salesforce handles the assignment of orders automatically and there is no performance impact.", "Rationale": "This is false; assigning all records to one user creates severe Owner Skew, leading to significant performance impact."}], "meta": {"source": "QuestionBank", "original_number": 149}}, {"id": "Q151", "number": 151, "question": "NTO has decided that it is going to build a channel sales portal with the following requirements: External resellers are able to authenticate to the portal with a login. Lead data, opportunity data and order data are available to authenticated users. Authenticated users many need to run reports and dashboards. There is no need for more than 10 custom objects or additional file storage. Which community cloud license type should a data architect recommend to meet the portal requirements?", "options": {"A": "Customer community.", "B": "Lightning external apps starter.", "C": "Customer community plus.", "D": "Partner community."}, "answer": ["D"], "explanations": [{"Status": "Correct", "Choice": "D. Partner community.", "Rationale": "[cite_start]The requirements (external resellers, access to Lead/Opportunity/Order data, and robust reporting/dashboards) are the exact use case for the **Partner Community** license[cite: 2191]. This license is designed for channel partners and includes access to Sales Cloud objects."}, {"Status": "Incorrect", "Choice": "A. Customer community.", "Rationale": "[cite_start]This license is for end-users/customers and does not typically include access to Opportunity or Lead objects, nor the full reporting capabilities required[cite: 2193]."}, {"Status": "Incorrect", "Choice": "B. Lightning external apps starter.", "Rationale": "This license is for custom apps and provides very limited access to standard objects and reporting features."}, {"Status": "Incorrect", "Choice": "C. Customer community plus.", "Rationale": "[cite_start]While it offers more robust sharing and reporting than the standard customer license, the Partner license is the most direct and appropriate fit for channel sales management[cite: 2196]."}], "meta": {"source": "QuestionBank", "original_number": 151}}, {"id": "Q152", "number": 152, "question": "UC is implementing sales cloud for patient management and would like to encrypt sensitive patient records being stored in files. Which solution should a data architect recommend to solve this requirement?", "options": {"A": "Implement shield platform encryption to encrypt files.", "B": "Use classic encryption to encrypt files.", "C": "Implement 3rd party App Exchange app to encrypt files.", "D": "Store files outside of salesforce and access them to real time."}, "answer": ["A"], "explanations": [{"Status": "Correct", "Choice": "A. Implement shield platform encryption to encrypt files.", "Rationale": "**Shield Platform Encryption** is the native, comprehensive encryption solution for Salesforce. [cite_start]It can encrypt sensitive data at rest, including data stored in fields, search indexes, and **Files** (attachments/documents)[cite: 2203]."}, {"Status": "Incorrect", "Choice": "B. Use classic encryption to encrypt files.", "Rationale": "Classic encryption is limited to specific custom fields and cannot be applied to Files."}, {"Status": "Incorrect", "Choice": "C. Implement 3rd party App Exchange app to encrypt files.", "Rationale": "[cite_start]While possible, Shield Platform Encryption is the recommended native solution for file encryption[cite: 2207]."}, {"Status": "Incorrect", "Choice": "D. Store files outside of salesforce and access them to real time.", "Rationale": "This avoids encryption but requires complex integration and violates the pattern if files must reside in Salesforce."}], "meta": {"source": "QuestionBank", "original_number": 152}}, {"id": "Q153", "number": 153, "question": "UC recently migrated 1 Billion customer related records from a legacy data store to Heroku Postgres. A subset of the data need to be synchronized with salesforce so that service agents are able to support customers directly within the service console. The remaining non- synchronized set of data will need to be accessed by salesforce at any point in time, but UC management is concerned about storage limitations. What should a data architect recommend to meet these requirements with minimal effort?", "options": {"A": "Virtualize the remaining set of data with salesforce connect and external objects.", "B": "Use Heroku connect to bi-directional, sync all data between systems.", "C": "As needed, make call outs into Heroku postgres and persist the data in salesforce.", "D": "Migrate the data to big objects and leverage async SOQL with custom objects."}, "answer": ["A"], "explanations": [{"Status": "Correct", "Choice": "A. Virtualize the remaining set of data with salesforce connect and external objects.", "Rationale": "The data is massive and already in Heroku Postgres. [cite_start]**Salesforce Connect** allows the large, non-synchronized portion of the data to be **virtualized** (viewed in real-time via External Objects) without being copied, directly addressing the concern about **storage limitations**[cite: 2220]. (Heroku Connect is used for syncing the *subset*, but Salesforce Connect/External Objects handles the large, remaining set)."}, {"Status": "Incorrect", "Choice": "B. Use Heroku connect to bi-directional, sync all data between systems.", "Rationale": "[cite_start]Heroku Connect would copy all 1 Billion records, violating the storage limitations constraint[cite: 2223]."}, {"Status": "Incorrect", "Choice": "C. As needed, make call outs into Heroku postgres and persist the data in salesforce.", "Rationale": "[cite_start]This requires complex custom Apex coding and development effort[cite: 2225]."}, {"Status": "Incorrect", "Choice": "D. Migrate the data to big objects and leverage async SOQL with custom objects.", "Rationale": "Big Objects are stored *in* Salesforce and count towards specialized limits. [cite_start]Moving 1 billion records is inefficient compared to virtualization[cite: 2228]."}], "meta": {"source": "QuestionBank", "original_number": 153}}, {"id": "Q154", "number": 154, "question": "UC is building a salesforce application to track contacts and their respective conferences that they have attended with the following requirements: Contacts will be stored in the standard contact object. Conferences will be stored in a custom conference_c object. Each contact may attend multiple conferences and each conference may be related to multiple contacts. How should a data architect model the relationship between the contact and conference objects?", "options": {"A": "Implement a Contact Conference junction object with master detail relationship to both contact and conference__c.", "B": "Create a master detail relationship field on the Contact object.", "C": "Create a master detail relationship field on the Conference object.", "D": "Create a lookup relationship field on contact object."}, "answer": ["A"], "explanations": [{"Status": "Correct", "Choice": "A. Implement a Contact Conference junction object with master detail relationship to both contact and conference__c.", "Rationale": "The relationship is Many-to-Many (\"Each contact may attend multiple conferences and each conference may be related to multiple contacts\"). [cite_start]The architectural solution for a Many-to-Many relationship is a **Junction Object** that links the two primary objects, usually with Master-Detail relationships to both parents[cite: 2239]."}, {"Status": "Incorrect", "Choice": "B. Create a master detail relationship field on the Contact object.", "Rationale": "This would make Contact the detail and Conference the master, which models a One-to-Many relationship (one contact attends many conferences, but a conference can only be attended by one contact)."}, {"Status": "Incorrect", "Choice": "C. Create a master detail relationship field on the Conference object.", "Rationale": "This models a One-to-Many relationship (one conference related to many contacts, but a contact can only attend one conference)."}, {"Status": "Incorrect", "Choice": "D. Create a lookup relationship field on contact object.", "Rationale": "This models a One-to-Many relationship and does not solve the Many-to-Many requirement."}], "meta": {"source": "QuestionBank", "original_number": 154}}, {"id": "Q155", "number": 155, "question": "UC has a requirement to migrate 100 million order records from a legacy ERP application into the salesforce platform. UC does not have any requirements around reporting on the migrated data. What should a data architect recommend to reduce the performance degradation of the platform?", "options": {"A": "Create a custom object to store the data.", "B": "Use a standard big object defined by salesforce.", "C": "Use the standard \u201cOrder\u201d object to store the data.", "D": "Implement a custom big object to store the data."}, "answer": ["D"], "explanations": [{"Status": "Correct", "Choice": "D. Implement a custom big object to store the data.", "Rationale": "The data volume is massive (**100 million records**) and there are **no reporting requirements**. [cite_start]**Custom Big Objects** are specifically designed to store massive volumes of read-only data, minimizing performance degradation on the transactional part of the platform[cite: 2255]."}, {"Status": "Incorrect", "Choice": "A. Create a custom object to store the data.", "Rationale": "Storing 100 million records in a standard custom object would immediately exceed storage limits and severely degrade performance."}, {"Status": "Incorrect", "Choice": "B. Use a standard big object defined by salesforce.", "Rationale": "Standard Big Objects are predefined (like FieldHistoryArchive) and cannot be used to store custom Order records."}, {"Status": "Incorrect", "Choice": "C. Use the standard \u201cOrder\u201d object to store the data.", "Rationale": "Storing 100 million records in the standard Order object would immediately cause major performance degradation."}], "meta": {"source": "QuestionBank", "original_number": 155}}, {"id": "Q157", "number": 157, "question": "UC has millions of Cases and are running out of storage. Some user groups need to have access to historical cases for up to 7 years. Which 2 solutions should a data architect recommend in order to minimize performance and storage issues? Choose 2 answers:", "options": {"A": "Export data out of salesforce and store in Flat files on external system.", "B": "Create a custom object to store case history and run reports on it.", "C": "Leverage on premise data archival and build integration to view archived data.", "D": "Leverage big object to archive case data and lightning components to show archived data."}, "answer": ["C", "D"], "explanations": [{"Status": "Correct", "Choice": "C. Leverage on premise data archival and build integration to view archived data.", "Rationale": "**Archiving to an on-premise external system** is the most scalable way to minimize **storage issues** for millions of records over 7 years. [cite_start]**Building an integration** (via Salesforce Connect, mashup, etc.) allows users to view the data on demand[cite: 2294]."}, {"Status": "Correct", "Choice": "D. Leverage big object to archive case data and lightning components to show archived data.", "Rationale": "**Big Objects** are designed to minimize **storage and performance issues** for massive archival data within the Salesforce platform. [cite_start]**Lightning Components** are used to provide the necessary UI access to query and display Big Object data to users[cite: 2297]."}, {"Status": "Incorrect", "Choice": "A. Export data out of salesforce and store in Flat files on external system.", "Rationale": "Flat files are difficult to search, retrieve, and view on demand, violating the accessibility requirement."}, {"Status": "Incorrect", "Choice": "B. Create a custom object to store case history and run reports on it.", "Rationale": "Storing millions of records for 7 years in a standard custom object would exceed standard storage limits and severely degrade performance."}], "meta": {"source": "QuestionBank", "original_number": 157}}, {"id": "Q158", "number": 158, "question": "What should a data architect do to provide additional guidance for users when they enter information in a standard field?", "options": {"A": "Provide custom help text under field properties.", "B": "Create a custom page with help text for user guidance.", "C": "Add custom help text in default value for the field.", "D": "Add a label field with help text adjacent to the custom field."}, "answer": ["A"], "explanations": [{"Status": "Correct", "Choice": "A. Provide custom help text under field properties.", "Rationale": "**Custom Help Text** is the native, declarative feature designed to display contextual guidance to users when they hover over a standard or custom field label. [cite_start]This is the simplest and most direct solution for providing user guidance[cite: 2308]."}, {"Status": "Incorrect", "Choice": "B. Create a custom page with help text for user guidance.", "Rationale": "This is an overly complex, coded solution for simple field guidance."}, {"Status": "Incorrect", "Choice": "C. Add custom help text in default value for the field.", "Rationale": "The default value is intended for data, not user guidance."}, {"Status": "Incorrect", "Choice": "D. Add a label field with help text adjacent to the custom field.", "Rationale": "You cannot add a custom label adjacent to a standard field; this is a cumbersome workaround."}], "meta": {"source": "QuestionBank", "original_number": 158}}, {"id": "Q159", "number": 159, "question": "Universal Containers (UC) is going thought major reorganization of their sales team. This would require changes to a large a number of group members and sharing rules. UCs administrator is concerned about long processing time and failure during the process. What should a Data architect implement to make changes efficiently?", "options": {"A": "Log a case with salesforce to make sharing rule changes.", "B": "Enable Defer Sharing Calculation prior to making sharing rule changes.", "C": "Delete old sharing rules and build new sharing rules", "D": "Log out all users and make changes to sharing rules."}, "answer": ["B"], "explanations": [{"Status": "Correct", "Choice": "B. Enable Defer Sharing Calculation prior to making sharing rule changes.", "Rationale": "When making **bulk changes to users, roles, or sharing rules** in a large data volume environment, the most efficient method is to contact Salesforce Support to enable the **Defer Sharing Calculation** feature. [cite_start]This temporarily suspends the intensive sharing recalculation process and runs it once after all changes are complete[cite: 2325]."}, {"Status": "Incorrect", "Choice": "A. Log a case with salesforce to make sharing rule changes.", "Rationale": "Logging a case is only necessary to *enable* the Defer Sharing Calculation feature, not to make the changes themselves."}, {"Status": "Incorrect", "Choice": "C. Delete old sharing rules and build new sharing rules", "Rationale": "This is a required step for simplifying the complexity, but it does not prevent the long processing time from recalculating the remaining sharing rules."}, {"Status": "Incorrect", "Choice": "D. Log out all users and make changes to sharing rules.", "Rationale": "Logging out users does not stop the sharing recalculations from running in the background."}], "meta": {"source": "QuestionBank", "original_number": 159}}, {"id": "Q161", "number": 161, "question": "Northern Trail outfitters in migrating to salesforce from a legacy CRM system that identifies the agent relationships in a look-up table. What should the data architect do in order to migrate the data to Salesfoce?", "options": {"A": "Create custom objects to store agent relationships.", "B": "Migrate to Salesforce without a record owner.", "C": "Assign record owner based on relationship.", "D": "Migrate the data and assign to a non-person system user."}, "answer": ["A"], "explanations": [{"Status": "Correct", "Choice": "A. Create custom objects to store agent relationships.", "Rationale": "The problem states that agent relationships are stored in a **lookup table** in the legacy system. [cite_start]The Salesforce pattern for storing relationships/metadata that doesn't fit the standard model is to use a **Custom Object** (a junction or configuration object) to store the migrated relationship data[cite: 2357]."}, {"Status": "Incorrect", "Choice": "B. Migrate to Salesforce without a record owner.", "Rationale": "All records in Salesforce must have an owner, so this is impossible."}, {"Status": "Incorrect", "Choice": "C. Assign record owner based on relationship.", "Rationale": "Ownership should be assigned based on functional requirements, not simply based on legacy relationships."}, {"Status": "Incorrect", "Choice": "D. Migrate the data and assign to a non-person system user.", "Rationale": "While a system user may own the records, the key is modeling the *relationship*, not just the ownership."}], "meta": {"source": "QuestionBank", "original_number": 161}}, {"id": "Q162", "number": 162, "question": "Universal Container is Implementing salesforce and needs to migrate data from two legacy systems. UC would like to clean and duplicate data before migrate to Salesforce. Which solution should a data architect recommend a clean migration?", "options": {"A": "Define external IDs for an object, migrate second database to first database, and load into Salesforce.", "B": "Define duplicate rules in Salesforce, and load data into Salesforce from both databases.", "C": "Set up staging data base, and define external IDs to merge, clean duplicate data, and load into Salesforce.", "D": "Define external IDs for an object, Insert data from one database, and use upsert for a second database"}, "answer": ["C"], "explanations": [{"Status": "Correct", "Choice": "C. Set up staging data base, and define external IDs to merge, clean duplicate data, and load into Salesforce.", "Rationale": "The most crucial step for a clean migration is performing **cleansing, de-duplication, and transformation** *outside* of Salesforce. [cite_start]A **staging database** is the required environment to perform these complex ETL tasks before loading the final, clean data set into Salesforce[cite: 2371]."}, {"Status": "Incorrect", "Choice": "A. Define external IDs for an object, migrate second database to first database, and load into Salesforce.", "Rationale": "This moves the problem; the cleansing should happen *before* the final load."}, {"Status": "Incorrect", "Choice": "B. Define duplicate rules in Salesforce, and load data into Salesforce from both databases.", "Rationale": "Duplicate rules are helpful, but they cannot effectively clean, transform, and merge millions of records as well as a staging environment can."}, {"Status": "Incorrect", "Choice": "D. Define external IDs for an object, Insert data from one database, and use upsert for a second database", "Rationale": "This risks inserting the same records twice and relies on Upsert, which is less efficient than loading a single, merged dataset."}], "meta": {"source": "QuestionBank", "original_number": 162}}, {"id": "Q163", "number": 163, "question": "Universal Containers (UC) has implemented Sales Cloud for its entire sales organization, UC has built a custom object called projects_c that stores customers project detail and employee bitable hours. The following requirements are needed: A subnet of individuals from the finance team will need to access to the projects object for reporting and adjusting employee utilization. The finance users will not access to any sales objects, but they will need to interact with the custom object. Which license type a data architect recommend for the finance team that best meets the requirements?", "options": {"A": "Service Cloud", "B": "Sales Cloud", "C": "Light Platform Start", "D": "Lighting platform plus"}, "answer": ["C"], "explanations": [{"Status": "Correct", "Choice": "C. Lightning Platform Start", "Rationale": "[cite_start]The **Lightning Platform Start** license is the most cost-effective option for users who only need access to a **small number of custom objects** (Project__c) [cite: 2384] [cite_start]and are explicitly restricted from standard Sales objects[cite: 2384]."}, {"Status": "Incorrect", "Choice": "A. Service Cloud", "Rationale": "[cite_start]This license grants access to standard Service objects (Case, etc.), which the finance team does not need[cite: 2386]."}, {"Status": "Incorrect", "Choice": "B. Sales Cloud", "Rationale": "[cite_start]This license grants access to standard Sales objects (Opportunity, Lead, etc.), which the finance team is explicitly required *not* to access[cite: 2387]."}, {"Status": "Incorrect", "Choice": "D. Lighting platform plus", "Rationale": "[cite_start]This license offers more custom objects and features and is more expensive than the \"Start\" version[cite: 2388]. [cite_start]The \"Start\" version meets the minimal access requirements[cite: 2388]."}], "meta": {"source": "QuestionBank", "original_number": 163}}, {"id": "Q164", "number": 164, "question": "To address different compliance requirements, such as general data protection regulation (GDPR), personally identifiable information (PII), of health insurance Portability and Accountability Act (HIPPA) and others, a SF customer decided to categorize each data element in SF with the following: Data owner, Security Level, such as confidential, Compliance types such as GDPR, PII, HIPPA. A compliance audit would require SF admins to generate reports to manage compliance. What should a data architect recommend to address this requirement?", "options": {"A": "Use metadata API, to extract field attribute information and use the extract to classify and build reports", "B": "Use field metadata attributes for compliance categorization, data owner, and data sensitivity level.", "C": "Create a custom object and field to capture necessary compliance information and build custom reports.", "D": "Build reports for field information, then export the information to classify and report for Audits."}, "answer": ["B"], "explanations": [{"Status": "Correct", "Choice": "B. Use field metadata attributes for compliance categorization, data owner, and data sensitivity level.", "Rationale": "[cite_start]Salesforce provides native **Data Classification metadata attributes** (Sensitivity Level, Compliance Category, Field Usage) on standard and custom fields specifically to capture and report on this compliance information, which is then accessible via the Metadata API[cite: 2395]."}, {"Status": "Incorrect", "Choice": "A. Use metadata API, to extract field attribute information and use the extract to classify and build reports", "Rationale": "[cite_start]This describes the *extraction* method, but the *solution* is using the classification fields themselves[cite: 2397]."}, {"Status": "Incorrect", "Choice": "C. Create a custom object and field to capture necessary compliance information and build custom reports.", "Rationale": "[cite_start]This is unnecessary custom modeling when the native platform already provides dedicated metadata attributes for compliance[cite: 2399]."}, {"Status": "Incorrect", "Choice": "D. Build reports for field information, then export the information to classify and report for Audits.", "Rationale": "[cite_start]Classification should happen inside Salesforce using the built-in metadata, not in a spreadsheet[cite: 2401]."}], "meta": {"source": "QuestionBank", "original_number": 164}}, {"id": "Q165", "number": 165, "question": "Northern trail Outfitters (NTO) uses Sales Cloud and service Cloud to manage sales and support processes. Some of NTOs team are complaining they see new fields on their page unsure of which values need be input. NTO is concerned about lack of governance in making changes to Salesforce. Which governance measure should a data architect recommend to solve this issue?", "options": {"A": "Add description fields to explain why the field is used, and mark the field as required.", "B": "Create and manage a data dictionary and ups a governance process for changes made to common objects.", "C": "Create reports to identify which users are leaving blank, and use external data sources 0 agreement the missing data.", "D": "Create validation rules with error messages to explain why the fields is used"}, "answer": ["B"], "explanations": [{"Status": "Correct", "Choice": "B. Create and manage a data dictionary and ups a governance process for changes made to common objects.", "Rationale": "[cite_start]The root cause is a \"lack of governance in making changes\"[cite: 2412]. [cite_start]A **Data Dictionary** is the centralized record defining all data elements and their purpose[cite: 2413]. [cite_start]Implementing a **governance process** controls who can make changes and ensures those changes are documented and communicated[cite: 2414, 2412]."}, {"Status": "Incorrect", "Choice": "A. Add description fields to explain why the field is used, and mark the field as required.", "Rationale": "[cite_start]This is a tactical step for improving guidance, but it does not solve the root cause of the \"lack of governance\" over the changes themselves[cite: 2416]."}, {"Status": "Incorrect", "Choice": "C. Create reports to identify which users are leaving blank, and use external data sources 0 agreement the missing data.", "Rationale": "[cite_start]This is a solution for *monitoring* data quality, not for *governing* the metadata changes[cite: 2418]."}, {"Status": "Incorrect", "Choice": "D. Create validation rules with error messages to explain why the fields is used", "Rationale": "[cite_start]Validation rules enforce requirements but do not establish the necessary governance process over field creation[cite: 2420]."}], "meta": {"source": "QuestionBank", "original_number": 165}}, {"id": "Q166", "number": 166, "question": "Universal Containers (UC) has adopted Salesforce as its primary sales automated tool. UC has 100,0 customers with a growth rate of 10% a year, UC uses an on-premise web-based billing and invoice system that generates over 1 million invoices a year supporting a monthly billing cycle. The UC sales team needs to be able to pull a customer record and view their account status, Invoice history, and opportunities without navigating outside of Salesforce. What should a data architect use to provide the sales team with the required functionality?", "options": {"A": "Create a custom object and migrate the last 12 months of Invoice data into Salesforce so it can be displayed on the Account layout.", "B": "Write an Apex callout and populate a related list to display on the account record.", "C": "Create a mashup page that will present the billing system records within Salesforce.", "D": "Create a visual force tab with the billing system encapsulated within an iframe."}, "answer": ["C"], "explanations": [{"Status": "Correct", "Choice": "C. Create a mashup page that will present the billing system records within Salesforce.", "Rationale": "[cite_start]The \"mashup\" pattern (using Canvas or an embedded Lightning Component/Visualforce Page) brings the **UI of the external system (billing system)** into Salesforce[cite: 2431]. [cite_start]This allows users to view **real-time invoice history** without duplicating the 1 million records in Salesforce or navigating outside the application[cite: 2432]."}, {"Status": "Incorrect", "Choice": "A. Create a custom object and migrate the last 12 months of Invoice data into Salesforce so it can be displayed on the Account layout.", "Rationale": "[cite_start]Migrating data is a complex solution that consumes storage and still only provides *historical* data, not the full real-time history[cite: 2434]."}, {"Status": "Incorrect", "Choice": "B. Write an Apex callout and populate a related list to display on the account record.", "Rationale": "[cite_start]This is the technical implementation for Salesforce Connect/External Objects or a custom solution, which is usually preferable to a mashup, but C represents the broader category of embedded external UI solution[cite: 2436]."}, {"Status": "Incorrect", "Choice": "D. Create a visual force tab with the billing system encapsulated within an iframe.", "Rationale": "[cite_start]While similar to a mashup, a simple Visualforce Tab with an iframe provides a poor user experience (clunky navigation, poor security) compared to an authenticated, integrated mashup page (Canvas/Lightning Component)[cite: 2438]."}], "meta": {"source": "QuestionBank", "original_number": 166}}, {"id": "Q167", "number": 167, "question": "A large retail B2C customer wants to build a 360 view of its customer for its call center agents. The customer interaction is currently maintained in the following system: Salesforce CRM, Custom billing solution, Customer Master Data management (MDM), Contract Management system, Marketing solution. What should a data architect recommend that would help uniquely identify customer across multiple systems:", "options": {"A": "Store the salesforce id in all the solutions to identify the customer.", "B": "Create a custom object that will serve as a cross reference for the customer id.", "C": "Create a customer data base and use this id in all systems.", "D": "Create a custom field as external id to maintain the customer Id from the MDM solution."}, "answer": ["D"], "explanations": [{"Status": "Correct", "Choice": "D. Create a custom field as external id to maintain the customer Id from the MDM solution.", "Rationale": "[cite_start]The **Master Data Management (MDM) solution** is defined as the single source of truth for customer master data[cite: 2446]. [cite_start]To uniquely identify the customer across the multiple connected systems, the MDM's **Master Key** must be stored on the Salesforce customer record (Account/Contact) as an **External ID** field[cite: 2447]."}, {"Status": "Incorrect", "Choice": "A. Store the salesforce id in all the solutions to identify the customer.", "Rationale": "[cite_start]Salesforce IDs are generated by Salesforce; the customer master key should originate from the system of record (MDM)[cite: 2449]."}, {"Status": "Incorrect", "Choice": "B. Create a custom object that will serve as a cross reference for the customer id.", "Rationale": "[cite_start]A custom object is unnecessary overhead for a single ID field[cite: 2451]."}, {"Status": "Incorrect", "Choice": "C. Create a customer data base and use this id in all systems.", "Rationale": "[cite_start]This is the function of the MDM system, which already exists[cite: 2453]."}], "meta": {"source": "QuestionBank", "original_number": 167}}, {"id": "Q168", "number": 168, "question": "Universal Containers has a requirement to store more than 100 million records in salesforce and needs to create a custom big object to support this business requirement. Which two tools should a data architect use to build custom object?", "options": {"A": "Use DX to create big object.", "B": "Use Metadata API to create big object.", "C": "Go to Big Object In setup select new to create big object.", "D": "Go to Object manager In setup and select new to create big object."}, "answer": ["B", "C"], "explanations": [{"Status": "Correct", "Choice": "B. Use Metadata API to create big object.", "Rationale": "[cite_start]**Big Objects** are part of the application metadata and can be created using the **Metadata API** (often deployed via tools like VS Code or an Ant migration tool)[cite: 2459]."}, {"Status": "Correct", "Choice": "C. Go to Big Object In setup select new to create big object.", "Rationale": "[cite_start]Big Objects can be created directly in the Salesforce Setup menu by navigating to **Big Objects** and using the point-and-click interface[cite: 2461]."}, {"Status": "Incorrect", "Choice": "A. Use DX to create big object.", "Rationale": "[cite_start]Salesforce DX (Developer Experience) tools are used for deployment, often leveraging the Metadata API, but the Metadata API is the core building block[cite: 2463]."}, {"Status": "Incorrect", "Choice": "D. Go to Object manager In setup and select new to create big object.", "Rationale": "[cite_start]Big Objects have their own separate section in Setup and are not created under the standard Object Manager[cite: 2465]."}], "meta": {"source": "QuestionBank", "original_number": 168}}, {"id": "Q170", "number": 170, "question": "Northern Trail Outfitters (NTO) wants to implement backup and restore for Salesforce data, Currently, it has data backup processes that runs weekly, which back up all Salesforce data to an enterprise data warehouse (EDW). NTO wants to move to daily backups and provide restore capability to avoid any data loss in case of outage. What should a data architect recommend for a daily backup and restore solution?", "options": {"A": "Use Appxchange package for backup and restore.", "B": "Use ETL for backup and restore from EDW.", "C": "Use Bulk API to extract data on daily basis to EDW and REST API for restore.", "D": "Change weekly backup process to daily backup, and implement a custom restore solution."}, "answer": ["A"], "explanations": [{"Status": "Correct", "Choice": "A. Use Appxchange package for backup and restore.", "Rationale": "[cite_start]The requirement is for **daily backups** and **restore capability**[cite: 2492]. [cite_start]A dedicated, third-party **Backup and Restore solution** (AppExchange) is the only recommended architectural approach that provides the necessary automation, granular field-level recovery, and proven restore capability[cite: 2493]."}, {"Status": "Incorrect", "Choice": "B. Use ETL for backup and restore from EDW.", "Rationale": "[cite_start]An ETL tool can extract data, but it does not inherently provide the required **restore capability** back into Salesforce[cite: 2495]."}, {"Status": "Incorrect", "Choice": "C. Use Bulk API to extract data on daily basis to EDW and REST API for restore.", "Rationale": "[cite_start]This describes a custom-built solution, which is high-risk and high-effort, and still lacks the certified restore functionality of a dedicated tool[cite: 2497, 2498]."}, {"Status": "Incorrect", "Choice": "D. Change weekly backup process to daily backup, and implement a custom restore solution.", "Rationale": "[cite_start]Implementing a custom restore solution is high-risk and high-effort compared to using a proven AppExchange solution[cite: 2499]."}], "meta": {"source": "QuestionBank", "original_number": 170}}, {"id": "Q171", "number": 171, "question": "A large Automobile company has implemented SF for its Sales Associates. Leads flow from its website to SF using a batch integration in SF. The Batch job converts the leads to Accounts in SF. Customer visiting their retail stores are also created in SF as Accounts. The company has noticed a large number of duplicate accounts in SF. On analysis, it was found that Global Search to search for customers in Salesforce before they create the customers. Which scalable option should a data Architect choose to implement to avoid duplicates?", "options": {"A": "Create duplicate rules in SF to validate duplicates during the account creation process", "B": "Implement a MDM solution to validate the customer information before creating Accounts in SF.", "C": "Build Custom search based on fields on Accounts which can be matched with customer when they visit the store", "D": "Customize Account creation process to search if customer exists before creating an Account."}, "answer": ["A"], "explanations": [{"Status": "Correct", "Choice": "A. Create duplicate rules in SF to validate duplicates during the account creation process", "Rationale": "[cite_start]**Duplicate Rules** are the native, declarative, and scalable solution to **proactively validate duplicates** during record creation (both manual and API/integration initiated) and prevent them from being created[cite: 2511]."}, {"Status": "Incorrect", "Choice": "B. Implement a MDM solution to validate the customer information before creating Accounts in SF.", "Rationale": "[cite_start]MDM is an expensive, complex architectural solution that is overkill for the immediate need, which can be solved with native features[cite: 2513]."}, {"Status": "Incorrect", "Choice": "C. Build Custom search based on fields on Accounts which can be matched with customer when they visit the store", "Rationale": "Sales associates already use Global Search. [cite_start]Building a custom search tool is high-effort and only addresses the manual entry component, not the batch integration component[cite: 2515]."}, {"Status": "Incorrect", "Choice": "D. Customize Account creation process to search if customer exists before creating an Account.", "Rationale": "[cite_start]This describes a custom (coded) solution[cite: 2517]. [cite_start]Duplicate Rules are the declarative alternative[cite: 2517]."}], "meta": {"source": "QuestionBank", "original_number": 171}}, {"id": "Q172", "number": 172, "question": "Universal Containers (UC) has implemented Salesforce, UC is running out of storage and needs to have an archiving solution, UC would like to maintain two years of data in Saleforce and archive older data out of Salesforce. Which solution should a data architect recommend as an archiving solution?", "options": {"A": "Use a third-party backup solution to backup all data off platform.", "B": "Build a batch join move all records off platform, and delete all records from Salesforce.", "C": "Build a batch join to move two-year-old records off platform, and delete records from Salesforce.", "D": "Build a batch job to move all restore off platform, and delete old records from Salesforce."}, "answer": ["C"], "explanations": [{"Status": "Correct", "Choice": "C. Build a batch join to move two-year-old records off platform, and delete records from Salesforce.", "Rationale": "[cite_start]The problem is **running out of storage** and the need to **maintain 2 years of active data** while **archiving older data**[cite: 2526]. [cite_start]The recommended solution is to automate a **purge process** (Batch Job) to move records older than 2 years *off-platform* (archive) and then delete them from Salesforce[cite: 2527]."}, {"Status": "Incorrect", "Choice": "A. Use a third-party backup solution to backup all data off platform.", "Rationale": "[cite_start]Backup is for recovery; it is not an archiving solution that removes data from the active system[cite: 2529]."}, {"Status": "Incorrect", "Choice": "B. Build a batch join move all records off platform, and delete all records from Salesforce.", "Rationale": "[cite_start]This deletes *all* data, violating the requirement to maintain two years of active data in Salesforce[cite: 2531]."}, {"Status": "Incorrect", "Choice": "D. Build a batch job to move all restore off platform, and delete old records from Salesforce.", "Rationale": "[cite_start]\"Restore\" is a function of recovery, not archiving[cite: 2533]."}], "meta": {"source": "QuestionBank", "original_number": 172}}, {"id": "Q173", "number": 173, "question": "Northern trail Outfitters (NTO) runs its entire out of an enterprise data warehouse (EDW), NTD\u2019s sales team starting to use Salesforce after a recent implementation, but currently lacks data required to advanced and opportunity to the next stage. NTO\u2019s management has research Salesforce Connect and would like to use It to virtualize and report on data from the EDW within Salesforce. NTO will be running thousands of reports per day across 10 to 15 external objects. What should a data architect consider before implementing Salesforce Connect for reporting?", "options": {"A": "Maximum number for records returned", "B": "OData callout limits per day", "C": "Maximum page size for server-driven paging", "D": "Maximum external objects per org"}, "answer": ["B"], "explanations": [{"Status": "Correct", "Choice": "B. OData callout limits per day", "Rationale": "[cite_start]Salesforce Connect uses OData (or other protocols) for real-time access[cite: 2539]. [cite_start]Since the org will be running **thousands of reports per day** across 10-15 external objects, the primary risk is hitting the hourly and daily **OData callout limits**, which will cause integrations to fail and reports to time out[cite: 2540]."}, {"Status": "Incorrect", "Choice": "A. Maximum number for records returned", "Rationale": "[cite_start]While the maximum records returned per external object report query is a consideration, the callout limit (B) is the more critical system-wide constraint that causes the integration to fail[cite: 2542]."}, {"Status": "Incorrect", "Choice": "C. Maximum page size for server-driven paging", "Rationale": "[cite_start]This is a technical setting that affects data retrieval size but is secondary to the overall callout volume[cite: 2544]."}, {"Status": "Incorrect", "Choice": "D. Maximum external objects per org", "Rationale": "[cite_start]The limit on External Objects is high and is not typically a constraint in this scenario[cite: 2545]."}], "meta": {"source": "QuestionBank", "original_number": 173}}, {"id": "Q175", "number": 175, "question": "NTO has a loyalty program to reward repeat customers. The following conditions exists: Reward levels are earned based on the amount spent during the previous 12 months. The program will track every item a customer has bought and grant them points for discount. The program generates 100 million records each month. NTO customer support would like to see a summary of a customer\u2019s recent transaction and reward level(s) they have attained. Which solution should the data architect use to provide the information within the salesforce for the customer support agents?", "options": {"A": "Create a custom object in salesforce to capture and store all reward program. Populate nightly from the point-of-scale system, and present on the customer record.", "B": "Capture the reward program data in an external data store and present the 12 months trailing summary in salesforce using salesforce connect and then external object.", "C": "Provide a button so that the agent can quickly open the point of sales system displaying the customer history.", "D": "Create a custom big object to capture the reward program data and display it on the contact record and update nightly from the point-of-scale system."}, "answer": ["D"], "explanations": [{"Status": "Correct", "Choice": "D. Create a custom big object to capture the reward program data and display it on the contact record and update nightly from the point-of-scale system.", "Rationale": "[cite_start]The data volume is massive (**100 million records each month**)[cite: 2570]. [cite_start]**Custom Big Objects** are the native solution for storing and managing this volume of transactional data within the Salesforce platform efficiently[cite: 2571]. [cite_start]The Big Object can be queried to generate the required summary (reward level, recent transactions) for support agents in the UI[cite: 2572]."}, {"Status": "Incorrect", "Choice": "A. Create a custom object in salesforce to capture and store all reward program. Populate nightly from the point-of-scale system, and present on the customer record.", "Rationale": "[cite_start]Storing 100 million records monthly in a standard custom object would immediately fail due to storage limits[cite: 2575]."}, {"Status": "Incorrect", "Choice": "B. Capture the reward program data in an external data store and present the 12 months trailing summary in salesforce using salesforce connect and then external object.", "Rationale": "[cite_start]Big Objects are often preferred for core business data that is needed for performance predictability within the Salesforce UI[cite: 2577]."}, {"Status": "Incorrect", "Choice": "C. Provide a button so that the agent can quickly open the point of sales system displaying the customer history.", "Rationale": "[cite_start]This is the mashup pattern, which forces the agent to leave the Service Console for the POS system, resulting in a poor user experience[cite: 2579]."}], "meta": {"source": "QuestionBank", "original_number": 175}}, {"id": "Q176", "number": 176, "question": "Universal Container (US) is replacing a home grown CRM solution with Salesforce, UC has decided to migrate operational (Open and active) records to Salesforce, while keeping historical records in legacy system, UC would like historical records to be available in Salesforce on an as needed basis. Which solution should a data architect recommend to meet business requirement?", "options": {"A": "Leverage real-time integration to pull records into Salesforce.", "B": "Bring all data Salesforce, and delete it after a year.", "C": "Leverage mashup to display historical records in Salesforce.", "D": "Build a chair solution to go the legacy system and display records."}, "answer": ["C"], "explanations": [{"Status": "Correct", "Choice": "C. Leverage mashup to display historical records in Salesforce.", "Rationale": "[cite_start]The requirement is to access historical data *on an as needed basis* (low frequency) without moving the data[cite: 2587]. [cite_start]A **mashup** (embedding the legacy system's UI via Canvas, Visualforce, or a Lightning Component) allows users to view the data in context within Salesforce without persisting the data on the platform, saving storage[cite: 2588]."}, {"Status": "Incorrect", "Choice": "A. Leverage real-time integration to pull records into Salesforce.", "Rationale": "[cite_start]This would consume Salesforce storage, violating the premise of keeping the data in the legacy system[cite: 2590]."}, {"Status": "Incorrect", "Choice": "B. Bring all data Salesforce, and delete it after a year.", "Rationale": "[cite_start]This is a purge strategy, not a solution for making historical data available on demand[cite: 2592]."}, {"Status": "Incorrect", "Choice": "D. Build a chair solution to go the legacy system and display records.", "Rationale": "[cite_start]This is unclear terminology but likely refers to a poorly integrated custom solution[cite: 2594]."}], "meta": {"source": "QuestionBank", "original_number": 176}}, {"id": "Q177", "number": 177, "question": "A large retail B2C customer wants to build a 360 view of its customer for its call center agents. The customer interaction is currently maintained in the following system: Salesforce CRM, Custom billing solution, Customer Master Data management (MDM), Contract Management system, Marketing solution. What should a data architect recommend that would help uniquely identify customer across multiple systems:", "options": {"A": "Store the salesforce id in all the solutions to identify the customer.", "B": "Create a custom object that will serve as a cross reference for the customer id.", "C": "Create a customer data base and use this id in all systems.", "D": "Create a custom field as external id to maintain the customer Id from the MDM solution."}, "answer": ["D"], "explanations": [{"Status": "Correct", "Choice": "D. Create a custom field as external id to maintain the customer Id from the MDM solution.", "Rationale": "[cite_start]The **Master Data Management (MDM) solution** is the single source of truth[cite: 2602]. [cite_start]To uniquely identify the customer across the multiple connected systems (building the 360 view), the MDM's **Master Key** must be stored on the Salesforce customer record (Account/Contact) as an **External ID** field[cite: 2603]."}, {"Status": "Incorrect", "Choice": "A. Store the salesforce id in all the solutions to identify the customer.", "Rationale": "[cite_start]Salesforce IDs are generated by Salesforce; the customer master key should originate from the system of record (MDM)[cite: 2605]."}, {"Status": "Incorrect", "Choice": "B. Create a custom object that will serve as a cross reference for the customer id.", "Rationale": "[cite_start]Creating a custom object is unnecessary overhead for a single ID field[cite: 2607]."}, {"Status": "Incorrect", "Choice": "C. Create a customer data base and use this id in all systems.", "Rationale": "[cite_start]This is the function of the MDM system, which already exists[cite: 2609]."}], "meta": {"source": "QuestionBank", "original_number": 177}}, {"id": "Q178", "number": 178, "question": "Universal Containers has a requirement to store more than 100 million records in salesforce and needs to create a custom big object to support this business requirement. Which two tools should a data architect use to build custom object?", "options": {"A": "Use DX to create big object.", "B": "Use Metadata API to create big object.", "C": "Go to Big Object In setup select new to create big object.", "D": "Go to Object manager In setup and select new to create big object."}, "answer": ["B", "C"], "explanations": [{"Status": "Correct", "Choice": "B. Use Metadata API to create big object.", "Rationale": "[cite_start]**Big Objects** are part of the application metadata and can be created using the **Metadata API** (which is used by deployment tools like VS Code or Ant)[cite: 2615]."}, {"Status": "Correct", "Choice": "C. Go to Big Object In setup select new to create big object.", "Rationale": "[cite_start]Big Objects can be created directly in the Salesforce Setup menu by navigating to the **Big Objects** section and using the point-and-click interface[cite: 2617]."}, {"Status": "Incorrect", "Choice": "A. Use DX to create big object.", "Rationale": "[cite_start]Salesforce DX uses the Metadata API, but the Metadata API is the core building block[cite: 2619]."}, {"Status": "Incorrect", "Choice": "D. Go to Object manager In setup and select new to create big object.", "Rationale": "[cite_start]Big Objects have their own separate section in Setup and are not created under the standard Object Manager[cite: 2621]."}], "meta": {"source": "QuestionBank", "original_number": 178}}, {"id": "Q180", "number": 180, "question": "Northern Trail Outfitters (NTO) wants to implement backup and restore for Salesforce data, Currently, it has data backup processes that runs weekly, which back up all Salesforce data to an enterprise data warehouse (EDW). NTO wants to move to daily backups and provide restore capability to avoid any data loss in case of outage. What should a data architect recommend for a daily backup and restore solution?", "options": {"A": "Use Appxchange package for backup and restore.", "B": "Use ETL for backup and restore from EDW.", "C": "Use Bulk API to extract data on daily basis to EDW and REST API for restore.", "D": "Change weekly backup process to daily backup, and implement a custom restore solution."}, "answer": ["A"], "explanations": [{"Status": "Correct", "Choice": "A. Use Appxchange package for backup and restore.", "Rationale": "[cite_start]The requirement is for **daily backups** and **restore capability**[cite: 2648]. [cite_start]A dedicated, third-party **Backup and Restore solution** (AppExchange) is the only recommended architectural approach that provides the necessary automation, granular field-level recovery, and certified restore capability[cite: 2649]."}, {"Status": "Incorrect", "Choice": "B. Use ETL for backup and restore from EDW.", "Rationale": "[cite_start]An ETL tool can extract data, but it does not inherently provide the required **restore capability** back into Salesforce[cite: 2651]."}, {"Status": "Incorrect", "Choice": "C. Use Bulk API to extract data on daily basis to EDW and REST API for restore.", "Rationale": "[cite_start]This describes a custom-built solution, which is high-risk and high-effort, and still lacks the certified restore functionality of a dedicated tool[cite: 2653, 2654]."}, {"Status": "Incorrect", "Choice": "D. Change weekly backup process to daily backup, and implement a custom restore solution.", "Rationale": "[cite_start]Implementing a custom restore solution is high-risk and high-effort compared to using a proven AppExchange solution[cite: 2655]."}], "meta": {"source": "QuestionBank", "original_number": 180}}, {"id": "Q181", "number": 181, "question": "A large Automobile company has implemented SF for its Sales Associates. Leads flow from its website to SF using a batch integration in SF. The Batch job converts the leads to Accounts in SF. Customer visiting their retail stores are also created in SF as Accounts. The company has noticed a large number of duplicate accounts in SF. On analysis, it was found that Global Search to search for customers in Salesforce before they create the customers. Which scalable option should a data Architect choose to implement to avoid duplicates?", "options": {"A": "Create duplicate rules in SF to validate duplicates during the account creation process", "B": "Implement a MDM solution to validate the customer information before creating Accounts in SF.", "C": "Build Custom search based on fields on Accounts which can be matched with customer when they visit the store", "D": "Customize Account creation process to search if customer exists before creating an Account."}, "answer": ["A"], "explanations": [{"Status": "Correct", "Choice": "A. Create duplicate rules in SF to validate duplicates during the account creation process", "Rationale": "[cite_start]**Duplicate Rules** are the native, declarative, and scalable solution to **proactively validate duplicates** during record creation (both manual and API/integration initiated) and prevent them from being created[cite: 2667]."}, {"Status": "Incorrect", "Choice": "B. Implement a MDM solution to validate the customer information before creating Accounts in SF.", "Rationale": "[cite_start]MDM is an expensive, complex architectural solution that is overkill for the immediate need, which can be solved with native features[cite: 2669]."}, {"Status": "Incorrect", "Choice": "C. Build Custom search based on fields on Accounts which can be matched with customer when they visit the store", "Rationale": "[cite_start]Building a custom search tool is high-effort and only addresses the manual entry component, not the batch integration component[cite: 2671]."}, {"Status": "Incorrect", "Choice": "D. Customize Account creation process to search if customer exists before creating an Account.", "Rationale": "[cite_start]This describes a custom (coded) solution[cite: 2673]. [cite_start]Duplicate Rules are the declarative alternative[cite: 2673]."}], "meta": {"source": "QuestionBank", "original_number": 181}}, {"id": "Q182", "number": 182, "question": "Universal Containers (UC) has implemented Salesforce, UC is running out of storage and needs to have an archiving solution, UC would like to maintain two years of data in Saleforce and archive older data out of Salesforce. Which solution should a data architect recommend as an archiving solution?", "options": {"A": "Use a third-party backup solution to backup all data off platform.", "B": "Build a batch join move all records off platform, and delete all records from Salesforce.", "C": "Build a batch join to move two-year-old records off platform, and delete records from Salesforce.", "D": "Build a batch job to move all restore off platform, and delete old records from Salesforce."}, "answer": ["C"], "explanations": [{"Status": "Correct", "Choice": "C. Build a batch join to move two-year-old records off platform, and delete records from Salesforce.", "Rationale": "[cite_start]The problem is **running out of storage** and the need to **maintain 2 years of active data** while **archiving older data**[cite: 2682]. [cite_start]The recommended solution is to automate a **purge process** (Batch Job) to move records older than 2 years *off-platform* (archive) and then delete them from Salesforce[cite: 2683]."}, {"Status": "Incorrect", "Choice": "A. Use a third-party backup solution to backup all data off platform.", "Rationale": "[cite_start]Backup is for recovery; it is not an archiving solution that removes data from the active system[cite: 2685]."}, {"Status": "Incorrect", "Choice": "B. Build a batch join move all records off platform, and delete all records from Salesforce.", "Rationale": "[cite_start]This deletes *all* data, violating the requirement to maintain two years of active data in Salesforce[cite: 2687]."}, {"Status": "Incorrect", "Choice": "D. Build a batch job to move all restore off platform, and delete old records from Salesforce.", "Rationale": "[cite_start]\"Restore\" is a function of recovery, not archiving[cite: 2689]."}], "meta": {"source": "QuestionBank", "original_number": 182}}, {"id": "Q183", "number": 183, "question": "Northern trail Outfitters (NTO) runs its entire out of an enterprise data warehouse (EDW), NTD\u2019s sales team starting to use Salesforce after a recent implementation, but currently lacks data required to advanced and opportunity to the next stage. NTO\u2019s management has research Salesforce Connect and would like to use It to virtualize and report on data from the EDW within Salesforce. NTO will be running thousands of reports per day across 10 to 15 external objects. What should a data architect consider before implementing Salesforce Connect for reporting?", "options": {"A": "Maximum number for records returned", "B": "OData callout limits per day", "C": "Maximum page size for server-driven paging", "D": "Maximum external objects per org"}, "answer": ["B"], "explanations": [{"Status": "Correct", "Choice": "B. OData callout limits per day", "Rationale": "[cite_start]Since the org will be running **thousands of reports per day** across 10-15 external objects, the primary risk is hitting the hourly and daily **OData callout limits**, which will cause integrations to fail and reports to time out[cite: 2695]."}, {"Status": "Incorrect", "Choice": "A. Maximum number for records returned", "Rationale": "[cite_start]While the maximum records returned per external object report query is a consideration, the callout limit (B) is the more critical system-wide constraint that causes the integration to fail[cite: 2697]."}, {"Status": "Incorrect", "Choice": "C. Maximum page size for server-driven paging", "Rationale": "[cite_start]This is a technical setting that affects data retrieval size but is secondary to the overall callout volume[cite: 2699]."}, {"Status": "Incorrect", "Choice": "D. Maximum external objects per org", "Rationale": "[cite_start]The limit on External Objects is high and is not typically a constraint in this scenario[cite: 2700]."}], "meta": {"source": "QuestionBank", "original_number": 183}}, {"id": "Q185", "number": 185, "question": "NTO has a loyalty program to reward repeat customers. The following conditions exists: Reward levels are earned based on the amount spent during the previous 12 months. The program will track every item a customer has bought and grant them points for discount. The program generates 100 million records each month. NTO customer support would like to see a summary of a customer\u2019s recent transaction and reward level(s) they have attained. Which solution should the data architect use to provide the information within the salesforce for the customer support agents?", "options": {"A": "Create a custom object in salesforce to capture and store all reward program. Populate nightly from the point-of-scale system, and present on the customer record.", "B": "Capture the reward program data in an external data store and present the 12 months trailing summary in salesforce using salesforce connect and then external object.", "C": "Provide a button so that the agent can quickly open the point of sales system displaying the customer history.", "D": "Create a custom big object to capture the reward program data and display it on the contact record and update nightly from the point-of-scale system."}, "answer": ["D"], "explanations": [{"Status": "Correct", "Choice": "D. Create a custom big object to capture the reward program data and display it on the contact record and update nightly from the point-of-scale system.", "Rationale": "[cite_start]The data volume is massive (**100 million records each month**)[cite: 2725]. [cite_start]**Custom Big Objects** are the native solution for storing and managing this volume of transactional data within the Salesforce platform efficiently[cite: 2726]. [cite_start]The Big Object can be queried to generate the required summary (reward level, recent transactions) for support agents in the UI[cite: 2727]."}, {"Status": "Incorrect", "Choice": "A. Create a custom object in salesforce to capture and store all reward program. Populate nightly from the point-of-scale system, and present on the customer record.", "Rationale": "[cite_start]Storing 100 million records monthly in a standard custom object would immediately fail due to storage limits[cite: 2730]."}, {"Status": "Incorrect", "Choice": "B. Capture the reward program data in an external data store and present the 12 months trailing summary in salesforce using salesforce connect and then external object.", "Rationale": "[cite_start]Big Objects are often preferred for core business data that is needed for performance predictability within the Salesforce UI[cite: 2732]."}, {"Status": "Incorrect", "Choice": "C. Provide a button so that the agent can quickly open the point of sales system displaying the customer history.", "Rationale": "[cite_start]This is the mashup pattern, which forces the agent to leave the Service Console for the POS system, resulting in a poor user experience[cite: 2734]."}], "meta": {"source": "QuestionBank", "original_number": 185}}, {"id": "Q186", "number": 186, "question": "Universal Container (US) is replacing a home grown CRM solution with Salesforce, UC has decided to migrate operational (Open and active) records to Salesforce, while keeping historical records in legacy system, UC would like historical records to be available in Salesforce on an as needed basis. Which solution should a data architect recommend to meet business requirement?", "options": {"A": "Leverage real-time integration to pull records into Salesforce.", "B": "Bring all data Salesforce, and delete it after a year.", "C": "Leverage mashup to display historical records in Salesforce.", "D": "Build a chair solution to go the legacy system and display records."}, "answer": ["C"], "explanations": [{"Status": "Correct", "Choice": "C. Leverage mashup to display historical records in Salesforce.", "Rationale": "[cite_start]The requirement is to access historical data *on an as needed basis* (low frequency) without moving the data[cite: 2742]. [cite_start]A **mashup** (embedding the legacy system's UI via Canvas, Visualforce, or a Lightning Component) allows users to view the data in context within Salesforce without persisting the data on the platform, saving storage[cite: 2743]."}, {"Status": "Incorrect", "Choice": "A. Leverage real-time integration to pull records into Salesforce.", "Rationale": "[cite_start]This would consume Salesforce storage, violating the premise of keeping the data in the legacy system[cite: 2745]."}, {"Status": "Incorrect", "Choice": "B. Bring all data Salesforce, and delete it after a year.", "Rationale": "[cite_start]This is a purge strategy, not a solution for making historical data available on demand[cite: 2747]."}, {"Status": "Incorrect", "Choice": "D. Build a chair solution to go the legacy system and display records.", "Rationale": "[cite_start]This is unclear terminology but refers to a poor custom solution[cite: 2749]."}], "meta": {"source": "QuestionBank", "original_number": 186}}, {"id": "Q187", "number": 187, "question": "A large retail B2C customer wants to build a 360 view of its customer for its call center agents. The customer interaction is currently maintained in the following system: Salesforce CRM, Custom billing solution, Customer Master Data management (MDM), Contract Management system, Marketing solution. What should a data architect recommend that would help upgrade uniquely identify customer across multiple systems:", "options": {"A": "Store the salesforce id in all the solutions to identify the customer.", "B": "Create a custom object that will serve as a cross reference for the customer id.", "C": "Create a customer data base and use this id in all systems.", "D": "Create a custom field as external id to maintain the customer Id from the MDM solution."}, "answer": ["D"], "explanations": [{"Status": "Correct", "Choice": "D. Create a custom field as external id to maintain the customer Id from the MDM solution.", "Rationale": "[cite_start]The **Master Data Management (MDM) solution** is the single source of truth[cite: 2757]. [cite_start]To uniquely identify the customer across the multiple connected systems (building the 360 view), the MDM's **Master Key** must be stored on the Salesforce customer record (Account/Contact) as an **External ID** field[cite: 2758]."}, {"Status": "Incorrect", "Choice": "A. Store the salesforce id in all the solutions to identify the customer.", "Rationale": "[cite_start]Salesforce IDs are generated by Salesforce; the customer master key should originate from the system of record (MDM)[cite: 2760]."}, {"Status": "Incorrect", "Choice": "B. Create a custom object that will serve as a cross reference for the customer id.", "Rationale": "[cite_start]Creating a custom object is unnecessary overhead for a single ID field[cite: 2762]."}, {"Status": "Incorrect", "Choice": "C. Create a customer data base and use this id in all systems.", "Rationale": "[cite_start]This is the function of the MDM system, which already exists[cite: 2764]."}], "meta": {"source": "QuestionBank", "original_number": 187}}, {"id": "Q188", "number": 188, "question": "Universal Containers has a requirement to store more than 100 million records in salesforce and needs to create a custom big object to support this business requirement. Which two tools should a data architect use to build custom object?", "options": {"A": "Use DX to create big object.", "B": "Use Metadata API to create big object.", "C": "Go to Big Object In setup select new to create big object.", "D": "Go to Object manager In setup and select new to create big object."}, "answer": ["B", "C"], "explanations": [{"Status": "Correct", "Choice": "B. Use Metadata API to create big object.", "Rationale": "[cite_start]**Big Objects** are part of the application metadata and can be created using the **Metadata API** (which is used by deployment tools like VS Code or Ant)[cite: 2770]."}, {"Status": "Correct", "Choice": "C. Go to Big Object In setup select new to create big object.", "Rationale": "[cite_start]Big Objects can be created directly in the Salesforce Setup menu by navigating to the **Big Objects** section and using the point-and-click interface[cite: 2772]."}, {"Status": "Incorrect", "Choice": "A. Use DX to create big object.", "Rationale": "[cite_start]Salesforce DX uses the Metadata API, but the Metadata API is the core building block[cite: 2774]."}, {"Status": "Incorrect", "Choice": "D. Go to Object manager In setup and select new to create big object.", "Rationale": "[cite_start]Big Objects have their own separate section in Setup and are not created under the standard Object Manager[cite: 2776]."}], "meta": {"source": "QuestionBank", "original_number": 188}}, {"id": "Q190", "number": 190, "question": "Northern Trail Outfitters (NTO) wants to implement backup and restore for Salesforce data, Currently, it has data backup processes that runs weekly, which back up all Salesforce data to an enterprise data warehouse (EDW). NTO wants to move to daily backups and provide restore capability to avoid any data loss in case of outage. What should a data architect recommend for a daily backup and restore solution?", "options": {"A": "Use Appxchange package for backup and restore.", "B": "Use ETL for backup and restore from EDW.", "C": "Use Bulk API to extract data on daily basis to EDW and REST API for restore.", "D": "Change weekly backup process to daily backup, and implement a custom restore solution."}, "answer": ["A"], "explanations": [{"Status": "Correct", "Choice": "A. Use Appxchange package for backup and restore.", "Rationale": "[cite_start]The requirement is for **daily backups** and **restore capability**[cite: 2803]. [cite_start]A dedicated, third-party **Backup and Restore solution** (AppExchange) is the only recommended architectural approach that provides the necessary automation, granular field-level recovery, and certified restore capability[cite: 2804]."}, {"Status": "Incorrect", "Choice": "B. Use ETL for backup and restore from EDW.", "Rationale": "[cite_start]An ETL tool can extract data, but it does not inherently provide the required **restore capability** back into Salesforce[cite: 2806]."}, {"Status": "Incorrect", "Choice": "C. Use Bulk API to extract data on daily basis to EDW and REST API for restore.", "Rationale": "[cite_start]This describes a custom-built solution, which is high-risk and high-effort, and still lacks the certified restore functionality of a dedicated tool[cite: 2808, 2809]."}, {"Status": "Incorrect", "Choice": "D. Change weekly backup process to daily backup, and implement a custom restore solution.", "Rationale": "[cite_start]Implementing a custom restore solution is high-risk and high-effort compared to using a proven AppExchange solution[cite: 2810]."}], "meta": {"source": "QuestionBank", "original_number": 190}}, {"id": "Q191", "number": 191, "question": "A large Automobile company has implemented SF for its Sales Associates. Leads flow from its website to SF using a batch integration in SF. The Batch job converts the leads to Accounts in SF. Customer visiting their retail stores are also created in SF as Accounts. The company has noticed a large number of duplicate accounts in SF. On analysis, it was found that Global Search to search for customers in Salesforce before they create the customers. Which scalable option should a data Architect choose to implement to avoid duplicates?", "options": {"A": "Create duplicate rules in SF to validate duplicates during the account creation process", "B": "Implement a MDM solution to validate the customer information before creating Accounts in SF.", "C": "Build Custom search based on fields on Accounts which can be matched with customer when they visit the store", "D": "Customize Account creation process to search if customer exists before creating an Account."}, "answer": ["A"], "explanations": [{"Status": "Correct", "Choice": "A. Create duplicate rules in SF to validate duplicates during the account creation process", "Rationale": "[cite_start]**Duplicate Rules** are the native, declarative, and scalable solution to **proactively validate duplicates** during record creation (both manual and API/integration initiated) and prevent them from being created[cite: 2822]."}, {"Status": "Incorrect", "Choice": "B. Implement a MDM solution to validate the customer information before creating Accounts in SF.", "Rationale": "[cite_start]MDM is an expensive, complex architectural solution that is overkill for the immediate need, which can be solved with native features[cite: 2824]."}, {"Status": "Incorrect", "Choice": "C. Build Custom search based on fields on Accounts which can be matched with customer when they visit the store", "Rationale": "[cite_start]Building a custom search tool is high-effort and only addresses the manual entry component, not the batch integration component[cite: 2826]."}, {"Status": "Incorrect", "Choice": "D. Customize Account creation process to search if customer exists before creating an Account.", "Rationale": "[cite_start]This describes a custom (coded) solution[cite: 2828]. [cite_start]Duplicate Rules are the declarative alternative[cite: 2828]."}], "meta": {"source": "QuestionBank", "original_number": 191}}, {"id": "Q192", "number": 192, "question": "Universal Containers (UC) has implemented Salesforce, UC is running out of storage and needs to have an archiving solution, UC would like to maintain two years of data in Saleforce and archive older data out of Salesforce. Which solution should a data architect recommend as an archiving solution?", "options": {"A": "Use a third-party backup solution to backup all data off platform.", "B": "Build a batch join move all records off platform, and delete all records from Salesforce.", "C": "Build a batch join to move two-year-old records off platform, and delete records from Salesforce.", "D": "Build a batch job to move all restore off platform, and delete old records from Salesforce."}, "answer": ["C"], "explanations": [{"Status": "Correct", "Choice": "C. Build a batch join to move two-year-old records off platform, and delete records from Salesforce.", "Rationale": "[cite_start]The problem is **running out of storage** and the need to **maintain 2 years of active data** while **archiving older data**[cite: 2837]. [cite_start]The recommended solution is to automate a **purge process** (Batch Job) to move records older than 2 years *off-platform* (archive) and then delete them from Salesforce[cite: 2838]."}, {"Status": "Incorrect", "Choice": "A. Use a third-party backup solution to backup all data off platform.", "Rationale": "[cite_start]Backup is for recovery; it is not an archiving solution that removes data from the active system[cite: 2840]."}, {"Status": "Incorrect", "Choice": "B. Build a batch join move all records off platform, and delete all records from Salesforce.", "Rationale": "[cite_start]This deletes *all* data, violating the requirement to maintain two years of active data in Salesforce[cite: 2842]."}, {"Status": "Incorrect", "Choice": "D. Build a batch job to move all restore off platform, and delete old records from Salesforce.", "Rationale": "[cite_start]\"Restore\" is a function of recovery, not archiving[cite: 2844]."}], "meta": {"source": "QuestionBank", "original_number": 192}}, {"id": "Q193", "number": 193, "question": "Northern trail Outfitters (NTO) runs its entire out of an enterprise data warehouse (EDW), NTD\u2019s sales team starting to use Salesforce after a recent implementation, but currently lacks data required to advanced and opportunity to the next stage. NTO\u2019s management has research Salesforce Connect and would like to use It to virtualize and report on data from the EDW within Salesforce. NTO will be running thousands of reports per day across 10 to 15 external objects. What should a data architect consider before implementing Salesforce Connect for reporting?", "options": {"A": "Maximum number for records returned", "B": "OData callout limits per day", "C": "Maximum page size for server-driven paging", "D": "Maximum external objects per org"}, "answer": ["B"], "explanations": [{"Status": "Correct", "Choice": "B. OData callout limits per day", "Rationale": "[cite_start]Since the org will be running **thousands of reports per day** across 10-15 external objects, the primary risk is hitting the hourly and daily **OData callout limits**, which will cause integrations to fail and reports to time out[cite: 2850]."}, {"Status": "Incorrect", "Choice": "A. Maximum number for records returned", "Rationale": "[cite_start]While the maximum records returned per external object report query is a consideration, the callout limit (B) is the more critical system-wide constraint that causes the integration to fail[cite: 2852]."}, {"Status": "Incorrect", "Choice": "C. Maximum page size for server-driven paging", "Rationale": "[cite_start]This is a technical setting that affects data retrieval size but is secondary to the overall callout volume[cite: 2854]."}, {"Status": "Incorrect", "Choice": "D. Maximum external objects per org", "Rationale": "[cite_start]The limit on External Objects is high and is not typically a constraint in this scenario[cite: 2855]."}], "meta": {"source": "QuestionBank", "original_number": 193}}, {"id": "Q195", "number": 195, "question": "NTO has a loyalty program to reward repeat customers. The following conditions exists: Reward levels are earned based on the amount spent during the previous 12 months. The program will track every item a customer has bought and grant them points for discount. The program generates 100 million records each month. NTO customer support would like to see a summary of a customer\u2019s recent transaction and reward level(s) they have attained. Which solution should the data architect use to provide the information within the salesforce for the customer support agents?", "options": {"A": "Create a custom object in salesforce to capture and store all reward program. Populate nightly from the point-of-scale system, and present on the customer record.", "B": "Capture the reward program data in an external data store and present the 12 months trailing summary in salesforce using salesforce connect and then external object.", "C": "Provide a button so that the agent can quickly open the point of sales system displaying the customer history.", "D": "Create a custom big object to capture the reward program data and display it on the contact record and update nightly from the point-of-scale system."}, "answer": ["D"], "explanations": [{"Status": "Correct", "Choice": "D. Create a custom big object to capture the reward program data and display it on the contact record and update nightly from the point-of-scale system.", "Rationale": "[cite_start]The data volume is massive (**100 million records each month**)[cite: 2880]. [cite_start]**Custom Big Objects** are the native solution for storing and managing this volume of transactional data within the Salesforce platform efficiently[cite: 2881]. [cite_start]The Big Object can be queried to generate the required summary (reward level, recent transactions) for support agents in the UI[cite: 2882]."}, {"Status": "Incorrect", "Choice": "A. Create a custom object in salesforce to capture and store all reward program. Populate nightly from the point-of-scale system, and present on the customer record.", "Rationale": "[cite_start]Storing 100 million records monthly in a standard custom object would immediately fail due to storage limits[cite: 2885]."}, {"Status": "Incorrect", "Choice": "B. Capture the reward program data in an external data store and present the 12 months trailing summary in salesforce using salesforce connect and then external object.", "Rationale": "[cite_start]Big Objects are often preferred for core business data that is needed for performance predictability within the Salesforce UI[cite: 2887]."}, {"Status": "Incorrect", "Choice": "C. Provide a button so that the agent can quickly open the point of sales system displaying the customer history.", "Rationale": "[cite_start]This is the mashup pattern, which forces the agent to leave the Service Console for the POS system, resulting in a poor user experience[cite: 2889]."}], "meta": {"source": "QuestionBank", "original_number": 195}}, {"id": "Q196", "number": 196, "question": "Universal Container (US) is replacing a home grown CRM solution with Salesforce, UC has decided to migrate operational (Open and active) records to Salesforce, while keeping historical records in legacy system, UC would like historical records to be available in Salesforce on an as needed basis. Which solution should a data architect recommend to meet business requirement?", "options": {"A": "Leverage real-time integration to pull records into Salesforce.", "B": "Bring all data Salesforce, and delete it after a year.", "C": "Leverage mashup to display historical records in Salesforce.", "D": "Build a chair solution to go the legacy system and display records."}, "answer": ["C"], "explanations": [{"Status": "Correct", "Choice": "C. Leverage mashup to display historical records in Salesforce.", "Rationale": "[cite_start]The requirement is to access historical data *on an as needed basis* (low frequency) without moving the data[cite: 2897]. [cite_start]A **mashup** (embedding the legacy system's UI via Canvas, Visualforce, or a Lightning Component) allows users to view the data in context within Salesforce without persisting the data on the platform, saving storage[cite: 2898]."}, {"Status": "Incorrect", "Choice": "A. Leverage real-time integration to pull records into Salesforce.", "Rationale": "[cite_start]This would consume Salesforce storage, violating the premise of keeping the data in the legacy system[cite: 2900]."}, {"Status": "Incorrect", "Choice": "B. Bring all data Salesforce, and delete it after a year.", "Rationale": "[cite_start]This is a purge strategy, not a solution for making historical data available on demand[cite: 2902]."}, {"Status": "Incorrect", "Choice": "D. Build a chair solution to go the legacy system and display records.", "Rationale": "[cite_start]This is unclear terminology but refers to a poor custom solution[cite: 2904]."}], "meta": {"source": "QuestionBank", "original_number": 196}}, {"id": "Q197", "number": 197, "question": "A large retail B2C customer wants to build a 360 view of its customer for its call center agents. The customer interaction is currently maintained in the following system: Salesforce CRM, Custom billing solution, Customer Master Data management (MDM), Contract Management system, Marketing solution. What should a data architect recommend that would help upgrade uniquely identify customer across multiple systems:", "options": {"A": "Store the salesforce id in all the solutions to identify the customer.", "B": "Create a custom object that will serve as a cross reference for the customer id.", "C": "Create a customer data base and use this id in all systems.", "D": "Create a custom field as external id to maintain the customer Id from the MDM solution."}, "answer": ["D"], "explanations": [{"Status": "Correct", "Choice": "D. Create a custom field as external id to maintain the customer Id from the MDM solution.", "Rationale": "[cite_start]The **Master Data Management (MDM) solution** is the single source of truth[cite: 2912]. [cite_start]To uniquely identify the customer across the multiple connected systems (building the 360 view), the MDM's **Master Key** must be stored on the Salesforce customer record (Account/Contact) as an **External ID** field[cite: 2913]."}, {"Status": "Incorrect", "Choice": "A. Store the salesforce id in all the solutions to identify the customer.", "Rationale": "[cite_start]Salesforce IDs are generated by Salesforce; the customer master key should originate from the system of record (MDM)[cite: 2915]."}, {"Status": "Incorrect", "Choice": "B. Create a custom object that will serve as a cross reference for the customer id.", "Rationale": "[cite_start]Creating a custom object is unnecessary overhead for a single ID field[cite: 2917]."}, {"Status": "Incorrect", "Choice": "C. Create a customer data base and use this id in all systems.", "Rationale": "[cite_start]This is the function of the MDM system, which already exists[cite: 2919]."}], "meta": {"source": "QuestionBank", "original_number": 197}}, {"id": "Q198", "number": 198, "question": "Universal Containers has a requirement to store more than 100 million records in salesforce and needs to create a custom big object to support this business requirement. Which two tools should a data architect use to build custom object?", "options": {"A": "Use DX to create big object.", "B": "Use Metadata API to create big object.", "C": "Go to Big Object In setup select new to create big object.", "D": "Go to Object manager In setup and select new to create big object."}, "answer": ["B", "C"], "explanations": [{"Status": "Correct", "Choice": "B. Use Metadata API to create big object.", "Rationale": "[cite_start]**Big Objects** are part of the application metadata and can be created using the **Metadata API** (which is used by deployment tools like VS Code or Ant)[cite: 2925]."}, {"Status": "Correct", "Choice": "C. Go to Big Object In setup select new to create big object.", "Rationale": "[cite_start]Big Objects can be created directly in the Salesforce Setup menu by navigating to the **Big Objects** section and using the point-and-click interface[cite: 2927]."}, {"Status": "Incorrect", "Choice": "A. Use DX to create big object.", "Rationale": "[cite_start]Salesforce DX uses the Metadata API, but the Metadata API is the core building block[cite: 2929]."}, {"Status": "Incorrect", "Choice": "D. Go to Object manager In setup and select new to create big object.", "Rationale": "[cite_start]Big Objects have their own separate section in Setup and are not created under the standard Object Manager[cite: 2931]."}], "meta": {"source": "QuestionBank", "original_number": 198}}, {"id": "Q200", "number": 200, "question": "Northern Trail Outfitters (NTO) wants to implement backup and restore for Salesforce data, Currently, it has data backup processes that runs weekly, which back up all Salesforce data to an enterprise data warehouse (EDW). NTO wants to move to daily backups and provide restore capability to avoid any data loss in case of outage. What should a data architect recommend for a daily backup and restore solution?", "options": {"A": "Use Appxchange package for backup and restore.", "B": "Use ETL for backup and restore from EDW.", "C": "Use Bulk API to extract data on daily basis to EDW and REST API for restore.", "D": "Change weekly backup process to daily backup, and implement a custom restore solution."}, "answer": ["A"], "explanations": [{"Status": "Correct", "Choice": "A. Use Appxchange package for backup and restore.", "Rationale": "[cite_start]The requirement is for **daily backups** and **restore capability**[cite: 2958]. [cite_start]A dedicated, third-party **Backup and Restore solution** (AppExchange) is the only recommended architectural approach that provides the necessary automation, granular field-level recovery, and certified restore capability[cite: 2959]."}, {"Status": "Incorrect", "Choice": "B. Use ETL for backup and restore from EDW.", "Rationale": "[cite_start]An ETL tool can extract data, but it does not inherently provide the required **restore capability** back into Salesforce[cite: 2961]."}, {"Status": "Incorrect", "Choice": "C. Use Bulk API to extract data on daily basis to EDW and REST API for restore.", "Rationale": "[cite_start]This describes a custom-built solution, which is high-risk and high-effort, and still lacks the certified restore functionality of a dedicated tool[cite: 2963, 2964]."}, {"Status": "Incorrect", "Choice": "D. Change weekly backup process to daily backup, and implement a custom restore solution.", "Rationale": "[cite_start]Implementing a custom restore solution is high-risk and high-effort compared to using a proven AppExchange solution[cite: 2965]."}], "meta": {"source": "QuestionBank", "original_number": 200}}, {"id": "Q201", "number": 201, "question": "Universal Containers (UC) is migrating from a legacy system to Salesforce CRM, UC is concerned about the quality of data being entered by users and through external integrations. Which two solution should a data architect recommend to mitigate data quality issues?", "options": {"A": "Leverage picklist and lookup fields where possible", "B": "Leverage Apex to validate the format of data being entered via a mobile device.", "C": "Leverage validation rules and workflows.", "D": "Leverage third-party- AppExchange tools"}, "answer": ["A", "C"], "explanations": [{"Status": "Correct", "Choice": "A. Leverage picklist and lookup fields where possible", "Rationale": "[cite_start]Using **Picklists** and **Lookup Fields** maintains data consistency and quality by limiting user input to predefined lists or existing records, preventing manual entry errors and inconsistent values[cite: 2971]."}, {"Status": "Correct", "Choice": "C. Leverage validation rules and workflows.", "Rationale": "[cite_start]**Validation Rules** enforce data integrity and completeness by preventing records from being saved if they violate business rules[cite: 2973]. [cite_start]**Workflows** (or Flows) can automate standardized processes to ensure data is handled consistently[cite: 2974]."}, {"Status": "Incorrect", "Choice": "B. Leverage Apex to validate the format of data being entered via a mobile device.", "Rationale": "[cite_start]Apex can be used, but **Validation Rules** are the preferred declarative solution for simple data format validation[cite: 2976]."}, {"Status": "Incorrect", "Choice": "D. Leverage third-party- AppExchange tools", "Rationale": "[cite_start]While useful, the architect should recommend leveraging native Salesforce declarative features first before resorting to paid third-party tools[cite: 2977]."}], "meta": {"source": "QuestionBank", "original_number": 201}}, {"id": "Q202", "number": 202, "question": "Universal Containers (CU) is in the process of implementing an enterprise data warehouse (EDW). UC needs to extract 100 million records from Salesforce for migration to the EDW. What data extraction strategy should a data architect use for maximum performance?", "options": {"A": "Install a third-party AppExchange tool.", "B": "Call the REST API in successive queries.", "C": "Utilize PK Chunking with the Bulk API.", "D": "Use the Bulk API in parallel mode."}, "answer": ["C"], "explanations": [{"Status": "Correct", "Choice": "C. Utilize PK Chunking with the Bulk API.", "Rationale": "[cite_start]For extracting massive volumes of data (100 million records) [cite: 2984] [cite_start]with **maximum performance**, the strategy is to use the **Bulk API** combined with **PK Chunking** (Primary Key Chunking)[cite: 2985]. [cite_start]This method automatically splits the massive query into smaller, manageable chunks based on the record ID, bypassing query timeouts and optimizing throughput[cite: 2985]."}, {"Status": "Incorrect", "Choice": "A. Install a third-party AppExchange tool.", "Rationale": "[cite_start]While AppExchange tools exist for ETL, PK Chunking is the core strategy used by those tools for maximum performance[cite: 2987]."}, {"Status": "Incorrect", "Choice": "B. Call the REST API in successive queries.", "Rationale": "[cite_start]The REST API is synchronous and highly inefficient for large volume data extraction, risking API limits and timeouts[cite: 2989]."}, {"Status": "Incorrect", "Choice": "D. Use the Bulk API in parallel mode.", "Rationale": "[cite_start]The Bulk API is used, but specifying **PK Chunking** is the critical step that ensures the query itself is selective enough to complete on the massive table size[cite: 2991]."}], "meta": {"source": "QuestionBank", "original_number": 202}}, {"id": "Q205", "number": 205, "question": "It is decided to maintain the order in opportunity object How should the data architect model this requirement?", "options": {"A": "Create lookup to Custom Price object and share with distributors.", "B": "Configure price Books for each region and share with distributors.", "C": "Manually update Opportunities with Prices application to distributors.", "D": "Add custom fields in Opportunity and use triggers to update prices."}, "answer": ["B"], "explanations": [{"Status": "Correct", "Choice": "B. Configure price Books for each region and share with distributors.", "Rationale": "[cite_start]The requirement is to manage **product prices applicable to their region**[cite: 3008]. [cite_start]**Price Books** are the standard Salesforce feature designed to manage multiple lists of prices for products and can be shared with specific users or regions (Distributors) via sharing rules[cite: 3009]. [cite_start]This is the optimal declarative solution[cite: 3010]."}, {"Status": "Incorrect", "Choice": "A. Create lookup to Custom Price object and share with distributors.", "Rationale": "[cite_start]This is a custom solution for a problem that the standard Price Book object is designed to solve[cite: 3012]."}, {"Status": "Incorrect", "Choice": "C. Manually update Opportunities with Prices application to distributors.", "Rationale": "[cite_start]This is a manual, non-scalable process[cite: 3013]."}, {"Status": "Incorrect", "Choice": "D. Add custom fields in Opportunity and use triggers to update prices.", "Rationale": "[cite_start]This is a high-effort, custom solution that duplicates standard functionality and is difficult to maintain[cite: 3015]."}], "meta": {"source": "QuestionBank", "original_number": 205}}, {"id": "Q206", "number": 206, "question": "North Trail Outfitters (NTO) operates a majority of its business from a central Salesforce org, NTO also owns several secondary orgs that the service, finance, and marketing teams work out of, At the moment, there is no integration between central and secondary orgs, leading to data-visibility issues. Moving forward, NTO has identified that a hub-and-spoke model is the proper architect to manage its data, where the central org is the hub and the secondary orgs are the spokes. Which tool should a data architect use to orchestrate data between the hub org and spoke orgs?", "options": {"A": "A middleware solution that extracts and distributes data across both the hub and spokes.", "B": "Develop custom APIs to poll the hub org for change data and push into the spoke orgs.", "C": "Develop custom APIs to poll the spoke for change data and push into the org.", "D": "A backup and archive solution that extracts and restores data across orgs."}, "answer": ["A"], "explanations": [{"Status": "Correct", "Choice": "A. A middleware solution that extracts and distributes data across both the hub and spokes.", "Rationale": "[cite_start]In a complex, enterprise **hub-and-spoke model**, a **Middleware Solution** (such as an ESB or iPaaS) is necessary to **orchestrate** the data flows, handling extraction, transformation, routing, and distribution of data between the central Hub org and multiple Spoke orgs[cite: 3025]."}, {"Status": "Incorrect", "Choice": "B. Develop custom APIs to poll the hub org for change data and push into the spoke orgs.", "Rationale": "[cite_start]Custom APIs are high-effort, coded solutions that do not provide the centralized **orchestration** or monitoring capabilities of a middleware solution[cite: 3027]."}, {"Status": "Incorrect", "Choice": "C. Develop custom APIs to poll the spoke for change data and push into the org.", "Rationale": "[cite_start]This is too complex for a multi-spoke model and lacks centralized orchestration[cite: 3029]."}, {"Status": "Incorrect", "Choice": "D. A backup and archive solution that extracts and restores data across orgs.", "Rationale": "[cite_start]Backup/archive solutions are for recovery, not for continuous, operational data synchronization[cite: 3031]."}], "meta": {"source": "QuestionBank", "original_number": 206}}, {"id": "Q207", "number": 207, "question": "UC developers have created a new lightning component that uses an Apex controller using a SOQL query to populate a custom list view. Users are complaining that the component often fails to load and returns a time-out error. What tool should a data architect use to identify why the query is taking too long?", "options": {"A": "Use Splunk to query the system logs looking for transaction time and CPU usage.", "B": "Enable and use the query plan tool in the developer console.", "C": "Use salesforce\u2019s query optimizer to analyze the query in the developer console.", "D": "Open a ticket with salesforce support to retrieve transaction logs to e analyzed for processing time."}, "answer": ["B"], "explanations": [{"Status": "Correct", "Choice": "B. Enable and use the query plan tool in the developer console.", "Rationale": "[cite_start]The **Query Plan Tool** (available in the Developer Console or Query Editor) is the native tool designed to analyze the performance of a SOQL query[cite: 3041]. [cite_start]It shows the query cost and indicates if indexes are being used (selectivity), which is exactly what's needed to debug a time-out error[cite: 3042]."}, {"Status": "Incorrect", "Choice": "A. Use Splunk to query the system logs looking for transaction time and CPU usage.", "Rationale": "[cite_start]While transaction logs (accessible via Event Monitoring/Splunk) provide good system-wide diagnostics, the Query Plan Tool is more direct for isolating and debugging a specific slow query[cite: 3044]."}, {"Status": "Incorrect", "Choice": "C. Use salesforce\u2019s query optimizer to analyze the query in the developer console.", "Rationale": "[cite_start]Salesforce's query optimizer runs automatically; the tool needed to *view* the result of the optimization process is the Query Plan Tool[cite: 3046]."}, {"Status": "Incorrect", "Choice": "D. Open a ticket with salesforce support to retrieve transaction logs to e analyzed for processing time.", "Rationale": "[cite_start]This should be done only after exhausting all self-service tools[cite: 3048]."}], "meta": {"source": "QuestionBank", "original_number": 207}}, {"id": "Q209", "number": 209, "question": "The data architect for UC has written a SOQL query that will return all records from the Task object that do not have a value in the Whattd field: Select id, description, Subject from Task where Whatid != NULL When the data architect usages the query to select values for a process a time out error occurs. What does the data architect need to change to make this query more performant?", "options": {"A": "Remove description from the requested field set.", "B": "Change query to SOSL.", "C": "Add limit 100 to the query.", "D": "Change the where clause to filter by a deterministic defined value."}, "answer": ["D"], "explanations": [{"Status": "Correct", "Choice": "D. Change the where clause to filter by a deterministic defined value.", "Rationale": "[cite_start]The query is non-selective and causing a time-out because filtering by `WhatId != NULL` is non-selective (it includes too many records and can't use a standard index)[cite: 3075]. [cite_start]The solution is to add a highly **selective filter** (e.g., `WHERE ActivityDate = TODAY` or another indexed field with a small result set) to make the query performant[cite: 3076]."}, {"Status": "Incorrect", "Choice": "A. Remove description from the requested field set.", "Rationale": "[cite_start]Removing a field may slightly reduce data size, but does not solve the underlying non-selective query issue[cite: 3078]."}, {"Status": "Incorrect", "Choice": "B. Change query to SOSL.", "Rationale": "[cite_start]SOSL is used for text searching, not for querying specific record criteria, and would likely also time out on a non-selective search[cite: 3079]."}, {"Status": "Incorrect", "Choice": "C. Add limit 100 to the query.", "Rationale": "[cite_start]Adding a `LIMIT` clause does not solve the underlying non-selective query issue that forces the database to scan the entire table, which is what causes the time-out[cite: 3081]."}], "meta": {"source": "QuestionBank", "original_number": 209}}, {"id": "Q210", "number": 210, "question": "Universal Containers (UC) is in the process of migrating lagacy inventory data from an enterprise resources planning (ERP) system into Sales Cloud with the following requirements: Legacy inventory data will be stored in a custom child objects called Inventory_c. Inventory data should be related to the standard Account object. The Inventory_c object should Inherit the same sharing rules as the Account object. Anytime an Account record is deleted in Salesforce, the related Inventory_c record(s) should be deleted as well. What type of relationship field should a data architect recommend in this scenario?", "options": {"A": "Master-detail relationship filed on Account, related to Inventory_c", "B": "Master-detail relationship filed on Inventory_c, related to Account", "C": "Indirect lookup relationship field on Account, related to Inventory_c", "D": "Lookup relationship fields on Inventory related to Account"}, "answer": ["B"], "explanations": [{"Status": "Correct", "Choice": "B. Master-detail relationship filed on Inventory_c, related to Account", "Rationale": "[cite_start]A **Master-Detail** relationship meets all three requirements: 1) **Inherit Sharing** (`Inventory__c` inherits the `Account` sharing rules)[cite: 3090]. [cite_start]2) **Cascade Delete** (deleting the Account deletes related `Inventory__c` records)[cite: 3090]. [cite_start]3) **Required Relationship** (ensures `Inventory__c` is related to `Account`)[cite: 3090]. [cite_start]The relationship field should be created on the child object (`Inventory__c`) pointing to the parent (`Account`)[cite: 3093]."}, {"Status": "Incorrect", "Choice": "A. Master-detail relationship filed on Account, related to Inventory_c", "Rationale": "This choice is confusingly phrased and suggests the field is on the Account, which is incorrect. [cite_start]The field must be on the child object[cite: 3092]."}, {"Status": "Incorrect", "Choice": "C. Indirect lookup relationship field on Account, related to Inventory_c", "Rationale": "[cite_start]This is for Salesforce Connect/External Objects and does not enforce cascade delete or sharing inheritance[cite: 3096]."}, {"Status": "Incorrect", "Choice": "D. Lookup relationship fields on Inventory related to Account", "Rationale": "[cite_start]A **Lookup** relationship does not enforce cascade delete or sharing inheritance[cite: 3097]."}], "meta": {"source": "QuestionBank", "original_number": 210}}, {"id": "Q211", "number": 211, "question": "As part of addressing general data protection regulation (GDPR) requirements, UC plans to implement a data classification policy for all its internal systems that stores customer information including salesforce. What should a data architect recommend so that UC can easily classify consumer information maintained in salesforce under both standard and custom objects?", "options": {"A": "Use App Exchange products to classify fields based on policy.", "B": "Use data classification metadata fields available in field definition.", "C": "Create a custom picklist field to capture classification of information on customer.", "D": "Build reports for customer information and validate."}, "answer": ["B"], "explanations": [{"Status": "Correct", "Choice": "B. Use data classification metadata fields available in field definition.", "Rationale": "[cite_start]Salesforce provides native **Data Classification metadata attributes** (Sensitivity Level, Compliance Category, etc.) on standard and custom fields [cite: 3105] [cite_start]specifically to capture and report on compliance information, which is then accessible via the Metadata API and used for GDPR/PII reporting[cite: 3105]."}, {"Status": "Incorrect", "Choice": "A. Use App Exchange products to classify fields based on policy.", "Rationale": "[cite_start]This is unnecessary custom tooling when the native platform provides dedicated metadata attributes for classification[cite: 3107]."}, {"Status": "Incorrect", "Choice": "C. Create a custom picklist field to capture classification of information on customer.", "Rationale": "[cite_start]This would duplicate functionality and be difficult to enforce for every single field in the org[cite: 3109]."}, {"Status": "Incorrect", "Choice": "D. Build reports for customer information and validate.", "Rationale": "[cite_start]Classification should happen inside Salesforce using the built-in metadata, not in a spreadsheet[cite: 3110]."}], "meta": {"source": "QuestionBank", "original_number": 211}}, {"id": "Q212", "number": 212, "question": "Northern Trail Outfitters has these simple requirements for a data export process: File format should be in CSV. Process should be scheduled and run once per week. The expert should be configurable through the Salesforce UI. Which tool should a data architect leverage to accomplish these requirements?", "options": {"A": "Bulk API", "B": "Data export wizard", "C": "Third-party ETL tool", "D": "Data loader"}, "answer": ["B"], "explanations": [{"Status": "Correct", "Choice": "B. Data export wizard", "Rationale": "[cite_start]The **Data Export Wizard** (Weekly Export) is the native Salesforce tool that allows an administrator to **schedule** a weekly data export of the entire org or selected objects in **CSV format** directly through the **Salesforce UI**, meeting all requirements[cite: 3116]."}, {"Status": "Incorrect", "Choice": "A. Bulk API", "Rationale": "[cite_start]The Bulk API requires scripting or an external ETL tool; it is not directly configurable through the Salesforce UI for scheduling[cite: 3118]."}, {"Status": "Incorrect", "Choice": "C. Third-party ETL tool", "Rationale": "[cite_start]This is a viable solution, but the question asks for the simplest tool an architect should leverage, and the native Data Export Wizard is simplest[cite: 3119]."}, {"Status": "Incorrect", "Choice": "D. Data loader", "Rationale": "[cite_start]Data Loader is a desktop application and does not provide a scheduled, UI-configurable solution[cite: 3121]."}], "meta": {"source": "QuestionBank", "original_number": 212}}, {"id": "Q213", "number": 213, "question": "UC is planning a massive SF implementation with large volumes of data. As part of the org\u2019s implementation, several roles, territories, groups, and sharing rules have been configured. The data architect has been tasked with loading all of the required data, including user data, in a timely manner. What should a data architect do to minimize data load times due to system calculations?", "options": {"A": "Enable defer sharing calculations, and suspend sharing rule calculations", "B": "Load the data through data loader, and turn on parallel processing.", "C": "Leverage the Bulk API and concurrent processing with multiple batches", "D": "Enable granular locking to avoid \u201cUNABLE_TO_LOCK_ROW\" error."}, "answer": ["A"], "explanations": [{"Status": "Correct", "Choice": "A. Enable defer sharing calculations, and suspend sharing rule calculations", "Rationale": "[cite_start]With a massive implementation involving many sharing artifacts, the recalculation of sharing rules during a bulk load is the primary performance bottleneck[cite: 3129]. [cite_start]**Enabling Defer Sharing Calculations** (a feature enabled by Salesforce Support) is the most effective single step to **minimize load times**[cite: 3130]."}, {"Status": "Incorrect", "Choice": "B. Load the data through data loader, and turn on parallel processing.", "Rationale": "[cite_start]Parallel processing can increase **speed** but also **worsens** the sharing recalculation bottleneck and can cause row locks[cite: 3132]."}, {"Status": "Incorrect", "Choice": "C. Leverage the Bulk API and concurrent processing with multiple batches", "Rationale": "[cite_start]This also worsens the sharing recalculation bottleneck and is the opposite of the recommended action when sharing is the constraint[cite: 3134]."}, {"Status": "Incorrect", "Choice": "D. Enable granular locking to avoid \u201cUNABLE_TO_LOCK_ROW\" error.", "Rationale": "[cite_start]Granular Locking helps with specific record contention, but not the system-wide performance issue caused by complex sharing recalculations[cite: 3136]."}], "meta": {"source": "QuestionBank", "original_number": 213}}, {"id": "Q214", "number": 214, "question": "UC has the following system: Billing system, Customer support system, CRM system. US has been having trouble with business intelligence across the different systems, Recently US implemented a master data management (MDM) solution that will be the system of truth for the customer records. Which MDM data element is needed to allow reporting across these systems?", "options": {"A": "Global unique customer number.", "B": "Email address.", "C": "Phone number.", "D": "Full name."}, "answer": ["A"], "explanations": [{"Status": "Correct", "Choice": "A. Global unique customer number.", "Rationale": "[cite_start]To allow reporting (Business Intelligence) across disparate systems (Billing, Support, CRM), a common identifier is essential[cite: 3142]. [cite_start]The MDM solution provides the **Global Unique Customer Number** (often called the Master Key or Golden ID), which is used to link all records belonging to the same customer across the various systems[cite: 3143]."}, {"Status": "Incorrect", "Choice": "B. Email address.", "Rationale": "[cite_start]Email address is not guaranteed to be unique, may change, and is a data attribute, not a structural linking element[cite: 3144]."}, {"Status": "Incorrect", "Choice": "C. Phone number.", "Rationale": "[cite_start]Phone number is not guaranteed to be unique and may change[cite: 3145]."}, {"Status": "Incorrect", "Choice": "D. Full name.", "Rationale": "[cite_start]Full name is not guaranteed to be unique and is subject to multiple spellings and formats[cite: 3146]."}], "meta": {"source": "QuestionBank", "original_number": 214}}, {"id": "Q215", "number": 215, "question": "Universal Container is using Salesforce for Opportunity management and enterprise resource planning (ERP) for order management. Sales reps do not have access to the ERP and have no visibility into order status. What solution a data architect recommend to give the sales team visibility into order status?", "options": {"A": "Leverage Canvas to bring the order management UI in to the Salesforce tab.", "B": "Build batch jobs to push order line items to salesforce.", "C": "leverage Salesforce Connect top bring the order line item from the legacy system to Salesforce.", "D": "Build real-time integration to pull order line items into Salesforce when viewing orders."}, "answer": ["C"], "explanations": [{"Status": "Correct", "Choice": "C. leverage Salesforce Connect top bring the order line item from the legacy system to Salesforce.", "Rationale": "[cite_start]**Salesforce Connect** is the standard, modern, low-code solution for providing **visibility into external system data** (Order Status) without replicating the data (orders/line items) into Salesforce[cite: 3156]. [cite_start]It displays the external data as native **External Objects**[cite: 3157]."}, {"Status": "Incorrect", "Choice": "A. Leverage Canvas to bring the order management UI in to the Salesforce tab.", "Rationale": "This is a \"mashup\" solution. [cite_start]While viable, Salesforce Connect provides a more integrated, native UI experience and easier reporting capabilities[cite: 3158]."}, {"Status": "Incorrect", "Choice": "B. Build batch jobs to push order line items to salesforce.", "Rationale": "[cite_start]This would consume significant Salesforce storage and introduce complexity[cite: 3160]."}, {"Status": "Incorrect", "Choice": "D. Build real-time integration to pull order line items into Salesforce when viewing orders.", "Rationale": "[cite_start]This describes a custom (coded) solution (Apex callouts) when Salesforce Connect is the preferred standard alternative[cite: 3162]."}], "meta": {"source": "QuestionBank", "original_number": 215}}, {"id": "Q216", "number": 216, "question": "Universal Containers (UC) is transitioning from Classic to Lightning Experience. What does UC need to do to ensure users have access to its notices and attachments in Lightning Experience?", "options": {"A": "Add Notes and Attachments Related List to page Layout in Lighting Experience.", "B": "Manually upload Notes in Lighting Experience.", "C": "Migrate Notes and Attachment to Enhanced Notes and Files a migration tool", "D": "Manually upload Attachments in Lighting Experience."}, "answer": ["C"], "explanations": [{"Status": "Correct", "Choice": "C. Migrate Notes and Attachment to Enhanced Notes and Files a migration tool", "Rationale": "[cite_start]The best practice when moving from Classic to Lightning is to **migrate** the old `Notes & Attachments` data to the modern **Enhanced Notes and Files** data model using a migration tool[cite: 3169]. [cite_start]This ensures better security, easier sharing, and seamless integration with the Lightning UI[cite: 3170]."}, {"Status": "Incorrect", "Choice": "A. Add Notes and Attachments Related List to page Layout in Lighting Experience.", "Rationale": "[cite_start]The classic related list displays the old data model, but users should be encouraged to use the modern Files/Enhanced Notes[cite: 3172]."}, {"Status": "Incorrect", "Choice": "B. Manually upload Notes in Lighting Experience.", "Rationale": "[cite_start]Manual upload is not feasible for an existing organization[cite: 3174]."}, {"Status": "Incorrect", "Choice": "D. Manually upload Attachments in Lighting Experience.", "Rationale": "[cite_start]Manual upload is not feasible for an existing organization[cite: 3175]."}], "meta": {"source": "QuestionBank", "original_number": 216}}, {"id": "Q217", "number": 217, "question": "Universal Container has implemented Sales Cloud to manage patient and related health records. During a recent security audit of the system it was discovered that same standard and custom fields need to encrypted. Which solution should a data architect recommend to encrypt existing fields?", "options": {"A": "Use Apex Crypto Class encrypt customer and standard fields.", "B": "Implement classic encryption to encrypt custom and standard fields.", "C": "Implement shield platform encryption to encrypt and standard fields", "D": "Expert data out of Salesforce and encrypt custom and standard fields."}, "answer": ["C"], "explanations": [{"Status": "Correct", "Choice": "C. Implement shield platform encryption to encrypt and standard fields", "Rationale": "[cite_start]**Shield Platform Encryption (SPE)** is the comprehensive, recommended Salesforce solution for encrypting a broad range of **standard and custom fields** (and Files/Search Indexes) at rest to meet stringent regulatory requirements like HIPAA for patient data[cite: 3184]."}, {"Status": "Incorrect", "Choice": "A. Use Apex Crypto Class encrypt customer and standard fields.", "Rationale": "Apex encryption is for custom, complex logic. [cite_start]SPE is the declarative/governed solution for common standard/custom fields[cite: 3186]."}, {"Status": "Incorrect", "Choice": "B. Implement classic encryption to encrypt custom and standard fields.", "Rationale": "[cite_start]**Classic Encryption** is limited to a single 175-character custom text field and does not support standard fields[cite: 3188]."}, {"Status": "Incorrect", "Choice": "D. Expert data out of Salesforce and encrypt custom and standard fields.", "Rationale": "[cite_start]This does not solve the requirement to encrypt data *in* Salesforce[cite: 3190]."}], "meta": {"source": "QuestionBank", "original_number": 217}}, {"id": "Q219", "number": 219, "question": "Contacts When doing an initial performance test, the data architect noticed an extremely slow response for reports and list views. What should a data architect do to solve the performance issue?", "options": {"A": "Load only the data that the users is permitted to access", "B": "Add custom indexes on frequently searched account and contact objects fields", "C": "Limit data loading to the 2000 most recently created records.", "D": "Create a skinny table to represent account and contact objects."}, "answer": ["B"], "explanations": [{"Status": "Correct", "Choice": "B. Add custom indexes on frequently searched account and contact objects fields", "Rationale": "[cite_start]**Slow response for reports and list views** is often caused by non-selective queries[cite: 3201]. [cite_start]Adding **custom indexes** to the fields used in reports/list view filters is the direct, standard method to make the queries selective and significantly improve performance[cite: 3202]."}, {"Status": "Incorrect", "Choice": "A. Load only the data that the users is permitted to access", "Rationale": "Loading less data is not the primary solution. [cite_start]Query optimization is[cite: 3204]."}, {"Status": "Incorrect", "Choice": "C. Limit data loading to the 2000 most recently created records.", "Rationale": "[cite_start]This limits the data, but the solution should be to optimize the entire data set[cite: 3206]."}, {"Status": "Incorrect", "Choice": "D. Create a skinny table to represent account and contact objects.", "Rationale": "[cite_start]Skinny Tables are good, but adding indexes is the most direct and necessary step to fix the poor performance on filtering fields[cite: 3208]."}], "meta": {"source": "QuestionBank", "original_number": 219}}, {"id": "Q220", "number": 220, "question": "Universal Containers is experiencing frequent and persistent group membership locking issues that severely restricts its ability to manage manual and automated updates at the same time. What should a data architect do in order to restore the issue?", "options": {"A": "Enable granular locking", "B": "Enable parallel sharing rule calculation.", "C": "Enable defer sharing calculation", "D": "Enable implicit sharing"}, "answer": ["A"], "explanations": [{"Status": "Correct", "Choice": "A. Enable granular locking", "Rationale": "[cite_start]**Granular Locking** is a feature designed to mitigate the problem of **group membership locking issues** and contention on territory, group, and role hierarchy nodes during concurrent manual and automated updates[cite: 3214]."}, {"Status": "Incorrect", "Choice": "B. Enable parallel sharing rule calculation.", "Rationale": "[cite_start]Parallel sharing can increase lock contention if the issue is already persistent[cite: 3216]."}, {"Status": "Incorrect", "Choice": "C. Enable defer sharing calculation", "Rationale": "[cite_start]Deferral helps with bulk data loads and scheduled changes, but does not solve the frequent/persistent locking during concurrent manual updates[cite: 3217]."}, {"Status": "Incorrect", "Choice": "D. Enable implicit sharing", "Rationale": "[cite_start]Implicit sharing is a security feature that grants access between parents and children; it does not mitigate locking issues[cite: 3219]."}], "meta": {"source": "QuestionBank", "original_number": 220}}];